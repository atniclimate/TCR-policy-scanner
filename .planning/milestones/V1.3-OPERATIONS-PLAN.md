# TCR Policy Scanner — v1.3 Operations Plan

**Prepared:** 2026-02-12
**Context:** v1.2 SHIPPED (743 tests, 95 files, ~37,900 LOC, 992 documents generated)
**Objective:** Ship a production-ready website that serves real DOCX advocacy packets to 592 Tribal Nations via SquareSpace, stress-tested for 100–200 users over 48 hours.

---

## Architecture Overview

```
Phase 1: DOCX Visual QA          Phase 2: Website Launch          Phase 3: Production Hardening
┌─────────────────────┐    ┌──────────────────────────────┐    ┌───────────────────────────┐
│ Document Inspection │    │ 4-Agent Review Swarm         │    │ 4-Agent Bug Hunt Swarm    │
│ ├── Formatting QA   │    │ ├── Web Wizard (perf)        │    │ ├── Mr. Magoo (feel/listen)│
│ ├── Data Accuracy   │───>│ ├── Mind Reader (UX/a11y)    │───>│ ├── Cyclops (deep scan)    │
│ ├── Layout Fidelity │    │ ├── Keen Kerner (type)        │    │ ├── Dale Gribble (paranoia)│
│ └── Content Coverage│    │ ├── Pipeline Professor (data) │    │ └── Marie Kondo (minimize) │
│                     │    │ └── GitHub Pages Connection   │    │                           │
│ Playwright + DOCX   │    │                              │    │ Merge-Sort QA Strategy    │
│ Design Agents       │    │ SquareSpace Integration      │    │ Clone Passes for Coverage │
└─────────────────────┘    └──────────────────────────────┘    └───────────────────────────┘
```

---

## Phase 1: DOCX Visual QA ("The Inspector")

**Goal:** Verify that all 992 generated documents are visually correct, properly formatted, and contain accurate data before they become the downloadable product.

**Why this comes first:** The website is a delivery mechanism for these documents. If the documents have errors, a perfect website ships broken product. Fix the source before building the storefront.

### Context Rot Mitigation Strategy

This phase is the most vulnerable to context rot because DOCX inspection requires holding formatting rules, data expectations, and visual standards simultaneously across hundreds of documents. The strategy:

1. **Atomic inspection units.** Never inspect more than 5 documents per context window. Reset with a fresh session after each batch.
2. **Checklist-driven, not memory-driven.** Every inspection session begins by re-reading the checklist file. No relying on "remembering" what to look for.
3. **Progressive narrowing.** Start with automated structural checks (headings exist, tables populated, page count reasonable), then move to visual sampling, then targeted deep-reads.
4. **Written findings, not mental notes.** Every issue gets written to a findings file immediately. The findings file is the memory, not the context window.

### Phase 1 Session Structure

#### Session 1A: Structural Validation (Automated)

```
Prompt for Claude Code:
───────────────────────
I need to validate the structural integrity of the generated DOCX packets.
Read STATE.md for project context. Then:

1. Write a Python script that opens every DOCX in the output directory and checks:
   - Document has expected section headings (per document type A/B/C/D)
   - All tables have data (no empty cells where data is expected)
   - Page count is within expected range per document type
   - Tribal Nation name appears correctly in header/title
   - No placeholder text remains (scan for "TODO", "PLACEHOLDER", "TBD", "INSERT")
   - Font consistency (expected fonts: Lexend, League Spartan)
   - Images/logos render (not broken references)

2. Output results to: outputs/docx_qa/structural_audit.json
   Format: { tribe_name, doc_type, checks: [{name, pass, details}] }

3. Generate a summary: outputs/docx_qa/structural_summary.md
   With pass/fail counts and any patterns in failures.

Run the script. Do NOT try to inspect all results in one pass.
Write findings to disk and report the summary only.
```

#### Session 1B: Visual Sampling (Playwright + Screenshot)

```
Prompt for Claude Code:
───────────────────────
I need visual inspection of DOCX output quality. Read STATE.md for context.

Strategy: Convert a STRATIFIED SAMPLE of documents to PDF, then screenshot
each page for visual inspection. Stratify by:
- 1 Tribe per ecoregion (8 Tribes)
- All 4 document types per Tribe (32 documents total)
- Every page of each document

Steps:
1. Select 8 representative Tribes (one per region, mix of data richness)
2. Use LibreOffice headless to convert each DOCX → PDF
3. Use Playwright to render each PDF page as a screenshot
4. Save screenshots to: outputs/docx_qa/visual_samples/{tribe}/{doc_type}/page_{n}.png
5. Create an HTML gallery page that displays all screenshots in a grid
   organized by Tribe → Doc Type → Page for rapid visual scanning
6. Write the gallery to: outputs/docx_qa/visual_gallery.html

Do NOT attempt to visually analyze every screenshot in this session.
Just generate them and the gallery. I will review the gallery myself
and provide feedback in a follow-up session.
```

#### Session 1C: Targeted Revisions

```
Prompt for Claude Code:
───────────────────────
Read STATE.md and outputs/docx_qa/structural_summary.md for context.
[PASTE MY VISUAL REVIEW NOTES HERE]

Fix the following issues found during DOCX inspection:
[specific issues identified from 1A and 1B review]

After fixes, re-run the structural validation script from Session 1A
on ONLY the affected documents to confirm fixes. Write results to
outputs/docx_qa/revision_audit.json.
```

### Phase 1 Design Agents

These are lightweight, focused agents — not full personalities, just checklists with a point of view.

**Agent: Layout Auditor**
- Table column alignment consistency
- Margin/padding uniformity across document types
- Header/footer positioning
- Section break behavior
- Page number accuracy

**Agent: Data Fidelity Checker**
- Tribal Nation names match official federal register
- Program references map to correct agencies
- Dollar amounts formatted consistently
- Date formats consistent (no mixed MM/DD and DD/MM)
- Regional assignments match the 8 ecoregion taxonomy

**Agent: Brand Consistency Reviewer**
- ATNI/NCAI styling guidelines followed
- Lexend for body text, League Spartan for headings
- Color usage matches established palette
- Logo placement and sizing correct
- Professional tone maintained throughout

---

## Phase 2: Website Launch ("The Storefront")

**Goal:** Deploy a production-ready website that connects to real DOCX packets via GitHub Pages/Releases, embedded in SquareSpace, passing WCAG 2.1 AA accessibility.

### Phase 2 is three waves, not one.

#### Wave 2A: Review Swarm (READ-ONLY AUDIT)

Launch the 4-agent swarm from `LAUNCH-SWARM.md`. This is purely diagnostic — no source files are modified. The swarm produces:

- `outputs/website_review/web-wizard_findings.json`
- `outputs/website_review/mind-reader_findings.json`
- `outputs/website_review/keen-kerner_findings.json`
- `outputs/website_review/pipeline-professor_findings.json`
- `outputs/website_review/SYNTHESIS.md`

**Handoff artifact:** The SYNTHESIS.md becomes the work order for Wave 2B.

The 47 known issues are pre-loaded into agent checklists:

| Priority | Category | Count | Blocking? |
|----------|----------|-------|-----------|
| P0 | Fake data pipeline (downloads are text blobs, not DOCX) | 2 | YES |
| P0 | Mock data (46 Tribes / 29 regions vs 592 / 8) | 2 | YES |
| P1 | Accessibility (ARIA, focus trap, keyboard nav, contrast, min sizes) | ~12 | YES |
| P2 | Performance (Canvas O(n²), 30+ unused deps, pre-compiled CSS, fonts) | ~8 | Embed risk |
| P2 | Build (vite.config aliases, dual CSS, unused components) | ~5 | Tech debt |
| P3 | Typography (letter-spacing, display font at body, line measure, scale) | ~8 | Polish |
| Arch | SquareSpace integration (3 approaches evaluated) | 3 | Decision needed |

#### Wave 2B: Implementation (SOURCE MODIFICATIONS)

Execute fixes in priority order from SYNTHESIS.md. Each priority level is a separate Claude Code session to manage context:

**Session 2B-P0: Data Pipeline (Critical Path)**

```
Prompt for Claude Code:
───────────────────────
Read STATE.md, SYNTHESIS.md, and pipeline-professor_findings.json.

The website currently generates FAKE text blob downloads. Fix this:

1. Create a manifest.json that maps Tribe names → DOCX file URLs
   hosted on GitHub Pages or Releases
2. Replace mockData.ts with real data:
   - 592 Tribal Nations (from the pipeline's tribe registry)
   - 8 ecoregions (from the pipeline's regional aggregation)
3. Rewrite TribeSelector.tsx download handler to fetch real DOCX
   from the manifest URL
4. Rewrite RegionSelector.tsx download handler similarly
5. Update generate-packets.yml GitHub Action to:
   - Generate DOCX packets
   - Build manifest.json
   - Deploy to GitHub Pages (or create a Release with assets)

Test with at least 3 Tribes across different regions.
Verify the DOCX that downloads is byte-identical to what the pipeline generates.
```

**Session 2B-P1: Accessibility**

```
Prompt for Claude Code:
───────────────────────
Read SYNTHESIS.md and mind-reader_findings.json.

Fix all WCAG 2.1 AA failures:
1. Add ARIA combobox pattern to TribeSelector autocomplete
2. Add focus trap to DisclaimerModal
3. Implement full keyboard navigation (Tab, Escape, Arrow keys)
4. Fix color contrast ratios (test with axe-core)
5. Replace clamp() values that go below WCAG minimum sizes
6. Add skip-to-content link
7. Ensure all interactive elements have visible focus indicators
8. Add aria-live region for search results count

Run axe-core accessibility audit after fixes. Zero violations at AA level.
```

**Session 2B-P2: Performance & Build**

```
Prompt for Claude Code:
───────────────────────
Read SYNTHESIS.md and web-wizard_findings.json.

1. Replace Canvas particle system with CSS-only alternative
   (or make it opt-in with prefers-reduced-motion respect)
2. Remove unused Radix/shadcn dependencies (keep only the ~5 used)
3. Delete pre-compiled index.css (1424 lines), use only globals.css
   with proper Tailwind build
4. Fix vite.config.ts aliases
5. Implement proper font loading (font-display: swap, preload critical)
6. Remove unused PurposeSection component
7. Build and verify bundle size < 200KB gzipped
8. Test in iframe context (SquareSpace simulation)
```

**Session 2B-P3: Typography**

```
Prompt for Claude Code:
───────────────────────
Read SYNTHESIS.md and keen-kerner_findings.json.

1. Fix letter-spacing: 0.15em → appropriate values per size
2. Move League Spartan to headings only, use system/Lexend for body
3. Establish consistent type scale (use design tokens)
4. Fix line measure in modal (max-width: 65ch)
5. Normalize line-height approach (Tailwind classes, not inline)
6. Consolidate all inline style={{}} typography into CSS classes
```

#### Wave 2C: GitHub Connection & SquareSpace Deploy

```
Prompt for Claude Code:
───────────────────────
Read STATE.md and SYNTHESIS.md (Architecture Decision section).

1. Finalize GitHub Pages deployment:
   - Verify DOCX files are served with correct MIME type
   - Verify CORS headers allow SquareSpace domain
   - Test manifest.json endpoint

2. SquareSpace integration (use recommended approach from SYNTHESIS):
   - Generate the embeddable build artifact
   - Document the exact SquareSpace configuration steps
   - Test in a SquareSpace staging page

3. Create deployment documentation:
   - outputs/deployment/SQUARESPACE-SETUP.md
   - outputs/deployment/GITHUB-PAGES-CONFIG.md
   - outputs/deployment/TROUBLESHOOTING.md
```

---

## Phase 3: Production Hardening ("The Gauntlet")

**Goal:** Stress-test the complete system (pipeline → GitHub → website → download) for reliability under real-world usage (100–200 users over 48 hours).

### The Merge-Sort QA Strategy

Instead of one massive test pass that loses context, use a bottom-up approach inspired by merge sort:

```
Level 0 (atomic):   Test individual components in isolation
                    ├── Tribe search works for all 592 names
                    ├── Region selector maps correctly to 8 regions
                    ├── Each DOCX downloads successfully
                    └── Modal opens/closes/traps focus

Level 1 (pairs):    Test connected pairs
                    ├── Search → Download flow
                    ├── Region select → Download flow
                    ├── Modal → Return to main flow
                    └── Error state → Recovery flow

Level 2 (chains):   Test full user journeys
                    ├── Land → Search → Download → Verify DOCX
                    ├── Land → Region → Download → Verify DOCX
                    ├── Land → Disclaimer → Acknowledge → Search → Download
                    └── Land → Multiple downloads in sequence

Level 3 (stress):   Test under load and edge cases
                    ├── Rapid sequential downloads
                    ├── Browser back/forward during download
                    ├── Mobile viewport (SquareSpace responsive)
                    ├── Slow network simulation
                    └── Concurrent iframe instances
```

Each level must pass completely before moving to the next. Failures at any level get fixed and the level re-runs before proceeding.

### Phase 3 Agent Swarm: The Bug Hunters

These agents run in parallel, each reading STATE.md, the full project structure, and all planning documents before starting. They operate on the BUILT, DEPLOYED artifact — not source code. They are testing the live product.

**Important:** Each agent runs in clone pairs (2 instances of the same agent) to increase coverage through non-deterministic exploration. Different context, same skills, potentially different findings.

---

#### Agent: Mr. Magoo

**Personality:** Jovial, slightly bumbling, approaches everything by feel. Squints at things. Stumbles into bugs that nobody else would find because he takes unexpected paths. Says things like "Oh my, what's this?" and "Well that doesn't feel right, does it?"

**Domain:** Experiential testing — how does the product FEEL to use?

**Method:** Navigate the website the way a non-technical Tribal staff member would. No dev tools. No source code. Just the UI.

**Checklist:**
- [ ] Can I find my Tribe without knowing the exact federal spelling?
- [ ] Does the download button make it obvious what I'm getting?
- [ ] If I download the wrong one, is it easy to try again?
- [ ] Does anything feel slow or janky?
- [ ] Are there any moments of confusion about what to do next?
- [ ] Do error messages help me fix the problem?
- [ ] Does the TSDF disclaimer feel trustworthy or intimidating?
- [ ] On my phone, can I do everything I can do on desktop?
- [ ] If I share this link with a colleague, will they understand it?
- [ ] Does anything feel broken, even if it technically works?

**Output:** `outputs/bug_hunt/mr-magoo_findings.json`

**Clone strategy:** One Magoo instance tests on desktop viewport, one on mobile.

---

#### Agent: Cyclops

**Personality:** Heroic, precise, intense focus. Sees through walls. Laser-targets defects with surgical precision. Speaks with gravitas: "I see the flaw. It is here." Does not waste words.

**Domain:** Deep code inspection — structural integrity, edge cases, failure modes.

**Method:** Read source code function by function. Trace every data path. Find every place where assumptions could break.

**Checklist:**
- [ ] What happens when GitHub Pages is down? (fallback behavior)
- [ ] What happens with Tribal names containing special characters? (apostrophes, hyphens, diacritics)
- [ ] What happens when manifest.json is stale vs pipeline output?
- [ ] Are there race conditions in the search-while-typing flow?
- [ ] Does the download handler clean up on abort/retry?
- [ ] Are there XSS vectors in the Tribe name display?
- [ ] What happens at exactly 592 search results? (boundary)
- [ ] Is the ecoregion mapping injective? (no Tribe in two regions)
- [ ] Does the service worker (if any) cache stale DOCX files?
- [ ] Are error boundaries in place for React component failures?

**Output:** `outputs/bug_hunt/cyclops_findings.json`

**Clone strategy:** One Cyclops traces the happy path exhaustively, one traces every error/edge path.

---

#### Agent: Dale Gribble

**Personality:** Suspicious of everything. Sees conspiracies in clean code. Assumes the worst about every external dependency. Chain-smokes metaphorically while muttering about "the government" and "big framework." But occasionally, his paranoia catches real security issues nobody else thought about.

**Domain:** Security, trust boundaries, data sovereignty compliance.

**Method:** Think like an attacker. Think like a regulatory auditor. Think like a Tribal Leader who needs to trust this system with sensitive information.

**Checklist:**
- [ ] Are DOCX files served over HTTPS with proper headers?
- [ ] Can someone enumerate all 592 Tribal packets by iterating the manifest?
- [ ] Does the SquareSpace embed leak referrer data?
- [ ] Are there analytics/tracking scripts that violate data sovereignty?
- [ ] Is the TSDF T0/T1 classification actually enforced, or just displayed?
- [ ] Could a modified URL download packets for a different Tribe?
- [ ] Are there any third-party CDN dependencies that could be compromised?
- [ ] Does the site work with JavaScript disabled? (graceful degradation)
- [ ] Are GitHub API rate limits a risk for 200 concurrent users?
- [ ] Is there any PII in the DOCX filenames or URLs?
- [ ] Could someone MITM the GitHub Pages → SquareSpace connection?
- [ ] Does the font loading phone home to Google? (privacy)

**Output:** `outputs/bug_hunt/dale-gribble_findings.json`

**Clone strategy:** One Dale audits the client side, one audits the server/deployment side.

---

#### Agent: Marie Kondo

**Personality:** Calm, methodical, grateful. Holds each piece of code and asks: "Does this spark joy?" Finds things that work but are unnecessary — dead code, redundant checks, over-engineered solutions, dependencies that could be removed. Thanks each unnecessary piece before removing it.

**Domain:** Code hygiene, bundle optimization, dependency minimization.

**Method:** Inventory everything. Categorize as essential, useful, or unnecessary. Propose removals with confidence ratings.

**Checklist:**
- [ ] Which npm dependencies are actually imported? (vs just installed)
- [ ] Which React components are rendered? (vs just defined)
- [ ] Which CSS classes are applied? (vs just declared)
- [ ] Are there duplicate utility functions?
- [ ] Are there environment variables that aren't used?
- [ ] Are there GitHub Actions steps that could be combined?
- [ ] Is the build output tree-shaken effectively?
- [ ] Are there test fixtures that duplicate production data?
- [ ] Which config files are actually read by the build?
- [ ] Can any multi-file patterns be consolidated?
- [ ] Does the type system catch what the runtime checks duplicate?
- [ ] Are there polyfills for browsers we don't need to support?

**Output:** `outputs/bug_hunt/marie-kondo_findings.json`

**Clone strategy:** One Marie inventories front-end, one inventories the Python pipeline for deployment artifacts only.

---

### Phase 3 Synthesis

After all bug hunters complete:

```
Prompt for Claude Code:
───────────────────────
Read all files in outputs/bug_hunt/. Synthesize into outputs/bug_hunt/GAUNTLET.md.

Structure:
1. Critical issues (must fix before go-live)
2. Important issues (fix within 24 hours of launch)
3. Nice-to-have improvements
4. Cross-agent agreements (multiple agents found the same issue)
5. Cross-agent conflicts (agents disagree — present both sides)
6. Final go/no-go recommendation with specific conditions

For each issue:
- Which agent(s) found it
- Severity (blocks launch / degrades experience / cosmetic)
- Estimated fix effort (minutes / hours / days)
- Suggested fix approach
```

---

## Phase Sequencing & Dependencies

```
Phase 1 ──────────────────────> Phase 2 ──────────────────────> Phase 3
DOCX QA                        Website                         Hardening

1A: Structural    ─┐           2A: Review Swarm   ─┐          3: Bug Hunt Swarm
    (automated)    │               (4 agents)      │             (4 agents × 2 clones)
                   ├─> 1C Fix      (read-only)     │                    │
1B: Visual Sample ─┘                               │                    v
    (Playwright)              2B-P0: Data Pipeline ─┤          Gauntlet Synthesis
                              2B-P1: Accessibility  │                    │
                              2B-P2: Performance    ├─> 2C Deploy        v
                              2B-P3: Typography    ─┘              GO / NO-GO
```

**Gate criteria between phases:**
- Phase 1 → 2: Zero structural failures in DOCX audit. Visual review approved by Patrick.
- Phase 2A → 2B: SYNTHESIS.md reviewed and prioritized by Patrick.
- Phase 2B → 2C: All P0 and P1 issues resolved. P2/P3 at Patrick's discretion.
- Phase 2 → 3: Website deploys, downloads work for at least 10 test Tribes.
- Phase 3 → Launch: Gauntlet produces go recommendation with no critical issues.

---

## Estimated Timeline

| Phase | Sessions | Est. Duration | Notes |
|-------|----------|---------------|-------|
| 1A | 1 | 30 min | Script writes itself, runtime depends on 992 docs |
| 1B | 1 | 45 min | LibreOffice conversion + Playwright screenshots |
| 1C | 1–3 | Variable | Depends on findings |
| 2A | 1 | 60 min | 4 parallel agents |
| 2B | 4 | 2–3 hours total | One session per priority level |
| 2C | 1 | 45 min | Deploy + test |
| 3 | 1 | 90 min | 8 agent clones in parallel, then synthesis |
| **Total** | **~10–12 sessions** | **~6–8 hours** | Spread across 1–2 working days |

---

## Risk Register

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Context rot during DOCX inspection | High | Medium | Atomic sessions, checklist-driven, findings to disk |
| GitHub Pages CORS blocks SquareSpace | Medium | High | Test early in 2B-P0, have iframe fallback ready |
| 200 concurrent users overwhelm GitHub Pages | Low | High | GitHub Pages has CDN; also DOCX files are static |
| SquareSpace iframe sandboxing blocks downloads | Medium | High | Test download flow in actual SquareSpace sandbox |
| Agent swarm findings conflict | Medium | Low | Synthesis step explicitly handles disagreements |
| Claude Code suggests additional phases | Expected | Positive | Budget 1–2 extra sessions for emergent work |

---

## Files & Artifacts Map

```
outputs/
├── docx_qa/                          # Phase 1
│   ├── structural_audit.json
│   ├── structural_summary.md
│   ├── visual_samples/{tribe}/{doc_type}/
│   ├── visual_gallery.html
│   └── revision_audit.json
├── website_review/                   # Phase 2A
│   ├── web-wizard_findings.json
│   ├── mind-reader_findings.json
│   ├── keen-kerner_findings.json
│   ├── pipeline-professor_findings.json
│   └── SYNTHESIS.md
├── deployment/                       # Phase 2C
│   ├── SQUARESPACE-SETUP.md
│   ├── GITHUB-PAGES-CONFIG.md
│   └── TROUBLESHOOTING.md
└── bug_hunt/                         # Phase 3
    ├── mr-magoo_findings.json        (× 2 clones)
    ├── cyclops_findings.json         (× 2 clones)
    ├── dale-gribble_findings.json    (× 2 clones)
    ├── marie-kondo_findings.json     (× 2 clones)
    └── GAUNTLET.md
```

---

*This plan serves 592 Tribal Nations. Every bug we catch is a Tribal Leader who doesn't lose trust. Every accessibility fix is a colleague who can participate. Small things have big impact.*

---

> **STATUS: SUPERSEDED** -- This operations plan was the pre-execution planning document for v1.3.
> The authoritative execution record is `.planning/ROADMAP.md` (Phases 15-18).
> v1.3 completed 2026-02-14 with GO confirmed.

## Retrospective (Added 2026-02-14)

### Plan vs. Execution

The operations plan described a 3-phase architecture. Execution was 4 phases:

| Planned | Actual | Divergence |
|---------|--------|------------|
| (not planned) | Phase 15: Congressional Intelligence Pipeline | Added phase -- congressional intelligence was not in the original operations plan but became the milestone's highest-value capability |
| Phase 1: DOCX Visual QA | Phase 16: Document Quality Assurance | Evolved from Playwright visual sampling to 5-agent audit swarm with structural validation |
| Phase 2: Website Launch | Phase 17: Website Deployment | Technology pivot: planned React 18 + Vite 6 + Tailwind + shadcn/ui, shipped vanilla HTML/JS/CSS + Fuse.js |
| Phase 3: Production Hardening | Phase 18: Production Hardening | Close match; clone pairs became single instances, but the 2-round audit exceeded the plan's single pass |

### What Worked

1. **Agent swarm methodology.** Parallel agents with non-overlapping territories eliminated merge conflicts and produced comprehensive coverage. The 5-agent Phase 16 audit found 40 findings across 5 quality dimensions that a single reviewer would have missed.
2. **Written findings to disk.** Every agent writes JSON findings, and the synthesis document reconciles them. This survived context window resets perfectly -- no information lost between sessions.
3. **NO-GO/fix/re-audit pattern.** The initial Phase 18 synthesis recommended GO, but the user elected NO-GO to fix all actionable P2/P3. This second round raised trust from 8/10 to 9/10 and resolved all P2 issues. The extra investment paid off in quality.
4. **Checklist-driven agents.** Each bug hunter had a specific personality and checklist. Mr. Magoo (experiential) found UX issues that Cyclops (code inspector) would never notice. Marie Kondo (hygiene) found dead code that Dale Gribble (security) did not care about. Different perspectives found different bugs.
5. **Gate decisions between phases.** Each phase had explicit success criteria that had to be verified before the next phase started. This prevented premature progression.

### What Would Be Done Differently

1. **Plan should have anticipated Congressional Intelligence.** The operations plan was written before requirements were finalized. Congressional intelligence was the milestone's largest phase (7 plans, 15 INTEL + 3 XCUT requirements) but appeared nowhere in the plan. Next time, finalize requirements before writing the operations plan.
2. **Technology decision (React vs. vanilla) should have been made earlier.** The plan and requirements reference React/Vite/Tailwind, but the actual implementation used vanilla HTML/JS/CSS. This was the right call (sovereignty compliance, no build dependencies, 65KB vs hundreds of KB) but the decision should have been documented in the plan rather than discovered during execution.
3. **Clone pairs were unnecessary.** The plan called for 2 instances of each bug hunter for coverage. In practice, single instances produced thorough coverage -- 70 findings across 4 agents was more than sufficient.
4. **Visual sampling (Playwright screenshots) was never needed.** The plan's Phase 1 Session 1B described Playwright-based visual inspection. The structural validation script (641 LOC, 7 automated checks) plus the 5-agent audit provided sufficient quality assurance without visual screenshots. Automated structural checks scale better than visual inspection.
5. **Timeline estimate was close but compressed.** The plan estimated 6-8 hours across 10-12 sessions. Actual execution was approximately 3 days with more sessions but similar active time. The additional phase (congressional intelligence) and the NO-GO remediation cycle added scope not anticipated in the estimate.

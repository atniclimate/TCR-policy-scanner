---
phase: 20-flood-water-data
plan: 07
type: execute
wave: 3
depends_on: ["20-02", "20-03", "20-04", "20-05", "20-06"]
files_modified:
  - scripts/populate_flood_data.py
  - tests/test_populate_flood_data.py
  - src/paths.py
autonomous: true

must_haves:
  truths:
    - "CLI orchestrator runs all 6 flood builders in sequence with progress logging"
    - "Individual builders can be run selectively via CLI flags"
    - "Combined FloodProfile JSON written per Tribe after all builders complete"
    - "Coverage report shows per-source breakdown: data/not-applicable/monitoring-gap for 592 Tribes"
    - "All 6 flood data sources route through existing county FIPS crosswalk (no new geographic files)"
    - "Every API fetch logged with timestamp, URL, params, response size, HTTP status"
  artifacts:
    - path: "scripts/populate_flood_data.py"
      provides: "CLI orchestrator for all 6 flood builders"
      contains: "def main"
    - path: "tests/test_populate_flood_data.py"
      provides: "Integration tests for orchestrator and combined profile assembly"
      contains: "test_combine_flood_profiles"
  key_links:
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_nfip.py"
      via: "NFIPBuilder import"
      pattern: "from src.packets.flood_nfip import NFIPBuilder"
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_declarations.py"
      via: "DeclarationsBuilder import"
      pattern: "from src.packets.flood_declarations import DeclarationsBuilder"
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_nfhl.py"
      via: "NFHLBuilder import"
      pattern: "from src.packets.flood_nfhl import NFHLBuilder"
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_streamflow.py"
      via: "StreamflowBuilder import"
      pattern: "from src.packets.flood_streamflow import StreamflowBuilder"
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_precipitation.py"
      via: "PrecipitationBuilder import"
      pattern: "from src.packets.flood_precipitation import PrecipitationBuilder"
    - from: "scripts/populate_flood_data.py"
      to: "src/packets/flood_coastal.py"
      via: "CoastalBuilder import"
      pattern: "from src.packets.flood_coastal import CoastalBuilder"
---

<objective>
Build the CLI orchestrator script that runs all 6 flood builders, combines per-source results into unified FloodProfile JSON per Tribe, and generates a comprehensive coverage report with 3-way classification (data / not applicable / monitoring gap).

Purpose: This is the user-facing entry point for populating flood data. It coordinates all 6 builders and produces the final per-Tribe flood cache files that Phase 22 (vulnerability profiles) will consume. The coverage report validates Phase 20 success criteria.

Output: `scripts/populate_flood_data.py` (CLI orchestrator) and `tests/test_populate_flood_data.py` (integration tests).
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@scripts/populate_hazards.py -- existing population script pattern (CLI, argparse, logging)
@src/packets/flood_nfip.py -- NFIPBuilder (from Plan 02)
@src/packets/flood_declarations.py -- DeclarationsBuilder (from Plan 03)
@src/packets/flood_nfhl.py -- NFHLBuilder (from Plan 04)
@src/packets/flood_streamflow.py -- StreamflowBuilder (from Plan 05)
@src/packets/flood_precipitation.py -- PrecipitationBuilder (from Plan 06)
@src/packets/flood_coastal.py -- CoastalBuilder (from Plan 06)
@src/packets/_flood_common.py -- classify_tribe_coastal, load_coastal_counties (from Plan 01)
@src/schemas/flood.py -- FloodProfile (from Plan 01)
@src/paths.py -- FLOOD_CACHE_DIR, TRIBAL_REGISTRY_PATH
@.planning/phases/20-flood-water-data/20-01-SUMMARY.md
@.planning/phases/20-flood-water-data/20-02-SUMMARY.md
@.planning/phases/20-flood-water-data/20-03-SUMMARY.md
@.planning/phases/20-flood-water-data/20-04-SUMMARY.md
@.planning/phases/20-flood-water-data/20-05-SUMMARY.md
@.planning/phases/20-flood-water-data/20-06-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement CLI orchestrator script</name>
  <files>scripts/populate_flood_data.py</files>
  <action>
    Create `scripts/populate_flood_data.py` following the pattern of existing population scripts (e.g., populate_hazards.py).

    **CLI interface:**
    ```python
    import argparse
    import json
    import logging
    import sys
    import time
    from datetime import datetime, timezone
    from pathlib import Path

    # Add project root to path for imports
    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

    from src.packets.flood_nfip import NFIPBuilder
    from src.packets.flood_declarations import DeclarationsBuilder
    from src.packets.flood_nfhl import NFHLBuilder
    from src.packets.flood_streamflow import StreamflowBuilder
    from src.packets.flood_precipitation import PrecipitationBuilder
    from src.packets.flood_coastal import CoastalBuilder
    from src.packets._flood_common import classify_tribe_coastal, load_coastal_counties
    from src.packets._geo_common import atomic_write_json, load_aiannh_crosswalk, load_area_weights
    from src.schemas.flood import FloodProfile, CoastalType
    from src.paths import (
        FLOOD_CACHE_DIR, COASTAL_COUNTIES_PATH, AIANNH_CROSSWALK_PATH,
        TRIBAL_COUNTY_WEIGHTS_PATH, TRIBAL_REGISTRY_PATH, OUTPUTS_DIR,
    )

    logger = logging.getLogger("populate_flood_data")

    def parse_args():
        parser = argparse.ArgumentParser(
            description="Populate flood and water data for all 592 Tribes"
        )
        parser.add_argument(
            "--sources", nargs="+",
            choices=["nfip", "declarations", "nfhl", "streamflow", "precipitation", "coastal", "all"],
            default=["all"],
            help="Which flood data sources to populate (default: all)"
        )
        parser.add_argument(
            "--combine-only", action="store_true",
            help="Skip fetching, only combine existing per-source files into FloodProfiles"
        )
        parser.add_argument(
            "--coverage-only", action="store_true",
            help="Skip fetching, only generate coverage report from existing data"
        )
        parser.add_argument(
            "--dry-run", action="store_true",
            help="Log what would be fetched without making API calls"
        )
        parser.add_argument(
            "--log-level", default="INFO",
            choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        )
        return parser.parse_args()
    ```

    **Main orchestration flow:**
    ```python
    def main():
        args = parse_args()
        logging.basicConfig(level=getattr(logging, args.log_level))

        sources = set(args.sources)
        run_all = "all" in sources

        config = {}  # Builder config (could load from scanner_config.json)

        start_time = time.time()
        results = {}

        # Phase 1: Run individual builders
        if not args.combine_only and not args.coverage_only:
            if run_all or "nfip" in sources:
                logger.info("=== FLOOD-01: NFIP Claims ===")
                builder = NFIPBuilder(config)
                results["nfip"] = builder.build_all_profiles()

            if run_all or "declarations" in sources:
                logger.info("=== FLOOD-02: Disaster Declarations ===")
                builder = DeclarationsBuilder(config)
                results["declarations"] = builder.build_all_profiles()

            if run_all or "nfhl" in sources:
                logger.info("=== FLOOD-03: NFHL Community Status ===")
                builder = NFHLBuilder(config)
                results["nfhl"] = builder.build_all_profiles()

            if run_all or "streamflow" in sources:
                logger.info("=== FLOOD-04: USGS Streamflow ===")
                builder = StreamflowBuilder(config)
                results["streamflow"] = builder.build_all_profiles()

            if run_all or "precipitation" in sources:
                logger.info("=== FLOOD-05: Atlas 14 Precipitation ===")
                builder = PrecipitationBuilder(config)
                results["precipitation"] = builder.build_all_profiles()

            if run_all or "coastal" in sources:
                logger.info("=== FLOOD-06: NOAA CO-OPS Coastal ===")
                builder = CoastalBuilder(config)
                results["coastal"] = builder.build_all_profiles()

        # Phase 2: Combine per-source files into unified FloodProfile per Tribe
        if not args.coverage_only:
            combine_flood_profiles()

        # Phase 3: Generate coverage report
        report = generate_coverage_report()

        elapsed = time.time() - start_time
        logger.info(
            "Flood data population complete in %.1f minutes. "
            "Results: %s",
            elapsed / 60, results,
        )
    ```

    **combine_flood_profiles() function:**
    - Load tribal registry for Tribe names
    - Load coastal classification for each Tribe
    - For each of 592 Tribes:
      - Check for per-source files: {tribe_id}_nfip.json, _declarations.json, _nfhl.json, _streamflow.json, _precipitation.json, _coastal.json
      - Load whichever exist
      - Combine into a single FloodProfile Pydantic model
      - Count data_sources_available (how many of the 6 have data)
      - Set coastal_type from classification
      - generated_at = current ISO timestamp
      - Validate against FloodProfile schema
      - Write to FLOOD_CACHE_DIR / f"{tribe_id}.json" (unified file)
    - Log: X Tribes with combined profiles, Y with 6/6 sources, Z with 0 sources

    **generate_coverage_report() function:**
    Per 20-CONTEXT.md and success criteria #4:
    - For each of 6 sources, classify all 592 Tribes into:
      - **Has data**: Profile exists and has actual data
      - **Not applicable**: Source doesn't apply (coastal data for inland Tribe)
      - **Monitoring gap**: Source should have data but doesn't
    - Per-source breakdown table
    - Overall: % of Tribes with at least 1 source, with all 6, etc.
    - Write JSON to OUTPUTS_DIR / "flood_coverage_report.json"
    - Write markdown to OUTPUTS_DIR / "flood_coverage_report.md"

    **Coverage report format (markdown):**
    ```markdown
    # Flood Data Coverage Report
    Generated: {timestamp}

    ## Summary
    - 592 Tribes total
    - {X} Tribes with at least one flood data source ({pct}%)
    - {Y} Tribes with all applicable sources

    ## Per-Source Coverage

    | Source | Has Data | Not Applicable | Monitoring Gap | Total |
    |--------|----------|----------------|----------------|-------|
    | NFIP Claims | {n} ({pct}%) | 0 | {n} ({pct}%) | 592 |
    | Declarations | {n} ({pct}%) | 0 | {n} ({pct}%) | 592 |
    | NFHL Status | {n} ({pct}%) | 0 | {n} ({pct}%) | 592 |
    | Streamflow | {n} ({pct}%) | 0 | {n} ({pct}%) | 592 |
    | Precipitation | {n} ({pct}%) | 0 | {n} ({pct}%) | 592 |
    | Coastal | {n} ({pct}%) | {n} ({pct}%) | {n} ({pct}%) | 592 |

    ## Coastal Classification
    - Marine: {n} Tribes
    - Lacustrine: {n} Tribes
    - Inland: {n} Tribes
    ```

    Use `logging.getLogger(__name__)` for logging. All file operations use `encoding="utf-8"`.
  </action>
  <verify>
    Run: `python scripts/populate_flood_data.py --help` -- shows usage.
    Run: `python scripts/populate_flood_data.py --dry-run --sources nfip` -- dry run logs without API calls.
    Run: `python -c "from scripts.populate_flood_data import combine_flood_profiles, generate_coverage_report; print('Import OK')"` -- functions importable.
  </verify>
  <done>
    CLI orchestrator runs all 6 builders with selective execution via --sources flag. Combines per-source files into unified FloodProfile JSON. Coverage report shows 3-way classification (data/not-applicable/monitoring-gap) per source.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write orchestrator integration tests</name>
  <files>tests/test_populate_flood_data.py</files>
  <action>
    Create `tests/test_populate_flood_data.py` with integration tests.

    **Test fixtures:**
    - `sample_nfip_profile`: Pre-built NFIPProfile dict
    - `sample_declarations_profile`: Pre-built DisasterDeclarationsProfile dict
    - `sample_nfhl_profile`: Pre-built NFHLProfile dict
    - `sample_streamflow_profile`: Pre-built StreamflowProfile dict
    - `sample_precipitation_profile`: Pre-built PrecipitationProfile dict
    - `sample_coastal_profile`: Pre-built CoastalProfile dict
    - `sample_registry`: Minimal tribal registry with 3 Tribes

    **TestCombineFloodProfiles:**
    - test_combine_all_six_sources: All 6 per-source files exist -> FloodProfile with data_sources_available=6
    - test_combine_partial_sources: Only 3 source files exist -> FloodProfile with data_sources_available=3
    - test_combine_no_sources: No source files -> FloodProfile with data_sources_available=0
    - test_combine_coastal_type: coastal_type set from classification
    - test_combine_pydantic_validation: Combined profile passes FloodProfile schema
    - test_combine_generated_at: Timestamp set correctly

    **TestCoverageReport:**
    - test_coverage_report_all_sources: Report includes all 6 sources
    - test_coverage_report_three_way_classification: Has data + not applicable + monitoring gap = 592
    - test_coverage_report_coastal_not_applicable: Inland Tribes counted as not_applicable for coastal
    - test_coverage_report_json_output: JSON file written with correct structure
    - test_coverage_report_markdown_output: Markdown file written with table format

    **TestCLI:**
    - test_argparse_default: Default sources=["all"]
    - test_argparse_selective: --sources nfip streamflow -> only 2 sources
    - test_argparse_combine_only: --combine-only flag respected
    - test_argparse_coverage_only: --coverage-only flag respected
    - test_argparse_dry_run: --dry-run flag respected

    **TestFloodProfileSchema:**
    - test_flood_profile_all_crosswalk_routed: All 6 sources use county FIPS crosswalk (no new geographic files)
    - test_flood_profile_provenance: Every sub-profile has metadata with fetch timestamp and URL

    Use `tmp_path` for file system tests. Use `unittest.mock.patch` for builder mocking.
  </action>
  <verify>
    Run: `python -m pytest tests/test_populate_flood_data.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass.
  </verify>
  <done>
    20+ integration tests cover profile combination, 3-way coverage classification, CLI argument parsing, provenance metadata, and crosswalk-only routing. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/populate_flood_data.py --help` -- shows valid CLI usage with all flags
2. `python -m pytest tests/test_populate_flood_data.py -v` -- all tests pass
3. `python -m pytest tests/ -x -q` -- all 1257+ tests pass
4. Coverage report includes all 6 sources with 3-way classification per source
</verification>

<success_criteria>
- CLI orchestrator with --sources, --combine-only, --coverage-only, --dry-run flags
- All 6 builders callable individually or together
- Combined FloodProfile JSON per Tribe with data_sources_available count
- Coverage report with 3-way classification: data / not applicable / monitoring gap
- All sources route through county FIPS crosswalk (verified by test)
- API fetch logging: every fetch logged with timestamp, URL, params, response size, HTTP status
- 20+ integration tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-07-SUMMARY.md`
</output>

---
phase: 20-flood-water-data
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/schemas/flood.py
  - src/packets/_flood_common.py
  - src/paths.py
  - scripts/download_flood_reference.py
  - tests/test_flood_schemas.py
  - tests/test_flood_common.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "All 6 flood data types have Pydantic v2 models importable from src.schemas.flood"
    - "Coastal county classification returns marine/lacustrine/inland for any county FIPS"
    - "OpenFEMA paginated fetch retrieves all records across multiple pages"
    - "Flood path constants exist in src/paths.py for all flood cache directories"
    - "Reference data download script fetches coastal counties JSON and NWS gauge CSV"
    - "dataretrieval package is in requirements.txt"
  artifacts:
    - path: "src/schemas/flood.py"
      provides: "Pydantic v2 models for all 6 flood data types"
      contains: "class NFIPProfile"
    - path: "src/packets/_flood_common.py"
      provides: "Shared HTTP helpers, rate limiting, coastal classification, FIPS normalization"
      contains: "def fetch_openfema_paginated"
    - path: "src/paths.py"
      provides: "FLOOD_CACHE_DIR and other flood-related path constants"
      contains: "FLOOD_CACHE_DIR"
    - path: "scripts/download_flood_reference.py"
      provides: "One-time download of coastal counties and NWS gauge reference files"
      contains: "def download_coastal_counties"
    - path: "tests/test_flood_schemas.py"
      provides: "Schema validation tests for all 6 flood Pydantic models"
      contains: "test_nfip_profile_valid"
    - path: "tests/test_flood_common.py"
      provides: "Tests for shared HTTP, coastal classification, FIPS helpers"
      contains: "test_classify_tribe_coastal"
  key_links:
    - from: "src/schemas/flood.py"
      to: "src/schemas/vulnerability.py"
      via: "DataGapType import for gap classification"
      pattern: "from src.schemas.vulnerability import DataGapType"
    - from: "src/packets/_flood_common.py"
      to: "src/paths.py"
      via: "FLOOD_CACHE_DIR and reference file paths"
      pattern: "from src.paths import"
    - from: "src/packets/_flood_common.py"
      to: "src/packets/_geo_common.py"
      via: "atomic_write_json reuse"
      pattern: "from src.packets._geo_common import"
---

<objective>
Build the Pydantic v2 schemas for all 6 flood data types and the shared flood infrastructure module -- the foundation that every subsequent flood builder depends on.

Purpose: Every builder plan (20-02 through 20-06) imports from these two modules. Schemas enforce data contracts; _flood_common provides the HTTP helpers, rate limiting, coastal classification, and FIPS normalization that all builders share.

Output: `src/schemas/flood.py` (all 6 data type models), `src/packets/_flood_common.py` (shared infrastructure), flood path constants in `src/paths.py`, reference data download script, and comprehensive test suites for both.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@src/schemas/vulnerability.py -- DataGapType enum, Pydantic model patterns (field_validator, model_validator)
@src/packets/_geo_common.py -- atomic_write_json, load_aiannh_crosswalk, load_area_weights (reuse patterns)
@src/packets/hazards.py -- _safe_float() import pattern, builder template
@src/paths.py -- existing path constants pattern (add flood paths here)
@requirements.txt -- add dataretrieval dependency
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create flood Pydantic schemas and path constants</name>
  <files>src/schemas/flood.py, src/paths.py, tests/test_flood_schemas.py</files>
  <action>
    **1a. Create `src/schemas/flood.py`** with Pydantic v2 models for all 6 flood data types. Follow the pattern from `src/schemas/vulnerability.py` (field_validator, model_validator, descriptive Field annotations).

    **Models to create:**

    ```python
    # Shared enum
    class CoastalType(StrEnum):
        MARINE = "marine"
        LACUSTRINE = "lacustrine"
        INLAND = "inland"

    class FloodDataGapReason(StrEnum):
        """Flood-specific data gap reasons -- two categories per 20-CONTEXT.md."""
        NOT_APPLICABLE = "not_applicable"  # Source doesn't apply (e.g., coastal for inland)
        MONITORING_GAP = "monitoring_gap"  # Source SHOULD have data but doesn't exist
        API_ERROR = "api_error"           # Fetch failed
        NO_RECORDS = "no_records"         # Query succeeded but returned zero results

    class FloodSourceMetadata(BaseModel):
        """Provenance metadata attached to every flood data record."""
        source_name: str  # e.g., "OpenFEMA NFIP Claims v2"
        source_url: str   # API endpoint queried
        query_params: dict = Field(default_factory=dict)
        fetch_timestamp: str  # ISO 8601
        record_count: int = Field(ge=0)
        http_status: int = Field(ge=100, le=599)

    # FLOOD-01: NFIP
    class NFIPClaimsSummary(BaseModel):
        """Aggregated NFIP claims for a county."""
        county_fips: str = Field(..., min_length=5, max_length=5)
        total_claims: int = Field(ge=0)
        total_paid_building: float = Field(ge=0.0)
        total_paid_contents: float = Field(ge=0.0)
        total_paid_icc: float = Field(ge=0.0)  # Increased Cost of Compliance
        total_paid_all: float = Field(ge=0.0)
        year_range: tuple[int, int] | None = None  # (earliest_year, latest_year)
        flood_zone_distribution: dict[str, int] = Field(default_factory=dict)
        # e.g., {"AE": 45, "A": 12, "X": 3}

    class NFIPProfile(BaseModel):
        """Per-Tribe NFIP flood insurance profile."""
        tribe_id: str
        county_summaries: list[NFIPClaimsSummary]
        total_claims: int = Field(ge=0)
        total_paid: float = Field(ge=0.0)
        area_weighted_claims_per_year: float = Field(ge=0.0)
        dominant_flood_zone: str | None = None
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None
        # tribe_id validator

    # FLOOD-02: Disaster Declarations
    class DisasterDeclaration(BaseModel):
        """Single disaster declaration record."""
        disaster_number: int
        declaration_date: str  # ISO date
        incident_type: str  # e.g., "Flood", "Hurricane", "Fire"
        declaration_title: str
        incident_begin_date: str | None = None
        incident_end_date: str | None = None
        ia_program: bool = False
        pa_program: bool = False
        hm_program: bool = False
        # Dollar amounts from PA/HMGP joins (may be None if not yet joined)
        pa_total: float | None = None
        hmgp_total: float | None = None

    class DisasterDeclarationsProfile(BaseModel):
        """Per-Tribe disaster declarations profile."""
        tribe_id: str
        declarations: list[DisasterDeclaration]
        total_declarations: int = Field(ge=0)
        declarations_by_type: dict[str, int] = Field(default_factory=dict)
        date_range_start: str | None = None  # ISO date
        date_range_end: str | None = None
        total_pa_dollars: float | None = None
        total_hmgp_dollars: float | None = None
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None

    # FLOOD-03: NFHL Community Status
    class NFHLCommunityStatus(BaseModel):
        """NFIP community participation status for a community."""
        community_id: str
        community_name: str
        county: str
        state: str
        participating: bool
        crs_class: int | None = None  # CRS rating 1-10
        sfha_discount_pct: float | None = None
        non_sfha_discount_pct: float | None = None
        effective_map_date: str | None = None  # ISO date
        is_tribal: bool = False

    class NFHLProfile(BaseModel):
        """Per-Tribe NFHL flood zone and community status profile."""
        tribe_id: str
        communities: list[NFHLCommunityStatus]
        any_participating: bool = False
        flood_zone_distribution: dict[str, float] = Field(default_factory=dict)
        # Zone -> pct of claims in that zone (from NFIP claims supplement)
        oldest_map_date: str | None = None
        newest_map_date: str | None = None
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None

    # FLOOD-04: USGS Streamflow
    class StreamflowGauge(BaseModel):
        """USGS streamflow monitoring location."""
        site_no: str
        site_name: str
        latitude: float
        longitude: float
        county_fips: str = Field(min_length=5, max_length=5)
        drainage_area_sqmi: float | None = None
        contributing_drainage_area_sqmi: float | None = None
        period_of_record_start: str | None = None  # ISO date
        period_of_record_end: str | None = None
        peak_flows: list[dict] = Field(default_factory=list)
        # Each: {date, peak_discharge_cfs, gage_height_ft}
        recent_daily_mean_cfs: list[dict] = Field(default_factory=list)
        # Each: {date, value_cfs}
        nws_flood_stage_ft: float | None = None
        nws_action_stage_ft: float | None = None
        nws_moderate_flood_stage_ft: float | None = None
        nws_major_flood_stage_ft: float | None = None

    class StreamflowProfile(BaseModel):
        """Per-Tribe USGS streamflow profile."""
        tribe_id: str
        gauges: list[StreamflowGauge]
        gauge_count: int = Field(ge=0)
        max_peak_discharge_cfs: float | None = None
        max_peak_gauge: str | None = None  # site_no of gauge with highest peak
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None
        raw_api_responses: dict = Field(default_factory=dict)
        # Store raw responses keyed by site_no for debugging/reprocessing

    # FLOOD-05: NOAA Atlas 14 Precipitation
    class PrecipitationFrequency(BaseModel):
        """Precipitation frequency estimate for a location."""
        county_fips: str = Field(min_length=5, max_length=5)
        latitude: float
        longitude: float
        # Return period depths in inches for 24-hour duration
        depth_10yr_24hr: float | None = None
        depth_25yr_24hr: float | None = None
        depth_50yr_24hr: float | None = None
        depth_100yr_24hr: float | None = None

    class PrecipitationProfile(BaseModel):
        """Per-Tribe NOAA Atlas 14 precipitation profile."""
        tribe_id: str
        county_estimates: list[PrecipitationFrequency]
        area_weighted_100yr_24hr: float | None = None  # inches
        area_weighted_25yr_24hr: float | None = None
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None

    # FLOOD-06: NOAA CO-OPS Coastal
    class CoastalStation(BaseModel):
        """NOAA CO-OPS water level station."""
        station_id: str
        station_name: str
        latitude: float
        longitude: float
        state: str
        county_fips: str | None = None  # Assigned by proximity
        minor_flood_level_ft: float | None = None
        moderate_flood_level_ft: float | None = None
        major_flood_level_ft: float | None = None
        mean_sea_level_trend_mm_yr: float | None = None
        days_above_minor_flood: int | None = None  # In 20-year record
        highest_observed_ft: float | None = None
        highest_observed_date: str | None = None

    class CoastalProfile(BaseModel):
        """Per-Tribe NOAA CO-OPS coastal water level profile."""
        tribe_id: str
        coastal_type: CoastalType
        stations: list[CoastalStation]
        station_count: int = Field(ge=0)
        max_flood_days: int | None = None  # Highest days_above_minor across stations
        sea_level_trend_mm_yr: float | None = None  # Average across stations
        coverage_pct: float = Field(ge=0.0, le=1.0)
        data_gap: FloodDataGapReason | None = None
        gap_advocacy_action: str | None = None
        metadata: FloodSourceMetadata | None = None

    # Top-level container for all flood data per Tribe
    class FloodProfile(BaseModel):
        """Per-Tribe comprehensive flood data profile.

        Top-level container combining all 6 flood data sources.
        Written to data/flood_cache/{tribe_id}.json.
        """
        tribe_id: str
        tribe_name: str
        coastal_type: CoastalType = CoastalType.INLAND
        nfip: NFIPProfile | None = None
        declarations: DisasterDeclarationsProfile | None = None
        nfhl: NFHLProfile | None = None
        streamflow: StreamflowProfile | None = None
        precipitation: PrecipitationProfile | None = None
        coastal: CoastalProfile | None = None
        generated_at: str  # ISO 8601
        data_sources_available: int = Field(ge=0, le=6)
        data_sources_total: int = Field(default=6)
    ```

    Add `tribe_id` validators (must start with "epa_") to all Profile models using the `_validate_epa_tribe_id` pattern from vulnerability.py. Import it: `from src.schemas.vulnerability import _validate_epa_tribe_id`.

    Add `county_fips` validator (exactly 5 digits, zero-padded) to all models with county_fips fields:
    ```python
    @field_validator("county_fips")
    @classmethod
    def validate_county_fips(cls, v: str) -> str:
        if len(v) != 5 or not v.isdigit():
            raise ValueError(f"county_fips must be 5 digits, got '{v}'")
        return v
    ```

    **1b. Add flood path constants to `src/paths.py`:**

    Add these constants in a new section after the Vulnerability Data Paths section:
    ```python
    # -- Flood Data Paths --
    FLOOD_CACHE_DIR: Path = DATA_DIR / "flood_cache"
    """Per-Tribe flood data JSON files (592 files)."""

    FLOOD_REFERENCE_DIR: Path = DATA_DIR / "reference"
    """Static reference files for flood data pipeline."""

    COASTAL_COUNTIES_PATH: Path = FLOOD_REFERENCE_DIR / "coastal_counties.json"
    """NOAA coastal county classification (FIPS -> marine/lacustrine)."""

    NWPS_GAUGES_PATH: Path = FLOOD_REFERENCE_DIR / "nwps_gauges.csv"
    """NWS NWPS gauge metadata with flood stage thresholds."""

    SVI_PROFILES_DIR: Path = SVI_DIR / "profiles"
    """Per-Tribe SVI profile JSON files (from Phase 19)."""
    ```

    Add helper function:
    ```python
    def flood_cache_path(tribe_id: str) -> Path:
        """Return the flood cache JSON path for a specific Tribe."""
        return FLOOD_CACHE_DIR / f"{tribe_id}.json"
    ```

    **1c. Add `dataretrieval` to `requirements.txt`:**
    Add `dataretrieval>=1.1.0` to the requirements file (needed for USGS peak flows in FLOOD-04).

    **1d. Create `tests/test_flood_schemas.py`:**

    Test every model:
    - test_nfip_claims_summary_valid: Valid county data passes
    - test_nfip_claims_summary_bad_fips: Non-5-digit FIPS raises
    - test_nfip_profile_valid: Full profile with county summaries passes
    - test_nfip_profile_bad_tribe_id: tribe_id without "epa_" raises
    - test_disaster_declaration_valid: Single declaration passes
    - test_declarations_profile_valid: Full declarations profile passes
    - test_nfhl_community_valid: Community status passes
    - test_nfhl_profile_valid: Full NFHL profile passes
    - test_streamflow_gauge_valid: Gauge with peaks and daily data passes
    - test_streamflow_profile_valid: Profile with multiple gauges passes
    - test_precipitation_frequency_valid: County estimate passes
    - test_precipitation_profile_valid: Full precip profile passes
    - test_coastal_station_valid: Station with flood levels passes
    - test_coastal_profile_valid: Full coastal profile passes
    - test_flood_profile_complete: Top-level FloodProfile with all 6 sub-profiles
    - test_flood_profile_partial: FloodProfile with only some sub-profiles (partial data)
    - test_coastal_type_enum: All 3 values (marine, lacustrine, inland) validate
    - test_flood_data_gap_reason_enum: All 4 gap reasons validate
    - test_source_metadata_valid: FloodSourceMetadata with all fields
    - test_coverage_pct_bounds: Values outside [0, 1] rejected
  </action>
  <verify>
    Run: `python -c "from src.schemas.flood import FloodProfile, NFIPProfile, CoastalType; print('Import OK')"` -- succeeds.
    Run: `python -c "from src.paths import FLOOD_CACHE_DIR, flood_cache_path; print(FLOOD_CACHE_DIR)"` -- prints path.
    Run: `python -m pytest tests/test_flood_schemas.py -v` -- all tests pass.
  </verify>
  <done>
    All 6 flood data type Pydantic models exist and validate correctly. Path constants for flood cache and reference data exist. dataretrieval added to requirements. 20+ schema tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create shared flood infrastructure module and reference data downloader</name>
  <files>src/packets/_flood_common.py, scripts/download_flood_reference.py, tests/test_flood_common.py</files>
  <action>
    **2a. Create `src/packets/_flood_common.py`:**

    This module provides shared infrastructure used by ALL 6 flood builders.

    **OpenFEMA paginated fetch:**
    ```python
    OPENFEMA_BASE_URL = "https://www.fema.gov/api/open"
    OPENFEMA_MAX_PER_PAGE = 1000
    OPENFEMA_RATE_LIMIT_DELAY = 0.5  # seconds between requests

    def fetch_openfema_paginated(
        endpoint: str,
        api_version: str,  # "v1" or "v2"
        filter_expr: str,
        select_fields: list[str] | None = None,
        max_records: int = 100_000,
    ) -> tuple[list[dict], FloodSourceMetadata]:
        """Paginated OpenFEMA fetch with metadata tracking.

        Returns (records, metadata) tuple. Logs every fetch
        with timestamp, URL, params, response size, HTTP status.
        """
    ```
    Use `requests.get()` (synchronous). Handle HTTP errors gracefully (log + return empty list, don't crash). Return `FloodSourceMetadata` with provenance info. Use `time.sleep(OPENFEMA_RATE_LIMIT_DELAY)` between pages.

    **FIPS normalization:**
    ```python
    def normalize_county_fips(fips: str | int) -> str:
        """Normalize any county FIPS to 5-digit zero-padded string."""
        return str(fips).strip().zfill(5)

    def split_county_fips(fips_5: str) -> tuple[str, str]:
        """Split 5-digit FIPS into (state_2, county_3)."""
        return fips_5[:2], fips_5[2:]
    ```

    **Coastal county classification:**
    ```python
    GREAT_LAKES_STATES = frozenset({"MN", "WI", "IL", "IN", "MI", "OH", "PA", "NY"})

    def load_coastal_counties(ref_path: Path) -> dict[str, str]:
        """Load coastal counties reference -> {fips: "marine"|"lacustrine"}.

        File format: {"counties": {"01003": "marine", "26005": "lacustrine", ...}}
        Falls back to empty dict with warning if file not found.
        """

    def classify_tribe_coastal(
        tribe_county_fips: list[str],
        coastal_ref: dict[str, str],
    ) -> CoastalType:
        """Classify Tribe as marine/lacustrine/inland per 20-CONTEXT.md.

        Rules:
        - "Coastal wins": if ANY county is coastal, Tribe is coastal
        - Marine > lacustrine priority (if both present, marine wins)
        - Default: inland
        """
    ```

    **Crosswalk helpers:**
    ```python
    def get_unique_county_fips_from_crosswalk(
        area_weights: dict[str, list[dict]],
        tribe_to_geoids: dict[str, list[str]],
    ) -> set[str]:
        """Extract all unique county FIPS codes from the crosswalk.

        Used to deduplicate API queries -- query each county ONCE,
        then distribute results to Tribes.
        """

    def get_tribe_county_weights(
        tribe_id: str,
        tribe_to_geoids: dict[str, list[str]],
        area_weights: dict[str, list[dict]],
    ) -> dict[str, float]:
        """Get county FIPS -> weight mapping for a single Tribe.

        Sums weights when multiple GEOIDs map to the same county.
        Normalizes to sum=1.0 across matched counties.
        Returns empty dict if no counties found.
        """
    ```

    **Monitoring gap advocacy actions (per 20-CONTEXT.md, Claude's discretion):**
    ```python
    MONITORING_GAP_ACTIONS: dict[str, str] = {
        "nfip": (
            "Contact FEMA Region to explore NFIP participation and "
            "flood insurance affordability programs"
        ),
        "declarations": (
            "Request FEMA review of historical disaster impacts that may "
            "not have received formal declarations"
        ),
        "nfhl": (
            "Contact FEMA Mapping to ensure community flood maps are "
            "current and Tribal lands are properly mapped"
        ),
        "streamflow": (
            "Advocate for USGS stream gauge installation through the "
            "Cooperative Water Program or Bureau of Indian Affairs "
            "water resources monitoring"
        ),
        "precipitation": (
            "Request inclusion in next NOAA Atlas update cycle for "
            "improved local precipitation frequency estimates"
        ),
        "coastal": None,  # Not applicable framing, not monitoring gap
    }
    ```

    Use `logging.getLogger(__name__)` for all logging. Import `_safe_float` from `src.packets.hazards`. Import `atomic_write_json` from `src.packets._geo_common`.

    **2b. Create `scripts/download_flood_reference.py`:**

    Script to download static reference files that change rarely:

    1. **Coastal counties JSON** (`data/reference/coastal_counties.json`):
       - Build from NOAA ENOW coastal counties list
       - Since the official source is a PDF, provide a hardcoded dictionary of ~452 shoreline county FIPS codes with "marine" classification
       - Apply Great Lakes state override: counties in GREAT_LAKES_STATES that are coastal get "lacustrine" instead of "marine"
       - Store as: `{"counties": {"01003": "marine", ...}, "metadata": {"source": "NOAA ENOW Shoreline Counties", "great_lakes_states": [...], "total_counties": N}}`
       - Use atomic write to data/reference/coastal_counties.json

    2. **NWS NWPS gauge reference** (`data/reference/nwps_gauges.csv`):
       - Download from https://water.noaa.gov/resources/downloads/reports/nwps_all_gauges_report.csv
       - Log HTTP status and file size
       - Save with atomic write pattern (write to temp, then rename)

    CLI interface:
    ```python
    if __name__ == "__main__":
        import argparse
        parser = argparse.ArgumentParser(description="Download flood reference data")
        parser.add_argument("--coastal-only", action="store_true")
        parser.add_argument("--gauges-only", action="store_true")
        args = parser.parse_args()
    ```

    IMPORTANT: The coastal counties JSON must include Alaska coastal counties (many Alaska boroughs are coastal). Per 20-CONTEXT.md, Alaska coastal Tribes should be flagged as coastal even if no CO-OPS data exists.

    **2c. Create `tests/test_flood_common.py`:**

    Tests for shared infrastructure:
    - test_normalize_county_fips_integer: `1001` -> `"01001"`
    - test_normalize_county_fips_string: `"1001"` -> `"01001"`
    - test_normalize_county_fips_already_padded: `"53033"` -> `"53033"`
    - test_split_county_fips: `"53033"` -> `("53", "033")`
    - test_classify_tribe_coastal_marine: County in marine list -> marine
    - test_classify_tribe_coastal_lacustrine: Great Lakes county -> lacustrine
    - test_classify_tribe_coastal_inland: No coastal counties -> inland
    - test_classify_tribe_coastal_marine_wins: Both marine and lacustrine -> marine
    - test_classify_tribe_coastal_empty: No counties -> inland
    - test_load_coastal_counties_missing_file: Returns empty dict with warning
    - test_load_coastal_counties_valid: Returns correct dict from test fixture
    - test_get_unique_county_fips: Extracts deduplicated FIPS from crosswalk
    - test_get_tribe_county_weights_basic: 2 counties, weights normalize to 1.0
    - test_get_tribe_county_weights_no_match: Empty dict when tribe has no GEOIDs
    - test_monitoring_gap_actions_all_sources: All 6 sources have entries
    - test_fetch_openfema_paginated_mocked: Mock requests.get, verify pagination loop (use unittest.mock.patch)
    - test_fetch_openfema_paginated_http_error: Mock 500 response, returns empty list

    Use `tmp_path` pytest fixture for file-based tests. Use `unittest.mock.patch("requests.get")` for HTTP tests.
  </action>
  <verify>
    Run: `python -c "from src.packets._flood_common import fetch_openfema_paginated, classify_tribe_coastal, normalize_county_fips; print('Import OK')"` -- succeeds.
    Run: `python -m pytest tests/test_flood_common.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass (no regressions).
  </verify>
  <done>
    Shared flood infrastructure module provides OpenFEMA paginated fetch, FIPS normalization, coastal classification, crosswalk helpers, and monitoring gap actions. Reference data download script created. 17+ tests pass covering all shared functionality.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.schemas.flood import FloodProfile, NFIPProfile, StreamflowProfile, CoastalProfile, CoastalType; print('All models importable')"` -- succeeds
2. `python -c "from src.packets._flood_common import fetch_openfema_paginated, classify_tribe_coastal, normalize_county_fips, MONITORING_GAP_ACTIONS; print('Infrastructure OK')"` -- succeeds
3. `python -c "from src.paths import FLOOD_CACHE_DIR, FLOOD_REFERENCE_DIR, COASTAL_COUNTIES_PATH, flood_cache_path; print(FLOOD_CACHE_DIR)"` -- prints valid path
4. `python -m pytest tests/test_flood_schemas.py tests/test_flood_common.py -v` -- all tests pass
5. `python -m pytest tests/ -x -q` -- all 1257+ tests pass (no regressions)
</verification>

<success_criteria>
- All 6 flood Pydantic models importable from src.schemas.flood with correct validation
- FloodSourceMetadata provides provenance tracking on every data record
- OpenFEMA paginated fetch handles multi-page results with rate limiting
- Coastal classification correctly implements marine > lacustrine > inland priority
- FIPS normalization handles integer, string, and short-form inputs
- Crosswalk helpers extract and normalize county weights per Tribe
- Reference data download script can fetch coastal counties and NWS gauge data
- dataretrieval added to requirements.txt
- 37+ tests passing across schema and infrastructure test files
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-01-SUMMARY.md`
</output>

---
phase: 20-flood-water-data
plan: 06
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - src/packets/flood_precipitation.py
  - src/packets/flood_coastal.py
  - tests/test_flood_precipitation.py
  - tests/test_flood_coastal.py
autonomous: true

must_haves:
  truths:
    - "Atlas 14 precipitation frequency estimates fetched for key return periods (10yr, 25yr, 50yr, 100yr)"
    - "Precipitation estimates queried per county centroid, then aggregated via crosswalk"
    - "CO-OPS stations discovered via Metadata API and matched to coastal counties"
    - "20-year daily water level record fetched (2 requests of 10-year windows)"
    - "Flood threshold levels from CO-OPS floodlevels endpoint"
    - "Inland Tribes get NOT_APPLICABLE for coastal (not monitoring_gap)"
    - "Coastal Tribes with no stations get monitoring_gap"
    - "Sea level trends fetched from Derived Product API"
  artifacts:
    - path: "src/packets/flood_precipitation.py"
      provides: "PrecipitationBuilder following 9-step builder template"
      contains: "class PrecipitationBuilder"
    - path: "src/packets/flood_coastal.py"
      provides: "CoastalBuilder following 9-step builder template"
      contains: "class CoastalBuilder"
    - path: "tests/test_flood_precipitation.py"
      provides: "Tests for Atlas 14 fetch and aggregation"
      contains: "test_aggregate_tribe_precipitation"
    - path: "tests/test_flood_coastal.py"
      provides: "Tests for CO-OPS station discovery, water levels, and coastal classification"
      contains: "test_aggregate_tribe_coastal"
  key_links:
    - from: "src/packets/flood_precipitation.py"
      to: "src/packets/_flood_common.py"
      via: "normalize_county_fips, get_tribe_county_weights, MONITORING_GAP_ACTIONS"
      pattern: "from src.packets._flood_common import"
    - from: "src/packets/flood_coastal.py"
      to: "src/packets/_flood_common.py"
      via: "classify_tribe_coastal, load_coastal_counties, MONITORING_GAP_ACTIONS"
      pattern: "from src.packets._flood_common import"
    - from: "src/packets/flood_coastal.py"
      to: "src/schemas/flood.py"
      via: "CoastalProfile, CoastalStation, CoastalType"
      pattern: "from src.schemas.flood import"
---

<objective>
Build the NOAA Atlas 14 precipitation builder (FLOOD-05) and the NOAA CO-OPS coastal water level builder (FLOOD-06) as a single plan. These are combined because both are NOAA sources, FLOOD-06 depends on the coastal classification from _flood_common, and they are the smallest two builders.

Purpose: Atlas 14 provides extreme rainfall risk context. CO-OPS provides coastal flood exposure data (water levels, flood thresholds, sea level trends) for the ~100-150 Tribes in coastal areas. Together they complete the 6 flood data sources.

Output: `src/packets/flood_precipitation.py`, `src/packets/flood_coastal.py`, and test suites for both.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@src/packets/svi_builder.py -- 9-step builder template
@src/packets/_flood_common.py -- classify_tribe_coastal, load_coastal_counties, normalize_county_fips, get_tribe_county_weights (from Plan 01)
@src/schemas/flood.py -- PrecipitationProfile, CoastalProfile, CoastalStation, CoastalType (from Plan 01)
@src/paths.py -- FLOOD_CACHE_DIR, COASTAL_COUNTIES_PATH
@.planning/phases/20-flood-water-data/20-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement PrecipitationBuilder (FLOOD-05)</name>
  <files>src/packets/flood_precipitation.py, tests/test_flood_precipitation.py</files>
  <action>
    **Create `src/packets/flood_precipitation.py`:**

    Atlas 14 has no formal REST API (per research). Two viable approaches:

    **Primary approach: PFDS URL scraping with structured parsing**
    - URL: `https://hdsc.nws.noaa.gov/cgi-bin/hdsc/new/cgi_readH5.py?lat={lat}&lon={lon}&type=pf&data=depth&units=english&series=pds`
    - This CGI endpoint returns structured data (not HTML) with precipitation depths for all return periods and durations
    - Query per county centroid (lat/lon from crosswalk or Census county centroid data)
    - Extract key return periods: 10yr, 25yr, 50yr, 100yr for 24-hour duration
    - Per 20-CONTEXT.md: only key return periods needed

    **Fallback approach: Hardcoded county centroid coordinates**
    - If the PFDS CGI is unreliable, use county centroid coordinates from Census TIGER data
    - Store as module constant or reference file
    - The crosswalk's area_weights already have county FIPS; derive centroids from county boundary data

    **Implementation notes:**
    - There are ~700 unique county FIPS in the crosswalk
    - Query one county centroid per unique county, cache result
    - Rate limit: 1 request per second (PFDS is a web service, not a formal API)
    - Handle timeouts and errors gracefully (some locations may not have data -- Hawaii, some Pacific territories)
    - Parse response: The CGI returns a structured text format with depths by duration and return period

    **Step 1-3: Init, crosswalk, weights** (standard pattern)
    - Also need county centroid lat/lon. Options:
      a) Use a county_centroids.json reference file (download or embed)
      b) Compute from crosswalk area_weights (approximate from AIANNH geometry)
      c) Use a simple state/county -> lat/lon lookup table for ~700 counties
    - Decision: Create a `_get_county_centroids()` method that loads or computes centroids. For Phase 20, a hardcoded lookup of ~700 county centroids from Census TIGER/Line data is practical. Store as a module constant or JSON file.
    - ALTERNATIVE: Skip county centroid lookup entirely and query Atlas 14 by a representative point within each Tribal area (from crosswalk geometry). This is simpler but less accurate for Tribes spanning multiple climate zones.

    **Step 4: fetch_precipitation(lat, lon) -> dict**
    - Query PFDS CGI endpoint
    - Parse response for 24-hour depths at 10yr, 25yr, 50yr, 100yr return periods
    - Return PrecipitationFrequency data
    - Handle: no data available, parse errors, timeout
    - Log every fetch with URL, params, response size, HTTP status

    **Step 5-6: Build county estimates and aggregate to Tribes**
    - For each county: fetch precipitation at centroid
    - Aggregate to Tribes: area-weighted average of 100yr and 25yr values
    - Coverage from crosswalk

    **Step 7-9: build_all, atomic write, coverage report** (standard)
    - Write to FLOOD_CACHE_DIR / f"{tribe_id}_precipitation.json"

    IMPORTANT: Atlas 14 PFDS may be flaky. Implement retry with backoff (3 attempts). If PFDS is entirely unreliable at runtime, the builder should still produce profiles with data_gap=API_ERROR rather than crashing.

    **Create `tests/test_flood_precipitation.py`:**

    - test_fetch_precipitation_mocked: Mocked PFDS returns structured data
    - test_fetch_precipitation_timeout: Timeout -> None with warning
    - test_fetch_precipitation_parse: Response parsed correctly for key return periods
    - test_aggregate_basic: 2 counties -> area-weighted average
    - test_aggregate_no_data: All counties fail -> API_ERROR gap
    - test_aggregate_partial: Some counties have data, some don't
    - test_builder_init: Constructor loads crosswalk
    - test_build_all_profiles_mocked: Full loop -> JSON files
    - test_coverage_report: Report has expected keys
    - test_pydantic_validation: Output passes PrecipitationProfile schema
    - test_return_periods_only_key: Only 10yr/25yr/50yr/100yr extracted (not all 19)
    - test_24hr_duration_only: Only 24-hour duration extracted

    15+ tests covering fetch, parse, aggregation, error handling, and validation.
  </action>
  <verify>
    Run: `python -c "from src.packets.flood_precipitation import PrecipitationBuilder; print('Import OK')"` -- succeeds.
    Run: `python -m pytest tests/test_flood_precipitation.py -v` -- all tests pass.
  </verify>
  <done>
    PrecipitationBuilder queries Atlas 14 PFDS per county centroid, extracts key return periods (10yr/25yr/50yr/100yr) for 24-hour duration, area-weights to Tribes. Retry logic handles PFDS flakiness. 15+ tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement CoastalBuilder (FLOOD-06)</name>
  <files>src/packets/flood_coastal.py, tests/test_flood_coastal.py</files>
  <action>
    **Create `src/packets/flood_coastal.py`:**

    **Module constants:**
    ```python
    COOPS_DATAGETTER = "https://api.tidesandcurrents.noaa.gov/api/prod/datagetter"
    COOPS_META_BASE = "https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi"
    COOPS_DPAPI_BASE = "https://api.tidesandcurrents.noaa.gov/dpapi/prod/webapi"
    DAILY_MEAN_MAX_YEARS = 10  # CO-OPS limit per request for daily_mean
    WATER_LEVEL_YEARS = 20     # 20-year record per 20-CONTEXT.md
    ```

    **Step 1: __init__(config)**
    - Load crosswalk, area weights
    - Load coastal counties reference from COASTAL_COUNTIES_PATH
    - Pre-classify all Tribes as marine/lacustrine/inland
    - Discover and cache all CO-OPS stations (one-time fetch)
    - Map stations to counties by lat/lon proximity

    **Step 2-3: Load crosswalk, weights, coastal reference**

    **Step 4a: discover_coops_stations() -> list[dict]**
    - URL: `{COOPS_META_BASE}/stations.json?type=waterlevels`
    - Returns ~300-400 water level stations nationwide
    - Extract: id, name, lat, lon, state
    - This is a one-time fetch (all stations), not per-county

    **Step 4b: map_stations_to_counties(stations, coastal_counties) -> dict[str, list[str]]**
    - For each station, find nearest coastal county by lat/lon
    - Simple approach: for each station, check all coastal county centroids and find nearest
    - Alternative: just match by state (less precise but simpler)
    - Store mapping: county_fips -> [station_ids]
    - Stations not matching any coastal county -> skip with warning

    **Step 4c: fetch_flood_levels(station_id) -> dict**
    - URL: `{COOPS_META_BASE}/stations/{station_id}/floodlevels.json`
    - Returns NOS minor/moderate/major flood thresholds
    - Handle: station has no flood levels defined -> None values

    **Step 4d: fetch_daily_water_levels(station_id) -> list[dict]**
    - Use `daily_mean` product (10-year max per request per research)
    - Per 20-CONTEXT.md: 20-year record -> 2 requests (e.g., 2005-2015, 2015-2025)
    - Datum: MLLW
    - Application header: "TCR_Policy_Scanner"
    - Return list of {date, value_ft}
    - Count days above minor flood level (if flood level known)

    **Step 4e: fetch_sea_level_trend(station_id) -> float | None**
    - Use Derived Product API: `{COOPS_DPAPI_BASE}/product/sltrends/station/{station_id}.json`
    - Extract sea level trend in mm/yr
    - Handle: station too new for trend calculation -> None

    **Step 5: _build_station_record(station, flood_levels, daily_data, sl_trend) -> CoastalStation**
    - Populate all CoastalStation fields
    - Count days_above_minor_flood from daily data where value > minor_flood_level
    - Identify highest observed level and date

    **Step 6: _aggregate_tribe_coastal(tribe_id) -> CoastalProfile**
    - Get coastal_type for Tribe (pre-classified)
    - If coastal_type == INLAND: return profile with data_gap=NOT_APPLICABLE (not monitoring_gap per 20-CONTEXT.md)
    - Get county FIPS for Tribe
    - Find CO-OPS stations in those counties
    - For each station: fetch flood levels, daily data, sea level trend
    - station_count, max flood days across stations, average SLR trend
    - If coastal Tribe but no stations: data_gap=MONITORING_GAP
    - Alaska coastal Tribes: flag as coastal, note absence as monitoring gap per 20-CONTEXT.md

    **Step 7: build_all_profiles() -> int**
    - Phase 1: Discover all CO-OPS stations (one fetch)
    - Phase 2: Map stations to counties
    - Phase 3: For each coastal Tribe, fetch station data
    - Phase 4: Skip inland Tribes (mark NOT_APPLICABLE)
    - Log: expected ~100-150 coastal Tribes, ~300-400 stations

    **Step 8: Atomic write** to FLOOD_CACHE_DIR / f"{tribe_id}_coastal.json"

    **Step 9: Coverage report**
    - Tribes by coastal_type (marine, lacustrine, inland)
    - Coastal Tribes with station data vs monitoring gaps
    - Alaska coastal Tribes (expected monitoring gaps)
    - Average sea level trends by region

    **Create `tests/test_flood_coastal.py`:**

    - test_discover_stations_mocked: All stations fetched
    - test_map_stations_to_counties: Station matched to nearest county
    - test_fetch_flood_levels_mocked: Flood thresholds extracted
    - test_fetch_flood_levels_no_data: Station without levels -> None values
    - test_fetch_daily_levels_two_windows: 20-year data requires 2 requests
    - test_fetch_sea_level_trend_mocked: SLR mm/yr extracted
    - test_count_days_above_flood: Correct count from daily data
    - test_aggregate_coastal_tribe: Marine Tribe gets station data
    - test_aggregate_inland_tribe: Inland Tribe gets NOT_APPLICABLE
    - test_aggregate_coastal_no_stations: Coastal Tribe, no stations -> monitoring gap
    - test_aggregate_alaska_coastal: Alaska coastal flagged, monitoring gap noted
    - test_aggregate_lacustrine: Great Lakes Tribe classified correctly
    - test_builder_init: Stations discovered at init
    - test_build_all_profiles_mocked: Full loop with coastal + inland Tribes
    - test_coverage_report_coastal_breakdown: Report shows marine/lacustrine/inland counts
    - test_pydantic_validation: Output passes CoastalProfile schema

    20+ tests covering station discovery, flood level extraction, daily water level processing, SLR trends, coastal classification, and inland/gap handling.
  </action>
  <verify>
    Run: `python -c "from src.packets.flood_coastal import CoastalBuilder; print('Import OK')"` -- succeeds.
    Run: `python -m pytest tests/test_flood_coastal.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass.
  </verify>
  <done>
    CoastalBuilder discovers CO-OPS stations, fetches flood levels and 20-year daily water levels, computes SLR trends, and correctly handles coastal (marine/lacustrine) vs inland Tribes. PrecipitationBuilder queries Atlas 14 PFDS for key return periods. Both produce Pydantic-validated cached JSON. 35+ tests across both builders.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.flood_precipitation import PrecipitationBuilder; from src.packets.flood_coastal import CoastalBuilder; print('Both OK')"` -- succeeds
2. `python -m pytest tests/test_flood_precipitation.py tests/test_flood_coastal.py -v` -- all tests pass
3. `python -m pytest tests/ -x -q` -- all 1257+ tests pass
</verification>

<success_criteria>
- Atlas 14: key return periods (10yr/25yr/50yr/100yr) for 24-hour duration
- Atlas 14: queried per county centroid with retry/backoff for PFDS flakiness
- CO-OPS: 3 APIs used (datagetter for water levels, MDAPI for metadata, DPAPI for SLR trends)
- CO-OPS: 20-year daily record via 2 x 10-year requests
- CO-OPS: flood threshold levels from floodlevels endpoint
- Inland Tribes: NOT_APPLICABLE (not monitoring_gap)
- Coastal Tribes with no stations: MONITORING_GAP
- Alaska coastal: flagged as coastal even without CO-OPS data
- 35+ tests across both builders
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-06-SUMMARY.md`
</output>

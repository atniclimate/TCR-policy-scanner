---
phase: 20-flood-water-data
plan: 03
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - src/packets/flood_declarations.py
  - tests/test_flood_declarations.py
autonomous: true

must_haves:
  truths:
    - "Disaster declarations fetched from OpenFEMA for ALL disaster types (not just flood)"
    - "20-year window (2005-present) applied per 20-CONTEXT.md"
    - "PA and HMGP dollar amounts joined from separate OpenFEMA datasets by disaster number"
    - "Declarations aggregated to Tribes via area-weighted crosswalk"
    - "Provenance metadata records every API fetch"
    - "Tribes with no declarations get appropriate gap classification"
  artifacts:
    - path: "src/packets/flood_declarations.py"
      provides: "DeclarationsBuilder following 9-step builder template"
      contains: "class DeclarationsBuilder"
    - path: "tests/test_flood_declarations.py"
      provides: "Tests for declarations fetch, PA/HMGP join, and aggregation"
      contains: "test_aggregate_tribe_declarations"
  key_links:
    - from: "src/packets/flood_declarations.py"
      to: "src/packets/_flood_common.py"
      via: "fetch_openfema_paginated, normalize_county_fips, get_tribe_county_weights"
      pattern: "from src.packets._flood_common import"
    - from: "src/packets/flood_declarations.py"
      to: "src/schemas/flood.py"
      via: "DisasterDeclarationsProfile, DisasterDeclaration, FloodSourceMetadata"
      pattern: "from src.schemas.flood import"
---

<objective>
Build the FEMA disaster declarations builder (FLOOD-02): fetch ALL disaster types from OpenFEMA for a 20-year window, join PA/HMGP dollar amounts from separate datasets, and aggregate to Tribes via crosswalk.

Purpose: Disaster declaration history reveals the frequency and severity of federal emergency responses affecting Tribal lands. The 20-CONTEXT.md explicitly says to include ALL disaster types (not just flood) for a broader vulnerability picture. Dollar amounts enable advocacy narratives like "$X million in federal disaster aid received over 20 years."

Output: `src/packets/flood_declarations.py` with DeclarationsBuilder class and `tests/test_flood_declarations.py`.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@src/packets/svi_builder.py -- 9-step builder template
@src/packets/_flood_common.py -- fetch_openfema_paginated, normalize_county_fips, get_tribe_county_weights (from Plan 01)
@src/schemas/flood.py -- DisasterDeclarationsProfile, DisasterDeclaration, FloodSourceMetadata (from Plan 01)
@src/paths.py -- FLOOD_CACHE_DIR, AIANNH_CROSSWALK_PATH, TRIBAL_COUNTY_WEIGHTS_PATH
@.planning/phases/20-flood-water-data/20-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement DeclarationsBuilder with PA/HMGP dollar joins</name>
  <files>src/packets/flood_declarations.py</files>
  <action>
    Create `src/packets/flood_declarations.py` following the 9-step builder template.

    **Step 1: __init__(config: dict)**
    - Accept optional config: `output_dir`, `min_year` (default 2005 for 20-year window)
    - min_year should NOT be hardcoded -- use config with default. Per 20-CONTEXT.md: 2005 (post-Katrina)
    - Load crosswalk and area weights
    - Initialize county declaration cache and PA/HMGP caches

    **Step 2-3: Load crosswalk and area weights** (same pattern as NFIPBuilder)

    **Step 4a: fetch_county_declarations(county_fips: str) -> list[dict]**
    - Use `fetch_openfema_paginated()`
    - Endpoint: `DisasterDeclarationsSummaries` (v2)
    - Filter: `fipsStateCode eq '{state_2}' and fipsCountyCode eq '{county_3}' and fyDeclared ge {min_year}`
    - IMPORTANT per 20-CONTEXT.md: Include ALL disaster types (no incidentType filter)
    - Key fields: disasterNumber, declarationDate, incidentType, declarationTitle, incidentBeginDate, incidentEndDate, ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared, fyDeclared

    **Step 4b: fetch_pa_summaries(disaster_numbers: list[int]) -> dict[int, float]**
    - Endpoint: `PublicAssistanceFundedProjectsSummaries` (v1)
    - For each unique disaster number from declarations, sum `federalShareObligated`
    - Filter: `disasterNumber eq {num}`
    - Return dict: {disaster_number: total_pa_dollars}
    - Cache by disaster number (same disaster affects multiple counties)
    - This may be a large number of queries. Batch by fetching per-state or use broader filters to reduce API calls. Log total queries needed.

    **Step 4c: fetch_hmgp_amounts(disaster_numbers: list[int]) -> dict[int, float]**
    - Endpoint: `HazardMitigationAssistanceProjects` (v4)
    - Filter: `disasterNumber eq {num}`
    - Sum `federalShareObligated` per disaster number
    - Return dict: {disaster_number: total_hmgp_dollars}
    - Same caching strategy as PA

    **Step 5: _build_declaration_record(raw: dict, pa_cache, hmgp_cache) -> DisasterDeclaration**
    - Map raw API fields to DisasterDeclaration schema
    - Join PA dollars from pa_cache by disaster_number
    - Join HMGP dollars from hmgp_cache by disaster_number
    - Convert dates to ISO format
    - Convert program flags (string "true"/"false") to bool

    **Step 6: _aggregate_tribe_declarations(tribe_id) -> DisasterDeclarationsProfile**
    - Get county weights for Tribe
    - Collect all declarations from weighted counties (deduplicate by disaster_number -- same disaster may appear in multiple counties for same Tribe)
    - Count by incident type
    - Sum PA and HMGP dollars (NOT area-weighted -- dollars are per-declaration, not per-county)
    - Date range from earliest/latest declaration
    - coverage_pct from crosswalk weight matching
    - If zero declarations: data_gap = NO_RECORDS (not monitoring_gap -- zero disasters is possibly good news), gap_advocacy_action from MONITORING_GAP_ACTIONS["declarations"]

    **Step 7: build_all_profiles() -> int**
    - Phase 1: Fetch declarations for all unique counties (batch)
    - Phase 2: Collect all unique disaster numbers across all counties
    - Phase 3: Fetch PA summaries for unique disaster numbers
    - Phase 4: Fetch HMGP amounts for unique disaster numbers
    - Phase 5: Aggregate to Tribes
    - Log phase transitions and counts

    **Step 8: Atomic write** (same pattern as NFIPBuilder)
    - Write to FLOOD_CACHE_DIR / f"{tribe_id}_declarations.json"

    **Step 9: Coverage report**
    - Tribes with declarations vs without
    - Total declarations, by type, by year
    - Total PA + HMGP dollars
    - Top disaster types across all Tribes

    CRITICAL: Do NOT hardcode 2005 as the start year in logic. Use `self._min_year` from config. The config default is 2005 but must be overridable.
  </action>
  <verify>
    Run: `python -c "from src.packets.flood_declarations import DeclarationsBuilder; print('Import OK')"` -- succeeds.
  </verify>
  <done>
    DeclarationsBuilder fetches ALL disaster types for 20-year window, joins PA/HMGP dollars from separate OpenFEMA datasets, deduplicates by disaster_number, and writes per-Tribe cached JSON. Start year is configurable.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write disaster declarations builder tests</name>
  <files>tests/test_flood_declarations.py</files>
  <action>
    Create `tests/test_flood_declarations.py` with comprehensive tests.

    **Test fixtures:**
    - `sample_declarations_raw`: 4 declaration dicts with various incident types (Flood, Hurricane, Fire, Severe Storm)
    - `sample_pa_data`: PA summaries for 2 disaster numbers
    - `sample_hmgp_data`: HMGP amounts for 2 disaster numbers
    - `sample_crosswalk_data`: Minimal crosswalk for 2 Tribes

    **TestDeclarationRecordBuilding:**
    - test_build_declaration_basic: Raw dict -> DisasterDeclaration with correct fields
    - test_build_declaration_with_pa_join: PA dollars joined by disaster number
    - test_build_declaration_with_hmgp_join: HMGP dollars joined by disaster number
    - test_build_declaration_no_dollars: No PA/HMGP match -> None amounts
    - test_build_declaration_date_conversion: Dates converted to ISO format
    - test_build_declaration_program_flags: String "true"/"false" -> bool conversion

    **TestDeclarationsAggregation:**
    - test_aggregate_basic: 2 declarations in 1 county -> correct profile
    - test_aggregate_multi_county: Declarations from 2 counties, deduplicated by disaster_number
    - test_aggregate_deduplication: Same disaster in 2 counties counted once
    - test_aggregate_all_incident_types: Multiple types tallied correctly
    - test_aggregate_zero_declarations: No declarations -> NO_RECORDS gap
    - test_aggregate_dollar_totals: PA + HMGP summed correctly
    - test_aggregate_date_range: Earliest and latest dates identified

    **TestDeclarationsBuilder (integration with mocks):**
    - test_builder_init: Constructor accepts config with min_year
    - test_builder_default_min_year: Default is 2005
    - test_builder_custom_min_year: Config override works
    - test_fetch_county_declarations_mocked: Mocked API returns filtered data
    - test_fetch_pa_summaries_mocked: PA join returns dollar amounts
    - test_fetch_hmgp_amounts_mocked: HMGP join returns dollar amounts
    - test_build_all_profiles_mocked: Full loop -> JSON files in tmp_path
    - test_all_disaster_types_included: Filter does NOT restrict incidentType (per 20-CONTEXT.md)
    - test_coverage_report_has_type_breakdown: Report includes declarations_by_type

    **TestDeclarationsValidation:**
    - test_profile_pydantic_validation: Builder output passes schema
    - test_profile_bad_tribe_id: Invalid tribe_id fails

    Use `unittest.mock.patch("requests.get")` for all HTTP mocking.
  </action>
  <verify>
    Run: `python -m pytest tests/test_flood_declarations.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass.
  </verify>
  <done>
    30+ tests cover declaration fetching, PA/HMGP dollar joins, deduplication by disaster_number, all-disaster-type inclusion, configurable start year, and Pydantic validation. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.flood_declarations import DeclarationsBuilder; print('Import OK')"` -- succeeds
2. `python -m pytest tests/test_flood_declarations.py -v` -- all tests pass
3. `python -m pytest tests/ -x -q` -- all 1257+ tests pass
</verification>

<success_criteria>
- ALL disaster types included (not just flood) per 20-CONTEXT.md
- 20-year window (configurable, default 2005)
- PA + HMGP dollar amounts joined from separate OpenFEMA datasets
- Declarations deduplicated by disaster_number within each Tribe
- Dollar totals NOT area-weighted (per-declaration, not per-county)
- Provenance metadata on every fetch
- 30+ tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-03-SUMMARY.md`
</output>

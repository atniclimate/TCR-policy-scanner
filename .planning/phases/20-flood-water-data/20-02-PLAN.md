---
phase: 20-flood-water-data
plan: 02
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - src/packets/flood_nfip.py
  - tests/test_flood_nfip.py
autonomous: true

must_haves:
  truths:
    - "NFIP claims are fetched from OpenFEMA per county FIPS and aggregated to Tribes via crosswalk"
    - "All dollar amounts use area-weighted aggregation, not simple summation"
    - "Flood zone distribution tracks which zones have the most claims per Tribe"
    - "Provenance metadata records every API fetch with URL, params, timestamp, status"
    - "Tribes with zero claims get monitoring_gap classification, not empty data"
    - "Atomic writes with utf-8 encoding on all output files"
  artifacts:
    - path: "src/packets/flood_nfip.py"
      provides: "NFIPBuilder following 9-step builder template"
      contains: "class NFIPBuilder"
    - path: "tests/test_flood_nfip.py"
      provides: "Tests for NFIP claims fetch, aggregation, and atomic write"
      contains: "test_aggregate_tribe_nfip"
  key_links:
    - from: "src/packets/flood_nfip.py"
      to: "src/packets/_flood_common.py"
      via: "fetch_openfema_paginated, normalize_county_fips, get_tribe_county_weights"
      pattern: "from src.packets._flood_common import"
    - from: "src/packets/flood_nfip.py"
      to: "src/schemas/flood.py"
      via: "NFIPProfile, NFIPClaimsSummary, FloodSourceMetadata"
      pattern: "from src.schemas.flood import"
---

<objective>
Build the NFIP flood insurance data builder (FLOOD-01): fetch claims history from OpenFEMA per county, aggregate to Tribes via area-weighted crosswalk, track flood zone distribution, and write per-Tribe cached JSON.

Purpose: NFIP claims data reveals the historical financial impact of flooding on Tribal lands. Combined with FLOOD-03 (NFHL), this data enables advocacy narratives like "X dollars in flood losses over Y years" and identifies communities with low NFIP participation as monitoring gaps.

Output: `src/packets/flood_nfip.py` with NFIPBuilder class and `tests/test_flood_nfip.py` with 30+ tests.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@src/packets/svi_builder.py -- 9-step builder template to follow
@src/packets/_geo_common.py -- atomic_write_json, load_aiannh_crosswalk, load_area_weights
@src/packets/_flood_common.py -- fetch_openfema_paginated, normalize_county_fips, get_tribe_county_weights, MONITORING_GAP_ACTIONS (from Plan 01)
@src/schemas/flood.py -- NFIPProfile, NFIPClaimsSummary, FloodSourceMetadata (from Plan 01)
@src/paths.py -- FLOOD_CACHE_DIR, AIANNH_CROSSWALK_PATH, TRIBAL_COUNTY_WEIGHTS_PATH (from Plan 01)
@.planning/phases/20-flood-water-data/20-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement NFIPBuilder</name>
  <files>src/packets/flood_nfip.py</files>
  <action>
    Create `src/packets/flood_nfip.py` following the 9-step builder template from `src/packets/svi_builder.py`.

    **Step 1: __init__(config: dict)**
    - Accept optional config dict with keys: `output_dir` (default FLOOD_CACHE_DIR)
    - Load crosswalk and area weights via _geo_common functions
    - Load coastal counties reference (for coastal_type in FloodProfile, passed through to orchestrator)
    - Initialize `self._county_claims_cache: dict[str, list[dict]]` for deduplication

    **Step 2-3: Load crosswalk and area weights**
    - `load_aiannh_crosswalk(AIANNH_CROSSWALK_PATH, label="NFIP")`
    - `load_area_weights(TRIBAL_COUNTY_WEIGHTS_PATH, label="NFIP")`
    - Extract unique county FIPS set from crosswalk

    **Step 4: fetch_county_claims(county_fips: str) -> list[dict]**
    - Use `fetch_openfema_paginated()` from _flood_common
    - Endpoint: `FimaNfipClaims` (v2)
    - IMPORTANT: NFIP `countyCode` field format is uncertain (3-digit vs 5-digit). Try 5-digit first. If zero results, try state+3-digit filter. Document the approach.
    - Filter: `countyCode eq '{county_fips}'` (try 5-digit first)
    - Fallback filter: `state eq '{state_abbrev}' and countyCode eq '{county_3digit}'`
    - Select fields: countyCode, state, dateOfLoss, yearOfLoss, amountPaidOnBuildingClaim, amountPaidOnContentsClaim, amountPaidOnIncreasedCostOfComplianceClaim, ratedFloodZone, floodZoneCurrent, occupancyType
    - NO time cap on NFIP (full cumulative record per 20-CONTEXT.md)
    - Cache results by county FIPS to avoid re-fetching

    **Step 5: _build_county_summary(county_fips, claims) -> NFIPClaimsSummary**
    - Sum dollar amounts: building, contents, ICC
    - total_paid_all = building + contents + ICC
    - Count flood zone distribution from ratedFloodZone field
    - Compute year_range from yearOfLoss field (min, max)
    - Total claims count

    **Step 6: _aggregate_tribe_nfip(tribe_id) -> NFIPProfile**
    - Get county weights for this Tribe via `get_tribe_county_weights()`
    - For each county: get or fetch claims, build county summary
    - Aggregate: total_claims, total_paid (area-weighted sum across counties)
    - area_weighted_claims_per_year: total_claims * weight / year_span for each county, summed
    - dominant_flood_zone: most common zone across all weighted claims
    - coverage_pct from crosswalk weight matching
    - If zero claims across all counties: data_gap = MONITORING_GAP, gap_advocacy_action from MONITORING_GAP_ACTIONS["nfip"]
    - Validate against NFIPProfile Pydantic schema

    **Step 7: build_all_profiles() -> int**
    - Fetch claims for ALL unique counties first (batch approach, ~700 counties)
    - Then iterate 592 Tribes and aggregate from cached county data
    - Log progress every 50 Tribes
    - Return count of profiles built

    **Step 8: _write_profile(tribe_id, profile_data)**
    - Use `atomic_write_json()` from _geo_common
    - Path: FLOOD_CACHE_DIR / f"{tribe_id}_nfip.json" (component file, not full FloodProfile -- orchestrator combines)
    - Path traversal guard via Path(tribe_id).name + reject ".."

    **Step 9: generate_coverage_report() -> dict**
    - Tribes with claims data vs without
    - Total claims fetched, total dollars
    - Top 10 Tribes by total_paid
    - Monitoring gaps list
    - Write JSON + markdown report to OUTPUTS_DIR

    Use `logging.getLogger(__name__)` for all logging. All `open()` calls use `encoding="utf-8"`. All dollar values stored as float (formatting via `utils.format_dollars()` at render time, not here).
  </action>
  <verify>
    Run: `python -c "from src.packets.flood_nfip import NFIPBuilder; print('Import OK')"` -- succeeds.
  </verify>
  <done>
    NFIPBuilder implements all 9 steps: crosswalk loading, OpenFEMA claims fetch per county, area-weighted aggregation to Tribes, atomic JSON write, coverage report. Handles county FIPS format uncertainty with fallback strategy.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write NFIP builder tests</name>
  <files>tests/test_flood_nfip.py</files>
  <action>
    Create `tests/test_flood_nfip.py` with comprehensive test coverage. Use mocked HTTP responses (never hit real APIs in tests).

    **Test fixtures (module-level):**
    - `sample_claims_data`: List of 5 realistic NFIP claim dicts with varying flood zones, dollar amounts, years
    - `sample_crosswalk`: Minimal crosswalk with 2 Tribes, 3 GEOIDs, 4 counties
    - `sample_area_weights`: Weights for the 3 GEOIDs

    **TestNFIPCountySummary:**
    - test_build_county_summary_basic: 3 claims -> correct totals
    - test_build_county_summary_empty: 0 claims -> all zeros
    - test_build_county_summary_flood_zones: Zone distribution counts correctly
    - test_build_county_summary_year_range: Min/max year extracted
    - test_build_county_summary_missing_amounts: None/null amounts treated as 0

    **TestNFIPAggregation:**
    - test_aggregate_tribe_single_county: One county, weight 1.0
    - test_aggregate_tribe_multi_county: Two counties with different weights
    - test_aggregate_tribe_no_claims: Zero claims -> monitoring gap
    - test_aggregate_tribe_dominant_zone: Most frequent zone identified
    - test_aggregate_tribe_coverage_pct: Correct weight fraction

    **TestNFIPBuilder (integration with mocks):**
    - test_builder_init: Constructor loads crosswalk
    - test_fetch_county_claims_mocked: Mocked OpenFEMA returns paginated data
    - test_fetch_county_claims_http_error: 500 response -> empty list, no crash
    - test_build_all_profiles_mocked: Full loop with mocked data -> JSON files in tmp_path
    - test_atomic_write_creates_file: Profile written to expected path
    - test_path_traversal_rejection: tribe_id with ".." raises ValueError
    - test_coverage_report_structure: Report has required keys

    **TestNFIPPydanticValidation:**
    - test_nfip_profile_validates: Builder output passes NFIPProfile schema
    - test_nfip_profile_bad_coverage: coverage_pct > 1.0 fails validation

    Use `unittest.mock.patch("src.packets._flood_common.requests.get")` or `unittest.mock.patch("requests.get")` for HTTP mocking. Use `tmp_path` for file output tests.

    All tests must pass: `python -m pytest tests/test_flood_nfip.py -v`
    No regressions: `python -m pytest tests/ -x -q`
  </action>
  <verify>
    Run: `python -m pytest tests/test_flood_nfip.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass.
  </verify>
  <done>
    30+ tests cover NFIP county summary building, area-weighted aggregation, monitoring gap detection, HTTP mocking, atomic writes, and Pydantic validation. All tests pass with no regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.flood_nfip import NFIPBuilder; print('Import OK')"` -- succeeds
2. `python -m pytest tests/test_flood_nfip.py -v` -- all tests pass
3. `python -m pytest tests/ -x -q` -- all 1257+ tests pass
</verification>

<success_criteria>
- NFIPBuilder follows 9-step builder template
- OpenFEMA claims fetched per county with pagination and rate limiting
- Area-weighted aggregation (not simple sum) for dollar amounts
- Flood zone distribution tracked per Tribe
- Monitoring gap detection for Tribes with zero claims
- Provenance metadata on every fetch
- Atomic writes with utf-8 encoding
- 30+ tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-02-SUMMARY.md`
</output>

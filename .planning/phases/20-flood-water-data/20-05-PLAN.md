---
phase: 20-flood-water-data
plan: 05
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - src/packets/flood_streamflow.py
  - tests/test_flood_streamflow.py
autonomous: true

must_haves:
  truths:
    - "USGS monitoring locations discovered via new OGC API per county FIPS"
    - "Peak annual flows fetched via dataretrieval (legacy NWIS, not new API)"
    - "Recent daily streamflow fetched via new OGC API"
    - "Drainage area filter applied (configurable threshold)"
    - "NWS flood stage thresholds cross-referenced from NWPS gauge CSV"
    - "Raw API responses stored alongside parsed data for reprocessing"
    - "Tribes with no gauges get monitoring_gap classification"
  artifacts:
    - path: "src/packets/flood_streamflow.py"
      provides: "StreamflowBuilder following 9-step builder template"
      contains: "class StreamflowBuilder"
    - path: "tests/test_flood_streamflow.py"
      provides: "Tests for USGS site discovery, peak flows, daily values, and gauge filtering"
      contains: "test_aggregate_tribe_streamflow"
  key_links:
    - from: "src/packets/flood_streamflow.py"
      to: "src/packets/_flood_common.py"
      via: "normalize_county_fips, get_tribe_county_weights, MONITORING_GAP_ACTIONS"
      pattern: "from src.packets._flood_common import"
    - from: "src/packets/flood_streamflow.py"
      to: "src/schemas/flood.py"
      via: "StreamflowProfile, StreamflowGauge, FloodSourceMetadata"
      pattern: "from src.schemas.flood import"
    - from: "src/packets/flood_streamflow.py"
      to: "src/paths.py"
      via: "NWPS_GAUGES_PATH for flood stage reference"
      pattern: "from src.paths import.*NWPS_GAUGES_PATH"
---

<objective>
Build the USGS streamflow data builder (FLOOD-04): discover monitoring locations via new OGC API, fetch peak annual flows via dataretrieval (legacy NWIS), fetch recent daily streamflow, cross-reference NWS flood stages, and store raw + parsed data.

Purpose: Streamflow data provides the most direct measure of flood risk for inland Tribal communities. Per DEC-07 (Phase 18.5), build against the new USGS API for site discovery and daily values, but use legacy NWIS via dataretrieval for peak flows (not yet on new API). Per 20-CONTEXT.md, Tribes with no gauges are documented as "monitoring gaps" framed as federal infrastructure failures.

Output: `src/packets/flood_streamflow.py` with StreamflowBuilder class and `tests/test_flood_streamflow.py`.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-flood-water-data/20-CONTEXT.md
@.planning/phases/20-flood-water-data/20-RESEARCH.md

@src/packets/svi_builder.py -- 9-step builder template
@src/packets/_flood_common.py -- normalize_county_fips, get_tribe_county_weights, MONITORING_GAP_ACTIONS (from Plan 01)
@src/schemas/flood.py -- StreamflowProfile, StreamflowGauge, FloodSourceMetadata (from Plan 01)
@src/paths.py -- FLOOD_CACHE_DIR, NWPS_GAUGES_PATH
@.planning/phases/20-flood-water-data/20-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement StreamflowBuilder with hybrid USGS API strategy</name>
  <files>src/packets/flood_streamflow.py</files>
  <action>
    Create `src/packets/flood_streamflow.py` following the 9-step builder template.

    **Module constants:**
    ```python
    USGS_OGC_BASE = "https://api.waterdata.usgs.gov/ogcapi/v0"
    DEFAULT_DRAINAGE_MIN_SQMI = 10.0    # Filter out tiny headwater streams
    DEFAULT_DRAINAGE_MAX_SQMI = 5000.0  # Filter out massive river mainstems
    DAILY_LOOKBACK_DAYS = 365           # 1 year of recent daily data
    ```

    **Step 1: __init__(config: dict)**
    - Accept config: `output_dir`, `drainage_min_sqmi` (default 10), `drainage_max_sqmi` (default 5000), `daily_lookback_days` (default 365)
    - Drainage area thresholds configurable per 20-CONTEXT.md (Claude's discretion: 10-5000 sq mi)
    - Load crosswalk and area weights
    - Load NWS gauge reference from NWPS_GAUGES_PATH (if exists, else warn)
    - Initialize site cache

    **Step 2-3: Load crosswalk, area weights, and NWS gauge reference**
    - NWS gauge CSV loading:
      ```python
      def _load_nws_gauges(self) -> dict[str, dict]:
          """Load NWS NWPS gauge metadata keyed by USGS site ID."""
      ```
      Parse CSV with `csv.DictReader`, key by usgs_id column. Extract flood stages (action, flood, moderate, major). Use `_safe_float()` for numeric fields. Handle missing file gracefully (warn, return empty dict).

    **Step 4a: discover_gauges_in_county(county_fips: str) -> list[dict]**
    - New OGC API: `{USGS_OGC_BASE}/collections/monitoring-locations/items`
    - Params: `f=json`, `county_code={county_fips}`, `site_type_code=ST` (streams), `limit=500`
    - Handle pagination if `next` link in response
    - Log: timestamp, URL, params, response size, HTTP status
    - Return GeoJSON features list
    - Cache by county FIPS

    **Step 4b: filter_gauges_by_drainage(features: list[dict]) -> list[dict]**
    - Extract `drainage_area` from feature properties
    - Keep gauges where: drainage_min <= drainage_area <= drainage_max
    - If drainage_area is None or 0, INCLUDE the gauge (don't filter on missing data)
    - Per 20-CONTEXT.md: keep ALL gauges that pass the size filter (no cap)
    - Log filter results

    **Step 4c: fetch_peak_flows(site_no: str) -> list[dict]**
    - Use `dataretrieval.nwis.get_peak(sites=site_no)` (legacy NWIS)
    - IMPORTANT: Peak flows NOT on new API per research. Must use dataretrieval.
    - Handle ImportError gracefully: if dataretrieval not installed, log warning and return empty list
    - Return list of {date, peak_discharge_cfs, gage_height_ft}
    - Wrap in try/except for NWIS errors (some sites have no peak data)
    - This is the most time-consuming step -- log per-site fetch timing

    **Step 4d: fetch_recent_daily(site_no: str) -> list[dict]**
    - New OGC API: `{USGS_OGC_BASE}/collections/daily/items`
    - Params: `f=json`, `monitoring_location_id=USGS-{site_no}`, `parameter_code=00060`, `time=P{lookback_days}D`
    - Return list of {date, value_cfs}
    - Handle empty/error responses

    **Step 5: _build_gauge_record(feature, peaks, daily, nws_ref) -> StreamflowGauge**
    - Extract from GeoJSON feature: site_no, site_name, lat, lon, county_fips, drainage_area
    - Attach peak_flows list
    - Attach recent_daily_mean_cfs list
    - Cross-reference NWS gauge CSV by site_no for flood stages
    - If site_no found in NWS reference: populate flood_stage_ft, action_stage_ft, etc.

    **Step 6: _aggregate_tribe_streamflow(tribe_id) -> StreamflowProfile**
    - Get county FIPS for Tribe from crosswalk
    - Collect all gauges from those counties (already discovered)
    - Filter by drainage area
    - For each gauge: fetch peaks + daily (with caching)
    - Build StreamflowGauge records
    - gauge_count, max peak discharge, max peak gauge identification
    - coverage_pct from crosswalk
    - Store raw API responses keyed by site_no (per 20-CONTEXT.md)
    - If zero gauges: data_gap = MONITORING_GAP, gap_advocacy_action from MONITORING_GAP_ACTIONS["streamflow"]

    **Step 7: build_all_profiles() -> int**
    - Phase 1: Discover gauges for all unique counties
    - Phase 2: Filter by drainage area
    - Phase 3: Fetch peak flows for all retained gauges (this is the slow step)
    - Phase 4: Fetch recent daily for all retained gauges
    - Phase 5: Aggregate to Tribes
    - Log estimated time and progress (peak flows may take hours for many gauges)

    **Step 8: Atomic write**
    - Write to FLOOD_CACHE_DIR / f"{tribe_id}_streamflow.json"
    - Include raw_api_responses field in the JSON

    **Step 9: Coverage report**
    - Tribes with gauges vs monitoring gaps
    - Total gauges discovered, retained after filter, with peak data
    - Gauge count distribution (0, 1-3, 4-10, 10+)
    - States with most monitoring gaps

    IMPORTANT: The dataretrieval library may not be installed in all environments. Import it inside the method that uses it (lazy import) and handle ImportError with a clear message:
    ```python
    def fetch_peak_flows(self, site_no: str) -> list[dict]:
        try:
            from dataretrieval import nwis
        except ImportError:
            logger.warning(
                "dataretrieval not installed. Install with: pip install dataretrieval. "
                "Peak flows will be empty for all gauges."
            )
            return []
    ```
  </action>
  <verify>
    Run: `python -c "from src.packets.flood_streamflow import StreamflowBuilder; print('Import OK')"` -- succeeds.
  </verify>
  <done>
    StreamflowBuilder discovers gauges via new OGC API, fetches peaks via dataretrieval (legacy NWIS), fetches daily via new OGC API, cross-references NWS flood stages, and stores raw + parsed data. Drainage area filter is configurable. Monitoring gaps documented.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write streamflow builder tests</name>
  <files>tests/test_flood_streamflow.py</files>
  <action>
    Create `tests/test_flood_streamflow.py` with comprehensive tests.

    **Test fixtures:**
    - `sample_geojson_features`: 4 GeoJSON features with varying drainage areas and site metadata
    - `sample_peak_flows`: Peak flow data for 2 gauges
    - `sample_daily_values`: Daily streamflow for 1 gauge (10 days)
    - `sample_nws_gauge_csv`: NWS gauge reference data for 2 gauges with flood stages
    - `sample_crosswalk`: Minimal crosswalk for 2 Tribes with 3 counties

    **TestGaugeDiscovery:**
    - test_discover_gauges_mocked: Mocked OGC API returns GeoJSON features
    - test_discover_gauges_empty_county: No gauges in county -> empty list
    - test_discover_gauges_pagination: Multi-page response handled

    **TestDrainageFilter:**
    - test_filter_default_range: Gauges in 10-5000 sq mi range kept
    - test_filter_too_small: Gauge with 5 sq mi drainage filtered out
    - test_filter_too_large: Gauge with 10000 sq mi drainage filtered out
    - test_filter_none_drainage: Gauge with None drainage_area kept (don't filter missing)
    - test_filter_custom_thresholds: Config overrides applied

    **TestPeakFlows:**
    - test_fetch_peak_flows_mocked: Mocked dataretrieval returns peak data
    - test_fetch_peak_flows_no_dataretrieval: ImportError handled gracefully
    - test_fetch_peak_flows_nwis_error: NWIS error -> empty list, no crash

    **TestDailyValues:**
    - test_fetch_recent_daily_mocked: Mocked OGC API returns daily values
    - test_fetch_recent_daily_empty: No data -> empty list

    **TestNWSCrossReference:**
    - test_nws_gauge_match: Gauge found in NWS reference -> flood stages populated
    - test_nws_gauge_no_match: Gauge not in reference -> stages are None
    - test_load_nws_gauges_missing_file: File not found -> empty dict with warning

    **TestStreamflowAggregation:**
    - test_aggregate_basic: 2 gauges in Tribe's counties -> correct profile
    - test_aggregate_no_gauges: Zero gauges -> monitoring gap
    - test_aggregate_max_peak: Highest peak discharge identified
    - test_aggregate_raw_responses_stored: Raw API data present in output
    - test_aggregate_coverage_pct: Correct weight fraction

    **TestStreamflowBuilder (integration):**
    - test_builder_init_default_config: Defaults applied
    - test_builder_custom_drainage: Config overrides drainage thresholds
    - test_build_all_profiles_mocked: Full loop -> JSON files in tmp_path
    - test_atomic_write: Profile written to correct path
    - test_path_traversal_rejection: ".." in tribe_id raises

    **TestStreamflowValidation:**
    - test_profile_pydantic_validation: Output passes StreamflowProfile schema
    - test_gauge_pydantic_validation: Gauge record passes StreamflowGauge schema

    Use `unittest.mock.patch` for all HTTP and dataretrieval mocking.
  </action>
  <verify>
    Run: `python -m pytest tests/test_flood_streamflow.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass.
  </verify>
  <done>
    35+ tests cover gauge discovery, drainage filtering, peak flow fetching (with dataretrieval mock), daily value fetching, NWS cross-reference, monitoring gap detection, raw response storage, and Pydantic validation. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.flood_streamflow import StreamflowBuilder; print('Import OK')"` -- succeeds
2. `python -m pytest tests/test_flood_streamflow.py -v` -- all tests pass
3. `python -m pytest tests/ -x -q` -- all 1257+ tests pass
</verification>

<success_criteria>
- New OGC API used for site discovery and daily values (per DEC-07)
- Legacy NWIS via dataretrieval used for peak flows (not on new API)
- Drainage area filter configurable (default 10-5000 sq mi)
- All gauges passing filter kept (no cap per 20-CONTEXT.md)
- NWS flood stages cross-referenced from NWPS gauge CSV
- Raw API responses stored alongside parsed data (per 20-CONTEXT.md)
- Monitoring gaps for Tribes with no gauges
- dataretrieval import failure handled gracefully
- 35+ tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/20-flood-water-data/20-05-SUMMARY.md`
</output>

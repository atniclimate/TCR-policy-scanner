---
phase: 13-hazard-population
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/packets/hazards.py
  - tests/test_hazard_aggregation.py
autonomous: true

must_haves:
  truths:
    - "HazardProfileBuilder loads the pre-computed area-weighted crosswalk from tribal_county_area_weights.json"
    - "County NRI scores are aggregated using area-weighted averaging (not MAX) per overlapping county's weight"
    - "Percentile scores (risk_score, eal_score, sovi_score, resl_score) use weighted AVERAGE; EAL dollar amounts (eal_total) use weighted SUM"
    - "Risk rating strings are re-derived from weighted scores using quintile breakpoints via score_to_rating()"
    - "Only non-zero hazards are stored in all_hazards (zero-score hazard types omitted)"
    - "Top 5 hazards are extracted and ranked by risk_score descending (not top 3)"
    - "Tests verify area-weighted aggregation correctness with multi-county and single-county cases"
  artifacts:
    - path: "src/packets/hazards.py"
      provides: "Area-weighted NRI aggregation, score_to_rating(), top-5 hazards, non-zero filtering"
      contains: "score_to_rating"
    - path: "tests/test_hazard_aggregation.py"
      provides: "Unit tests for area-weighted aggregation and rating derivation"
      min_lines: 80
  key_links:
    - from: "src/packets/hazards.py"
      to: "src/paths.py"
      via: "imports TRIBAL_COUNTY_WEIGHTS_PATH"
      pattern: "from src\\.paths import.*TRIBAL_COUNTY_WEIGHTS_PATH"
    - from: "src/packets/hazards.py"
      to: "data/nri/tribal_county_area_weights.json"
      via: "loads crosswalk JSON at runtime"
      pattern: "tribal_county_area_weights"
---

<objective>
Refactor hazards.py from MAX-based aggregation to area-weighted averaging, add score_to_rating() for rating re-derivation, change top-3 to top-5 hazards, and filter out zero-score hazards. This is the core algorithmic change that makes hazard profiles accurate for multi-county Tribes.

Purpose: The current MAX aggregation inflates risk scores for Tribes spanning many counties (a Tribe touching 10 counties gets the worst score from any of them). Area-weighted averaging correctly proportions risk by the actual geographic overlap, giving Tribal Leaders accurate data for advocacy.

Output:
- Refactored `src/packets/hazards.py` with area-weighted aggregation
- New `tests/test_hazard_aggregation.py` with unit tests for the aggregation logic
</objective>

<execution_context>
@D:\Claude-Workspace\.claude\get-shit-done\workflows\execute-plan.md
@D:\Claude-Workspace\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-hazard-population/13-CONTEXT.md
@.planning/phases/13-hazard-population/13-RESEARCH.md
@.planning/phases/13-hazard-population/13-01-SUMMARY.md
@src/packets/hazards.py
@src/paths.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add score_to_rating() and refactor aggregation in hazards.py</name>
  <files>src/packets/hazards.py</files>
  <action>
  This task makes targeted modifications to `src/packets/hazards.py`. Read the full file first.

  **1a. Add TRIBAL_COUNTY_WEIGHTS_PATH import:**

  Update the import from src.paths to include TRIBAL_COUNTY_WEIGHTS_PATH:
  ```python
  from src.paths import (
      AIANNH_CROSSWALK_PATH,
      HAZARD_PROFILES_DIR,
      NRI_DIR,
      PROJECT_ROOT,
      TRIBAL_COUNTY_WEIGHTS_PATH,
      USFS_DIR,
  )
  ```

  **1b. Add module-level score_to_rating() function:**

  Add this function near the top of the file, after `_safe_float()` and `_resolve_path()`:

  ```python
  def score_to_rating(score: float) -> str:
      """Convert 0-100 percentile score to NRI rating string.

      Uses quintile breakpoints as approximation of FEMA's methodology.
      SOVI/RESL officially use quintiles. Risk/EAL officially use k-means
      but quintiles are a defensible approximation for area-weighted derived scores.

      Args:
          score: Percentile score in range 0-100.

      Returns:
          NRI rating string from "Very Low" to "Very High".
      """
      if score >= 80:
          return "Very High"
      elif score >= 60:
          return "Relatively High"
      elif score >= 40:
          return "Relatively Moderate"
      elif score >= 20:
          return "Relatively Low"
      else:
          return "Very Low"
  ```

  **1c. Add _load_area_weights() method to HazardProfileBuilder:**

  Add a new method in the class that loads the pre-computed crosswalk:

  ```python
  def _load_area_weights(self) -> dict[str, list[dict]]:
      """Load pre-computed area-weighted AIANNH-to-county crosswalk.

      Reads the JSON file produced by scripts/build_area_crosswalk.py.
      Each AIANNH GEOID maps to a list of county entries with weights.

      Returns:
          Dict mapping AIANNH GEOID -> list of {county_fips, weight, overlap_area_sqkm}.
          Empty dict if file not found.
      """
      if not TRIBAL_COUNTY_WEIGHTS_PATH.exists():
          logger.warning(
              "Area-weighted crosswalk not found at %s -- "
              "falling back to unweighted aggregation. "
              "Run scripts/build_area_crosswalk.py to generate.",
              TRIBAL_COUNTY_WEIGHTS_PATH,
          )
          return {}

      try:
          with open(TRIBAL_COUNTY_WEIGHTS_PATH, encoding="utf-8") as f:
              data = json.load(f)
      except (json.JSONDecodeError, OSError) as exc:
          logger.error(
              "Failed to load area weights from %s: %s",
              TRIBAL_COUNTY_WEIGHTS_PATH, exc,
          )
          return {}

      crosswalk = data.get("crosswalk", {})
      metadata = data.get("metadata", {})
      logger.info(
          "Loaded area-weighted crosswalk: %d AIANNH entities, %d total county links",
          metadata.get("total_aiannh_entities", len(crosswalk)),
          metadata.get("total_county_links", 0),
      )
      return crosswalk
  ```

  Call this method in `__init__` after `_load_crosswalk()`:
  ```python
  self._area_weights: dict[str, list[dict]] = {}
  self._load_area_weights_data()
  ```
  Store result in `self._area_weights`.

  **1d. Refactor _build_tribe_nri_profile() for area-weighted aggregation:**

  This is the major change. Replace the aggregation logic (Steps 4-6 in the current code, approximately lines 463-532).

  The new logic:

  **Step 2 replacement -- Use area weights when available:**
  After finding AIANNH GEOIDs for the Tribe (existing Step 1), look up area weights:

  ```python
  # Collect county weights from area-weighted crosswalk
  county_weights: dict[str, float] = {}  # fips -> weight

  if geoids and self._area_weights:
      for geoid in geoids:
          entries = self._area_weights.get(geoid, [])
          for entry in entries:
              fips = entry["county_fips"]
              weight = entry["weight"]
              # If same county appears via multiple GEOIDs, sum the weights
              county_weights[fips] = county_weights.get(fips, 0.0) + weight
      county_fips_set = set(county_weights.keys())
  ```

  If `county_weights` is empty but `tribal_counties` (the relational CSV) has data, fall back to equal weights:
  ```python
  if not county_weights and geoids and tribal_counties:
      for geoid in geoids:
          for fips in tribal_counties.get(geoid, []):
              county_fips_set.add(fips)
      # Equal weights fallback
      if county_fips_set:
          equal_w = 1.0 / len(county_fips_set)
          county_weights = {fips: equal_w for fips in county_fips_set}
  ```

  Keep the existing state-level fallback as last resort (also with equal weights).

  **Step 4 replacement -- Area-weighted composite scores:**
  ```python
  # Normalize weights
  total_w = sum(county_weights.get(c["fips"], 0.0) for c in matched_counties)
  if total_w == 0:
      total_w = len(matched_counties)  # Equal weight fallback
      norm_weights = {c["fips"]: 1.0 / total_w for c in matched_counties}
  else:
      norm_weights = {c["fips"]: county_weights.get(c["fips"], 0.0) / total_w for c in matched_counties}

  # Percentile scores: weighted AVERAGE
  composite = {
      "risk_score": sum(c["risk_score"] * norm_weights[c["fips"]] for c in matched_counties),
      "eal_score": sum(c["eal_score"] * norm_weights[c["fips"]] for c in matched_counties),
      "sovi_score": sum(c["sovi_score"] * norm_weights[c["fips"]] for c in matched_counties),
      "resl_score": sum(c["resl_score"] * norm_weights[c["fips"]] for c in matched_counties),
      # Dollar amounts: weighted SUM (proportional to overlap area)
      "eal_total": sum(c["eal_total"] * norm_weights[c["fips"]] for c in matched_counties),
  }

  # Re-derive ratings from weighted scores using quintile thresholds
  composite["risk_rating"] = score_to_rating(composite["risk_score"])
  composite["eal_rating"] = score_to_rating(composite["eal_score"])
  composite["sovi_rating"] = score_to_rating(composite["sovi_score"])
  composite["resl_rating"] = score_to_rating(composite["resl_score"])
  ```

  Note: `eal_rating` was missing from the old composite -- add it now from `eal_score`.

  **Step 5 replacement -- Area-weighted per-hazard scores + non-zero filter:**
  ```python
  all_hazards: dict[str, dict] = {}
  for code in NRI_HAZARD_CODES:
      hazard_records = [
          (c["hazards"][code], norm_weights[c["fips"]])
          for c in matched_counties
          if code in c.get("hazards", {})
      ]
      if not hazard_records:
          continue  # Skip -- only non-zero hazards stored

      weighted_risk = sum(h["risk_score"] * w for h, w in hazard_records)
      weighted_eal = sum(h["eal_total"] * w for h, w in hazard_records)
      weighted_freq = sum(h["annualized_freq"] * w for h, w in hazard_records)
      total_events = sum(h["num_events"] * w for h, w in hazard_records)

      # Only store if risk_score is non-zero (per CONTEXT.md)
      if weighted_risk <= 0:
          continue

      all_hazards[code] = {
          "risk_score": round(weighted_risk, 2),
          "risk_rating": score_to_rating(weighted_risk),
          "eal_total": round(weighted_eal, 2),
          "annualized_freq": round(weighted_freq, 4),
          "num_events": round(total_events, 2),
      }
  ```

  **Step 6 replacement -- Top 5 hazards (was top 3):**
  ```python
  sorted_hazards = sorted(
      all_hazards.items(),
      key=lambda x: x[1]["risk_score"],
      reverse=True,
  )
  top_hazards = [
      {
          "type": NRI_HAZARD_CODES[code],
          "code": code,
          "risk_score": data["risk_score"],
          "risk_rating": data["risk_rating"],
          "eal_total": data["eal_total"],
      }
      for code, data in sorted_hazards[:5]
  ]
  ```

  **1e. Remove _max_rating() and _min_rating() static methods:**

  These are no longer needed since score_to_rating() re-derives ratings from weighted scores. Remove both methods. If any other code references them, replace with score_to_rating() calls.

  **1f. Update _empty_nri_profile():**

  Change `"all_hazards"` from the dict with all 18 zero entries to an empty dict `{}` (consistent with non-zero-only storage). Also update `"version"` to not be hardcoded -- pass version as parameter or use empty string:

  ```python
  def _empty_nri_profile(self, note: str) -> dict:
      return {
          "version": "",
          "counties_analyzed": 0,
          "note": note,
          "composite": {
              "risk_score": 0.0,
              "risk_rating": "",
              "eal_total": 0.0,
              "eal_score": 0.0,
              "sovi_score": 0.0,
              "sovi_rating": "",
              "resl_score": 0.0,
              "resl_rating": "",
          },
          "top_hazards": [],
          "all_hazards": {},
      }
  ```

  **1g. Update version tracking in _build_tribe_nri_profile return:**

  Change `"version": "1.20"` to read from actual NRI data if possible, or pass it through. A simple approach: detect version from the NRI CSV filename or a metadata field. If neither available, use `"NRI_v1.20"` as a string that documents the source (not just a bare version number). Better yet, extract from the first county record's data or store as class attribute loaded during `_load_nri_county_data()`.
  </action>
  <verify>
  1. `python -c "from src.packets.hazards import score_to_rating; print(score_to_rating(85))"` prints "Very High"
  2. `python -c "from src.packets.hazards import score_to_rating; print(score_to_rating(0))"` prints "Very Low"
  3. `python -c "from src.packets.hazards import HazardProfileBuilder"` imports without error
  4. `python -m pytest tests/ -x -q` -- all existing tests pass
  5. `ruff check src/packets/hazards.py` -- lint clean
  </verify>
  <done>
  - score_to_rating() function exists and correctly maps 0-100 scores to 5 NRI rating strings
  - _build_tribe_nri_profile() uses area-weighted averaging for all percentile scores
  - EAL dollar amounts use weighted sum (proportional to overlap area)
  - Rating strings re-derived from weighted scores via quintile breakpoints
  - Only non-zero hazards stored in all_hazards
  - Top 5 hazards extracted (not top 3)
  - _max_rating() and _min_rating() removed (replaced by score_to_rating())
  - Empty profiles use empty all_hazards dict instead of 18-entry zero dict
  </done>
</task>

<task type="auto">
  <name>Task 2: Write unit tests for area-weighted aggregation</name>
  <files>tests/test_hazard_aggregation.py</files>
  <action>
  Create `tests/test_hazard_aggregation.py` with focused unit tests for the new aggregation logic.

  **Test the module-level score_to_rating() function:**

  ```python
  import pytest
  from src.packets.hazards import score_to_rating

  class TestScoreToRating:
      def test_very_high(self):
          assert score_to_rating(80) == "Very High"
          assert score_to_rating(100) == "Very High"
          assert score_to_rating(95.5) == "Very High"

      def test_relatively_high(self):
          assert score_to_rating(60) == "Relatively High"
          assert score_to_rating(79.9) == "Relatively High"

      def test_relatively_moderate(self):
          assert score_to_rating(40) == "Relatively Moderate"
          assert score_to_rating(59.9) == "Relatively Moderate"

      def test_relatively_low(self):
          assert score_to_rating(20) == "Relatively Low"
          assert score_to_rating(39.9) == "Relatively Low"

      def test_very_low(self):
          assert score_to_rating(0) == "Very Low"
          assert score_to_rating(19.9) == "Very Low"

      def test_boundary_values(self):
          # Exact boundary values go to the higher bracket
          assert score_to_rating(80) == "Very High"
          assert score_to_rating(60) == "Relatively High"
          assert score_to_rating(40) == "Relatively Moderate"
          assert score_to_rating(20) == "Relatively Low"
  ```

  **Test area-weighted aggregation with mock data:**

  Create a test class that builds mock county data and weights, then verifies the aggregation produces correct weighted averages. Test cases:

  1. **Single county** (weight 1.0): scores pass through unchanged
  2. **Two equal-weight counties** (0.5 each): scores average exactly
  3. **Two unequal-weight counties** (0.7 and 0.3): scores reflect area proportions
  4. **Non-zero filtering**: hazards with zero risk_score are omitted from all_hazards
  5. **Top 5 extraction**: with >5 non-zero hazards, only top 5 by risk_score appear in top_hazards
  6. **EAL dollar amounts**: use weighted sum, not average

  To test the aggregation without needing actual files, mock the data loading methods using `unittest.mock.patch` or create a HazardProfileBuilder with a test config and inject mock data via `_build_tribe_nri_profile()` directly by constructing the inputs.

  Simpler approach: since `_build_tribe_nri_profile` is a method that takes county_data and tribal_counties as arguments, create test data and call it directly:

  ```python
  class TestAreaWeightedAggregation:
      @pytest.fixture
      def builder(self, tmp_path):
          """Create a minimal HazardProfileBuilder for testing."""
          # Create minimal crosswalk file
          crosswalk = {"mappings": {"1234": "test_tribe"}}
          crosswalk_path = tmp_path / "crosswalk.json"
          crosswalk_path.write_text(json.dumps(crosswalk), encoding="utf-8")

          # Create minimal area weights file
          weights = {
              "metadata": {"total_aiannh_entities": 1, "total_county_links": 2},
              "crosswalk": {
                  "1234": [
                      {"county_fips": "04013", "overlap_area_sqkm": 100, "weight": 0.7},
                      {"county_fips": "04021", "overlap_area_sqkm": 42.9, "weight": 0.3},
                  ]
              }
          }
          weights_path = tmp_path / "weights.json"
          weights_path.write_text(json.dumps(weights), encoding="utf-8")

          # ... create builder with mocked config pointing to these files
      ```

  Use `unittest.mock.patch` to mock `TRIBAL_COUNTY_WEIGHTS_PATH` and `AIANNH_CROSSWALK_PATH` to point to the test fixtures.

  Alternatively, test `score_to_rating()` thoroughly (it's a pure function) and test the key calculation inline without needing a full builder instance. The builder integration is verified by the end-to-end run in Plan 13-03.
  </action>
  <verify>
  1. `python -m pytest tests/test_hazard_aggregation.py -v` -- all tests pass
  2. `python -m pytest tests/ -x -q` -- all tests pass (no regressions)
  3. `ruff check tests/test_hazard_aggregation.py` -- lint clean
  </verify>
  <done>
  - tests/test_hazard_aggregation.py exists with 10+ tests
  - score_to_rating() has comprehensive boundary-value tests
  - Area-weighted aggregation logic has at least 3 test scenarios (single-county, equal-weight, unequal-weight)
  - Non-zero filtering verified: zero-score hazards excluded from all_hazards
  - Top-5 extraction verified: correct count and ordering
  - HZRD-03 satisfied (area-weighted aggregation with tests)
  - HZRD-04 satisfied (top 5 hazards extracted and ranked)
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.hazards import score_to_rating; print(score_to_rating(75))"` prints "Relatively High"
2. `python -c "from src.packets.hazards import HazardProfileBuilder"` imports without error
3. `python -m pytest tests/test_hazard_aggregation.py -v` -- new tests pass
4. `python -m pytest tests/ -x -q` -- all tests pass (no regressions)
5. `ruff check src/packets/hazards.py tests/test_hazard_aggregation.py` -- lint clean
</verification>

<success_criteria>
- County NRI scores are aggregated to Tribe level using area-weighted averaging (HZRD-03)
- Top 5 hazards per Tribe extracted and ranked by risk_score descending (HZRD-04)
- score_to_rating() maps 0-100 scores to NRI rating strings via quintile breakpoints
- Non-zero hazard filtering: only hazards with risk_score > 0 stored in profiles
- EAL dollar amounts use weighted sum; percentile scores use weighted average
- Comprehensive unit tests cover aggregation correctness
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/13-hazard-population/13-02-SUMMARY.md`
</output>

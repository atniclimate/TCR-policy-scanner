---
phase: 13-hazard-population
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/download_nri_data.py
  - scripts/build_area_crosswalk.py
  - src/paths.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Running scripts/download_nri_data.py downloads NRI county CSV and tribal relational CSV to data/nri/"
    - "Running scripts/build_area_crosswalk.py produces data/nri/tribal_county_area_weights.json with area-weighted AIANNH-to-county mappings"
    - "The crosswalk JSON has per-GEOID entries with county_fips, overlap_area_sqkm, and weight fields that sum to ~1.0 per GEOID"
    - "A 1% minimum overlap filter removes sliver overlaps from the crosswalk"
    - "Alaska features use EPSG:3338 and CONUS features use EPSG:5070 for accurate equal-area calculation"
  artifacts:
    - path: "scripts/download_nri_data.py"
      provides: "NRI county CSV + tribal relational CSV download with progress and retry"
      min_lines: 80
    - path: "scripts/build_area_crosswalk.py"
      provides: "Geopandas-based AIANNH-to-county area intersection and weight computation"
      min_lines: 120
    - path: "src/paths.py"
      provides: "TRIBAL_COUNTY_WEIGHTS_PATH constant"
      contains: "TRIBAL_COUNTY_WEIGHTS_PATH"
    - path: "requirements.txt"
      provides: "geopandas, shapely, pyproj, pyogrio build-time dependencies"
      contains: "geopandas"
  key_links:
    - from: "scripts/build_area_crosswalk.py"
      to: "src/paths.py"
      via: "imports TRIBAL_COUNTY_WEIGHTS_PATH, NRI_DIR"
      pattern: "from src\\.paths import"
    - from: "scripts/download_nri_data.py"
      to: "src/paths.py"
      via: "imports NRI_DIR"
      pattern: "from src\\.paths import"
---

<objective>
Create download scripts for FEMA NRI county data and Census TIGER/Line shapefiles, then build the area-weighted AIANNH-to-county geographic crosswalk that maps 577+ AIANNH areas to their overlapping counties with precise area weights.

Purpose: The crosswalk is the critical bridge between federal county-level hazard data and Tribal geographic boundaries. Without area weights, multi-county Tribes get inflated risk scores (the current MAX aggregation problem). This plan creates the data foundation that Plan 13-02 consumes for area-weighted aggregation.

Output:
- `scripts/download_nri_data.py` -- downloads NRI county CSV + tribal relational CSV to `data/nri/`
- `scripts/build_area_crosswalk.py` -- computes area-weighted AIANNH-to-county crosswalk
- `data/nri/tribal_county_area_weights.json` -- pre-computed crosswalk (created at runtime, not committed)
- Updated `src/paths.py` with TRIBAL_COUNTY_WEIGHTS_PATH constant
- Updated `requirements.txt` with geospatial build-time dependencies
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-hazard-population/13-CONTEXT.md
@.planning/phases/13-hazard-population/13-RESEARCH.md
@src/paths.py
@scripts/build_registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create NRI data download script + add paths.py constant</name>
  <files>scripts/download_nri_data.py, src/paths.py, requirements.txt</files>
  <action>
  **1a. Add TRIBAL_COUNTY_WEIGHTS_PATH to src/paths.py:**

  Add this constant in the "Data Paths (subdirectories)" section, right after the USFS_DIR entry:

  ```python
  TRIBAL_COUNTY_WEIGHTS_PATH: Path = NRI_DIR / "tribal_county_area_weights.json"
  """Pre-computed area-weighted AIANNH-to-county crosswalk."""
  ```

  IMPORTANT: src/paths.py imports ONLY `pathlib.Path`. Do not add any other imports.

  **1b. Create scripts/download_nri_data.py:**

  Follow the exact same pattern as `scripts/build_registry.py` for:
  - sys.path insertion pattern (copy lines 17-25 of build_registry.py exactly)
  - argparse with --verbose flag
  - Logging setup

  The script downloads TWO files to `data/nri/`:
  1. NRI County CSV: `https://www.fema.gov/about/reports-and-data/openfema/nri/v120/NRI_Table_Counties.zip`
  2. NRI Tribal County Relational CSV: `https://www.fema.gov/about/reports-and-data/openfema/nri/v120/NRI_Table_Tribal_Counties.zip`

  Also downloads TWO Census TIGER/Line shapefiles (needed by build_area_crosswalk.py):
  3. AIANNH shapefile: `https://www2.census.gov/geo/tiger/TIGER2024/AIANNH/tl_2024_us_aiannh.zip`
  4. County shapefile: `https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/tl_2024_us_county.zip`

  Implementation details:
  - Use `requests` (already in requirements.txt) with streaming download + progress reporting
  - Each download: stream to temp file, then `os.replace()` to final path (atomic write pattern)
  - After download, extract ZIPs using `zipfile` stdlib
  - NRI ZIPs extract CSVs to `data/nri/`
  - Shapefile ZIPs extract to `data/nri/aiannh/` and `data/nri/county/` subdirectories
  - Skip download if the extracted file already exists (use --force flag to re-download)
  - Retry up to 2 times on connection error with 5-second delay
  - Timeout: 300 seconds for large files
  - Log file sizes after download
  - Import NRI_DIR from src.paths

  CLI: `python scripts/download_nri_data.py [--verbose] [--force]`

  **1c. Add geospatial dependencies to requirements.txt:**

  Add these lines with a comment block:

  ```
  # Geospatial (build-time only - for scripts/build_area_crosswalk.py)
  geopandas>=1.0.0
  shapely>=2.0.0
  pyproj>=3.3.0
  pyogrio>=0.7.2
  ```

  Place them after the existing dependencies, before any dev/test sections if they exist.
  </action>
  <verify>
  1. `python -c "from src.paths import TRIBAL_COUNTY_WEIGHTS_PATH; print(TRIBAL_COUNTY_WEIGHTS_PATH)"` prints the expected path
  2. `python scripts/download_nri_data.py --help` shows usage without errors
  3. `python -m pytest tests/ -x -q` -- all existing tests still pass
  </verify>
  <done>
  - TRIBAL_COUNTY_WEIGHTS_PATH exists in src/paths.py and resolves to `data/nri/tribal_county_area_weights.json`
  - download_nri_data.py is a working CLI script that can download 4 files (NRI county CSV, tribal relational CSV, AIANNH shapefile, county shapefile) to data/nri/
  - requirements.txt includes geopandas, shapely, pyproj, pyogrio
  - All existing tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create area-weighted crosswalk build script</name>
  <files>scripts/build_area_crosswalk.py</files>
  <action>
  Create `scripts/build_area_crosswalk.py` following the same pattern as `scripts/build_registry.py` for the sys.path insertion, argparse, and logging setup.

  The script reads AIANNH and county shapefiles, computes geometric intersections, calculates area overlap weights, and outputs a JSON crosswalk file.

  **Implementation steps:**

  1. **sys.path + imports:**
     ```python
     _SCRIPT_DIR = Path(__file__).resolve().parent
     _PROJECT_ROOT = _SCRIPT_DIR.parent
     if str(_PROJECT_ROOT) not in sys.path:
         sys.path.insert(0, str(_PROJECT_ROOT))

     import geopandas as gpd
     from src.paths import NRI_DIR, TRIBAL_COUNTY_WEIGHTS_PATH
     ```

  2. **Load shapefiles:**
     - AIANNH: `NRI_DIR / "aiannh"` directory (find the .shp file)
     - County: `NRI_DIR / "county"` directory (find the .shp file)
     - Use `gpd.read_file()` to load both

  3. **Split by Alaska vs CONUS:**
     - Alaska: STATEFP == "02" (both AIANNH and county features)
     - CONUS: everything else
     - Project Alaska features to EPSG:3338 (NAD83 Alaska Albers)
     - Project CONUS features to EPSG:5070 (NAD83 Conus Albers Equal Area)

  4. **Compute original AIANNH areas** (before intersection):
     - `aiannh_ea["aiannh_area"] = aiannh_ea.geometry.area`

  5. **Compute intersection overlay** (separately for Alaska and CONUS):
     - `intersection = gpd.overlay(aiannh_ea, counties_ea, how="intersection")`
     - `intersection["overlap_area"] = intersection.geometry.area`
     - Handle the GEOID column naming (overlay may suffix with `_1` and `_2`)

  6. **Build crosswalk dict:**
     - For each intersection row: compute `overlap_pct = overlap_area / aiannh_area`
     - Filter: skip if `overlap_pct < 0.01` (1% minimum per CONTEXT.md)
     - Key by AIANNH GEOID, value is list of {county_fips, overlap_area_sqkm, weight}

  7. **Normalize weights** so they sum to 1.0 per AIANNH GEOID:
     ```python
     for geoid, entries in crosswalk.items():
         total_weight = sum(e["weight"] for e in entries)
         if total_weight > 0:
             for e in entries:
                 e["weight"] = round(e["weight"] / total_weight, 6)
     ```

  8. **Combine Alaska + CONUS results** into a single crosswalk dict.

  9. **Write output JSON** to TRIBAL_COUNTY_WEIGHTS_PATH with metadata:
     ```json
     {
       "metadata": {
         "built_at": "ISO timestamp",
         "aiannh_source": "tl_2024_us_aiannh.zip",
         "county_source": "tl_2024_us_county.zip",
         "crs_conus": "EPSG:5070",
         "crs_alaska": "EPSG:3338",
         "min_overlap_pct": 0.01,
         "total_aiannh_entities": N,
         "total_county_links": M
       },
       "crosswalk": { ... }
     }
     ```
     Use atomic write pattern: write to temp file, then `os.replace()`.
     Always use `encoding="utf-8"`.

  10. **Log summary:** total AIANNH entities matched, total county links, any GEOIDs with zero matches.

  **Edge cases to handle:**
  - Empty geometry after intersection (skip with warning)
  - AIANNH areas with zero area (log and skip)
  - Topology errors in shapefiles: wrap overlay in try/except, log and skip problematic geometries
  - County FIPS codes: zero-pad to 5 characters (GEOID field in county shapefile should already be 5-char)

  CLI: `python scripts/build_area_crosswalk.py [--verbose] [--min-overlap 0.01]`
  </action>
  <verify>
  1. `python scripts/build_area_crosswalk.py --help` shows usage without import errors
  2. If shapefiles are present (after running download_nri_data.py): `python scripts/build_area_crosswalk.py --verbose` produces `data/nri/tribal_county_area_weights.json`
  3. Validate output: `python -c "import json; d=json.load(open('data/nri/tribal_county_area_weights.json')); print(f'GEOIDs: {len(d[\"crosswalk\"])}, Links: {d[\"metadata\"][\"total_county_links\"]}')"` shows reasonable numbers (expect 400-600 GEOIDs, 1000-3000 links)
  4. Spot-check weight normalization: `python -c "import json; d=json.load(open('data/nri/tribal_county_area_weights.json')); geoid=list(d['crosswalk'].keys())[0]; print(f'GEOID {geoid}: weights sum to {sum(e[\"weight\"] for e in d[\"crosswalk\"][geoid]):.4f}')"` shows ~1.0
  5. `python -m pytest tests/ -x -q` -- all existing tests still pass
  </verify>
  <done>
  - build_area_crosswalk.py is a working CLI script that computes area-weighted AIANNH-to-county crosswalk
  - Alaska features use EPSG:3338, CONUS features use EPSG:5070 for accurate area computation
  - 1% minimum overlap filter removes sliver intersections
  - Weights are normalized to sum to 1.0 per AIANNH GEOID
  - Output JSON includes metadata (source files, CRS, timestamps) and crosswalk dict
  - HZRD-01 satisfied (NRI data downloaded and cached)
  - HZRD-02 satisfied (Tribe-to-county geographic crosswalk built with area weights)
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.paths import TRIBAL_COUNTY_WEIGHTS_PATH; print(TRIBAL_COUNTY_WEIGHTS_PATH)"` -- constant exists
2. `python scripts/download_nri_data.py --help` -- script is importable and has CLI
3. `python scripts/build_area_crosswalk.py --help` -- script is importable and has CLI
4. If data files present: crosswalk JSON has expected structure with metadata and crosswalk keys
5. `python -m pytest tests/ -x -q` -- no regressions
6. `ruff check scripts/download_nri_data.py scripts/build_area_crosswalk.py src/paths.py` -- lint clean
</verification>

<success_criteria>
- FEMA NRI county-level dataset can be downloaded and cached locally (HZRD-01)
- Tribe-to-county geographic crosswalk maps AIANNH boundaries to overlapping counties with area weights (HZRD-02)
- Crosswalk uses dual-CRS approach (EPSG:5070 CONUS + EPSG:3338 Alaska) for accurate area calculation
- 1% minimum overlap filter removes sliver intersections
- Weights normalized to sum to 1.0 per AIANNH entity
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-hazard-population/13-01-SUMMARY.md`
</output>

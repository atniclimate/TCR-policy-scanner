---
phase: 13-hazard-population
plan: 03
type: execute
wave: 3
depends_on: ["13-01", "13-02"]
files_modified:
  - scripts/download_usfs_data.py
  - src/packets/hazards.py
  - scripts/populate_hazards.py
autonomous: true

must_haves:
  truths:
    - "Running scripts/download_usfs_data.py downloads USFS wildfire XLSX to data/usfs/"
    - "When USFS data exists for a fire-prone Tribe (NRI WFIR > 0), the USFS conditional risk to structures overrides the NRI WFIR entry in all_hazards"
    - "Original NRI WFIR score is preserved in usfs_wildfire.nri_wfir_original when USFS overrides"
    - "Running scripts/populate_hazards.py builds hazard profiles for all 592 Tribes and generates a coverage report"
    - "550+ of 592 hazard profile JSON files contain real scored data (non-zero risk_score)"
    - "A coverage report (JSON + Markdown) is generated at outputs/ with per-state breakdown"
  artifacts:
    - path: "scripts/download_usfs_data.py"
      provides: "USFS Wildfire Risk XLSX download script"
      min_lines: 60
    - path: "scripts/populate_hazards.py"
      provides: "End-to-end hazard profile population for 592 Tribes with coverage report"
      min_lines: 80
    - path: "src/packets/hazards.py"
      provides: "USFS override logic in build_all_profiles, coverage report generation"
      contains: "nri_wfir_original"
  key_links:
    - from: "src/packets/hazards.py"
      to: "data/usfs/*.xlsx"
      via: "_load_usfs_wildfire_data() reads XLSX and matches to Tribes"
      pattern: "_load_usfs_wildfire_data"
    - from: "scripts/populate_hazards.py"
      to: "src/packets/hazards.py"
      via: "imports HazardProfileBuilder and calls build_all_profiles()"
      pattern: "from src\\.packets\\.hazards import"
    - from: "src/packets/hazards.py"
      to: "outputs/hazard_coverage_report.json"
      via: "generates coverage report after population run"
      pattern: "coverage_report"
---

<objective>
Create USFS wildfire data download script, implement USFS override of NRI WFIR hazard data, build a CLI population script that runs the full hazard pipeline for 592 Tribes, and generate a coverage report. This completes the hazard population phase.

Purpose: Fire-prone Tribes (300+) get more actionable wildfire risk data from USFS (conditional risk to structures) than the generic NRI WFIR score. The population script is the entry point that orchestrates everything built in Plans 01 and 02, and the coverage report gives visibility into data quality for Phase 14 integration.

Output:
- `scripts/download_usfs_data.py` -- downloads USFS wildfire XLSX to data/usfs/
- Updated `src/packets/hazards.py` -- USFS override logic + coverage report generation
- `scripts/populate_hazards.py` -- CLI script that runs full hazard population
- 592 hazard profile JSON files in `data/hazard_profiles/` (550+ with real data)
- `outputs/hazard_coverage_report.json` + `outputs/hazard_coverage_report.md`
</objective>

<execution_context>
@D:\Claude-Workspace\.claude\get-shit-done\workflows\execute-plan.md
@D:\Claude-Workspace\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-hazard-population/13-CONTEXT.md
@.planning/phases/13-hazard-population/13-RESEARCH.md
@.planning/phases/13-hazard-population/13-01-SUMMARY.md
@.planning/phases/13-hazard-population/13-02-SUMMARY.md
@src/packets/hazards.py
@src/paths.py
@scripts/build_registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create USFS download script + implement USFS override in hazards.py</name>
  <files>scripts/download_usfs_data.py, src/packets/hazards.py</files>
  <action>
  **1a. Create scripts/download_usfs_data.py:**

  Follow the exact same pattern as `scripts/build_registry.py` for sys.path insertion, argparse, and logging.

  The script downloads the USFS Wildfire Risk to Communities XLSX:
  - URL: `https://wildfirerisk.org/wp-content/uploads/2025/05/wrc_download_202505.xlsx`
  - Destination: `data/usfs/wrc_download.xlsx` (normalize the filename)
  - Use `requests` with streaming download, progress reporting, retry (same pattern as download_nri_data.py)
  - Skip if file exists (unless --force flag)
  - Import USFS_DIR from src.paths
  - Timeout: 300 seconds

  CLI: `python scripts/download_usfs_data.py [--verbose] [--force]`

  **1b. Implement USFS override of NRI WFIR in hazards.py build_all_profiles():**

  In the `build_all_profiles()` method, after building the NRI profile and matching USFS data, add override logic:

  ```python
  # USFS wildfire override: replace NRI WFIR when USFS data exists
  # and Tribe is fire-prone (NRI WFIR score > 0)
  if usfs_match and nri_profile.get("all_hazards", {}).get("WFIR"):
      wfir_entry = nri_profile["all_hazards"]["WFIR"]
      nri_wfir_original = wfir_entry.get("risk_score", 0.0)

      # Override with USFS conditional risk to structures
      usfs_risk = usfs_match.get("risk_to_homes", 0.0)
      if usfs_risk > 0:
          wfir_entry["risk_score"] = usfs_risk
          wfir_entry["risk_rating"] = score_to_rating(usfs_risk)
          wfir_entry["source"] = "USFS"  # Mark the data source

          # Store original NRI value in usfs_wildfire section
          usfs_profile["nri_wfir_original"] = round(nri_wfir_original, 2)
  ```

  Also update the `usfs_profile` dict construction to include the `conditional_risk_to_structures` field. Search the USFS data for the column matching "conditional risk" or "risk to homes":

  ```python
  if usfs_match:
      usfs_matched += 1
      usfs_profile = {
          "risk_to_homes": usfs_match.get("risk_to_homes", 0.0),
          "conditional_risk_to_structures": usfs_match.get("risk_to_homes", 0.0),
          "wildfire_likelihood": usfs_match.get("wildfire_likelihood", 0.0),
          "source_name": usfs_match.get("name", ""),
      }
      if "additional_metrics" in usfs_match:
          usfs_profile["additional_metrics"] = usfs_match["additional_metrics"]
  ```

  After the USFS override, re-sort top_hazards since WFIR score may have changed:
  ```python
  # Re-extract top 5 after potential USFS override
  if usfs_match and "WFIR" in nri_profile.get("all_hazards", {}):
      sorted_hazards = sorted(
          nri_profile["all_hazards"].items(),
          key=lambda x: x[1]["risk_score"],
          reverse=True,
      )
      nri_profile["top_hazards"] = [
          {
              "type": NRI_HAZARD_CODES.get(code, code),
              "code": code,
              "risk_score": data["risk_score"],
              "risk_rating": data["risk_rating"],
              "eal_total": data["eal_total"],
          }
          for code, data in sorted_hazards[:5]
      ]
  ```

  **1c. Add coverage report generation to hazards.py:**

  Add a method `_generate_coverage_report()` to HazardProfileBuilder and call it at the end of `build_all_profiles()`.

  The report tracks:
  - Total profiles: 592
  - Profiles with NRI data (counties_analyzed > 0): count
  - Profiles with USFS data: count
  - Profiles with any scored data: count (target 550+)
  - Fully unmatched: count + list of Tribe names
  - Per-state breakdown: {state: {total, nri_matched, usfs_matched, unmatched, tribe_names}}

  Write TWO output files:
  1. `outputs/hazard_coverage_report.json` -- machine-readable
  2. `outputs/hazard_coverage_report.md` -- human-readable with tables

  The Markdown report should include:
  - Summary statistics
  - Per-state table with columns: State | Total Tribes | NRI Matched | USFS Matched | Unmatched
  - List of unmatched Tribes

  Use the OUTPUTS_DIR path from src.paths. Create the directory if needed.

  The method accepts the tracking data accumulated during `build_all_profiles()` (counts, unmatched list, per-state dict). Modify `build_all_profiles()` to track per-state stats as it iterates.

  **1d. Update the profile's note field:**

  For successfully populated profiles (counties_analyzed > 0), set `note` to `null` (omit the field from the dict). Only include `note` for profiles with issues (zero data, etc.). This means removing the `"note"` key from the return dict of `_build_tribe_nri_profile()` when it succeeds, or simply not including it.

  **1e. Track NRI dataset version dynamically:**

  In `_load_nri_county_data()`, detect the version from the CSV filename or first few rows. Store as `self._nri_version`. If detection fails, fall back to `"NRI_v1.20"`. Use this in the profile instead of hardcoded `"1.20"`.
  </action>
  <verify>
  1. `python scripts/download_usfs_data.py --help` -- shows usage without errors
  2. `python -c "from src.packets.hazards import HazardProfileBuilder"` -- imports cleanly
  3. `python -m pytest tests/ -x -q` -- all tests pass
  4. `ruff check scripts/download_usfs_data.py src/packets/hazards.py` -- lint clean
  </verify>
  <done>
  - download_usfs_data.py downloads USFS XLSX to data/usfs/
  - USFS conditional-risk-to-structures overrides NRI WFIR for fire-prone Tribes
  - Original NRI WFIR score preserved in usfs_wildfire.nri_wfir_original
  - Coverage report generation method produces JSON + Markdown outputs
  - NRI version tracked dynamically from actual data
  - Note field cleared for successfully populated profiles
  </done>
</task>

<task type="auto">
  <name>Task 2: Create population CLI script and run full population</name>
  <files>scripts/populate_hazards.py</files>
  <action>
  Create `scripts/populate_hazards.py` as the end-to-end CLI entry point.

  Follow the exact same pattern as `scripts/build_registry.py` for sys.path insertion, argparse, and logging.

  The script:
  1. Loads scanner config from `config/scanner_config.json`
  2. Creates a `HazardProfileBuilder` instance
  3. Calls `build_all_profiles()` to populate all 592 hazard profile JSONs
  4. Reports results (count, timing, coverage summary)

  ```python
  """Populate hazard profiles for all 592 Tribes.

  Runs the full FEMA NRI + USFS wildfire data pipeline:
    1. Loads NRI county CSV and tribal relational CSV from data/nri/
    2. Loads pre-computed area-weighted crosswalk from data/nri/tribal_county_area_weights.json
    3. Loads USFS wildfire XLSX from data/usfs/
    4. Builds area-weighted hazard profiles for each Tribe
    5. Writes 592 JSON profile files to data/hazard_profiles/
    6. Generates coverage report to outputs/

  Prerequisites:
    - Run scripts/download_nri_data.py first (downloads NRI data + shapefiles)
    - Run scripts/build_area_crosswalk.py second (computes area weights)
    - Run scripts/download_usfs_data.py third (downloads USFS data)

  Usage:
      python scripts/populate_hazards.py                  # Default
      python scripts/populate_hazards.py --verbose         # Debug logging
      python scripts/populate_hazards.py --dry-run         # Show what would be done
  """
  ```

  CLI flags:
  - `--verbose` -- debug logging
  - `--dry-run` -- load data and report what would happen without writing profiles
  - `--tribe TRIBE_ID` -- build profile for a single Tribe (debugging)

  After running `build_all_profiles()`:
  - Log total time
  - Log profile count, NRI match count, USFS match count
  - Log coverage percentage (target: 550+ / 592 = 92.9%+)
  - Exit with code 0 on success, 1 on failure

  **IMPORTANT:** After writing this script, run it to verify it works against the actual downloaded data. If the data hasn't been downloaded yet, verify the script at least imports cleanly and shows correct --help output.

  If data IS available (NRI CSV + crosswalk JSON + USFS XLSX exist), run:
  ```bash
  python scripts/populate_hazards.py --verbose
  ```

  Verify:
  - 592 files written to data/hazard_profiles/
  - Coverage report generated at outputs/hazard_coverage_report.json and .md
  - Check a few profiles: `python -c "import json; d=json.load(open('data/hazard_profiles/1.json')); print(d['sources']['fema_nri']['counties_analyzed'], len(d['sources']['fema_nri']['top_hazards']))"`
  - Count profiles with real data: should be 550+
  </action>
  <verify>
  1. `python scripts/populate_hazards.py --help` -- shows usage
  2. If data present: `python scripts/populate_hazards.py --verbose` completes successfully
  3. Count populated profiles: `python -c "import json, pathlib; profiles=[json.load(open(p)) for p in pathlib.Path('data/hazard_profiles').glob('*.json')]; scored=[p for p in profiles if p.get('sources',{}).get('fema_nri',{}).get('counties_analyzed',0) > 0]; print(f'{len(scored)}/{len(profiles)} profiles have scored data')"` shows 550+/592
  4. Coverage report exists: `ls outputs/hazard_coverage_report.*` shows .json and .md
  5. Spot-check top hazards count: `python -c "import json; d=json.load(open('data/hazard_profiles/1.json')); print(len(d['sources']['fema_nri']['top_hazards']))"` shows up to 5
  6. `python -m pytest tests/ -x -q` -- all tests pass
  </verify>
  <done>
  - populate_hazards.py is a working CLI script that orchestrates the full hazard pipeline
  - 592 hazard profile JSON files written with real scored data
  - 550+ profiles have non-zero risk scores (HZRD-06)
  - 300+ fire-prone Tribes have USFS wildfire data (HZRD-05)
  - Coverage report generated in JSON + Markdown format
  - HZRD-05 satisfied (USFS wildfire data populated for 300+ fire-prone Tribes)
  - HZRD-06 satisfied (550+ hazard profiles contain real scored data)
  </done>
</task>

</tasks>

<verification>
1. `python scripts/download_usfs_data.py --help` -- script works
2. `python scripts/populate_hazards.py --help` -- script works
3. If data present: full population run produces 550+ scored profiles
4. Coverage report exists at outputs/hazard_coverage_report.{json,md}
5. USFS override: at least one profile has `usfs_wildfire.nri_wfir_original` field
6. `python -m pytest tests/ -x -q` -- all tests pass
7. `ruff check scripts/ src/packets/hazards.py` -- lint clean
</verification>

<success_criteria>
- USFS wildfire risk data populated for fire-prone Tribes (300+) (HZRD-05)
- 550+ of 592 hazard profile JSON files contain real scored data (HZRD-06)
- USFS data overrides NRI WFIR with conditional risk to structures
- Original NRI WFIR preserved for audit trail
- Coverage report generated with per-state breakdown
- Population script is a simple CLI entry point that chains all data sources
</success_criteria>

<output>
After completion, create `.planning/phases/13-hazard-population/13-03-SUMMARY.md`
</output>

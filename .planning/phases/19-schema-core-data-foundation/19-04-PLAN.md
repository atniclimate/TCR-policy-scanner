---
phase: 19-schema-core-data-foundation
plan: 04
type: execute
wave: 2
depends_on: ["19-01", "19-02"]
files_modified:
  - src/packets/nri_expanded.py
  - tests/test_nri_expanded.py
autonomous: true

must_haves:
  truths:
    - "NRI CSV loads with SHA256 checksum validation and column header verification"
    - "National percentile computed from RISK_SCORE rank across all counties (not read from CSV)"
    - "EAL breakdown by consequence type (buildings, population, agriculture) extracted per county"
    - "Community resilience score (RESL_SCORE) extracted per county"
    - "Connecticut planning region FIPS codes (09110-09190) correctly handled for crosswalk compatibility"
    - "Area-weighted aggregation produces per-Tribe expanded NRI profiles using existing crosswalk"
    - "All 573+ existing Tribes with hazard data also have expanded NRI data"
    - "Atomic writes with utf-8 encoding on all output files"
  artifacts:
    - path: "src/packets/nri_expanded.py"
      provides: "NRIExpandedBuilder following 9-step HazardProfileBuilder template"
      contains: "class NRIExpandedBuilder"
    - path: "tests/test_nri_expanded.py"
      provides: "Tests for NRI expansion including CT FIPS, percentile, SHA256"
      contains: "test_national_percentile"
  key_links:
    - from: "src/packets/nri_expanded.py"
      to: "src/paths.py"
      via: "import NRI_DIR, TRIBAL_COUNTY_WEIGHTS_PATH, AIANNH_CROSSWALK_PATH"
      pattern: "from src.paths import"
    - from: "src/packets/nri_expanded.py"
      to: "src/schemas/vulnerability.py"
      via: "NRIExpanded model for output validation"
      pattern: "from src.schemas.vulnerability import NRIExpanded"
    - from: "src/packets/nri_expanded.py"
      to: "src/packets/hazards.py"
      via: "_safe_float() reuse, crosswalk loading pattern"
      pattern: "_safe_float"
---

<objective>
Build the NRI expanded metrics extraction pipeline: SHA256 version pinning (XCUT-03), national percentile computation (18.5-DEC-01), EAL breakdown by consequence type, community resilience score, and Connecticut FIPS handling -- all following the HazardProfileBuilder 9-step template.

Purpose: VULN-01 (expanded FEMA NRI metrics) and XCUT-03 (NRI version pinning + CT planning region mapping). This provides the hazard exposure component (0.40 weight) of the composite vulnerability score.

Output: `src/packets/nri_expanded.py` with NRIExpandedBuilder class that reads the existing NRI CSV, extracts expanded metrics not captured by the original hazard profiles, and writes per-Tribe expanded NRI JSON.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-schema-core-data-foundation/19-RESEARCH.md

@src/packets/hazards.py -- 9-step builder template (MANDATORY pattern to follow), _safe_float(), crosswalk loading, area-weighted aggregation, atomic writes
@src/paths.py -- NRI_DIR, TRIBAL_COUNTY_WEIGHTS_PATH, AIANNH_CROSSWALK_PATH (+ new constants from Plan 01)
@src/schemas/vulnerability.py -- NRIExpanded model (from Plan 02)
@.planning/phases/18.5-architectural-review/sovereignty-scout-report.md -- Confirmed NRI field names
@.planning/phases/19-schema-core-data-foundation/19-01-SUMMARY.md -- Plan 01 output (path constants)
@.planning/phases/19-schema-core-data-foundation/19-02-SUMMARY.md -- Plan 02 output (schema models)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement NRIExpandedBuilder with version pinning and CT FIPS handling</name>
  <files>src/packets/nri_expanded.py</files>
  <action>
    Create `src/packets/nri_expanded.py` following the 9-step HazardProfileBuilder template from `src/packets/hazards.py`. The builder reads the SAME NRI CSV already used by `hazards.py` but extracts ADDITIONAL fields not captured in the original hazard profiles.

    **Step 1: __init__(config: dict)**
    - Accept optional config dict with keys: `nri_csv_path`, `expected_sha256`, `output_dir`
    - Default `nri_csv_path` to `NRI_DIR / "NRI_Table_Counties.csv"`
    - Default `output_dir` to a subdirectory under the vulnerability profiles dir
    - Resolve paths via `_resolve_path()` pattern from hazards.py
    - Load crosswalk and area weights (Steps 2-3)

    **Step 2-3: _load_crosswalk() and _load_area_weights()**
    - Reuse IDENTICAL pattern from hazards.py lines 126-230
    - Import from `src.paths`: AIANNH_CROSSWALK_PATH, TRIBAL_COUNTY_WEIGHTS_PATH
    - Load `aiannh_tribe_crosswalk.json` for GEOID -> tribe_id mapping
    - Load `tribal_county_area_weights.json` for GEOID -> [{county_fips, weight}]

    **Step 4: _load_nri_csv() with version pinning (XCUT-03)**
    - Compute SHA256 of CSV file using `hashlib.sha256()` in 64KB chunks
    - If `expected_sha256` provided, compare and log WARNING (not error) on mismatch
    - Open CSV with `encoding="utf-8-sig"` (handles BOM)
    - Validate required column headers exist: STCOFIPS, RISK_SCORE, RISK_RATNG, EAL_VALT, EAL_VALB, EAL_VALP, EAL_VALA, EAL_VALPE, SOVI_SCORE, RESL_SCORE, POPULATION, BUILDVALUE
    - Log WARNING for any missing headers (graceful degradation, not crash)
    - Parse each row into a dict keyed by STCOFIPS (5-digit, zero-padded)
    - Apply `_safe_float()` to ALL numeric fields
    - Handle CT FIPS: Detect if CSV uses new planning region codes (09110-09190) or legacy codes (09001-09015). If new codes found, remap to legacy codes for crosswalk compatibility. If legacy codes found, no remapping needed. Log the detection result.
    - Return dict[str, dict] keyed by county FIPS

    **Connecticut FIPS mapping table (XCUT-03):**
    The many-to-many relationship means we need a best-fit mapping. Since only 2 CT Tribes exist, and the crosswalk already has correct FIPS codes, the primary concern is matching NRI CSV codes to crosswalk codes. Store the mapping as a module constant:
    ```python
    CT_LEGACY_TO_PLANNING = {
        "09001": "09120",  # Fairfield -> Western CT
        "09003": "09160",  # Hartford -> Capitol
        "09005": "09190",  # Litchfield -> Northwest Hills
        "09007": "09130",  # Middlesex -> Lower CT River Valley
        "09009": "09170",  # New Haven -> South Central CT
        "09011": "09140",  # New London -> Southeastern CT
        "09013": "09180",  # Tolland -> Northeastern CT
        "09015": "09150",  # Windham -> Windham
    }
    CT_PLANNING_TO_LEGACY = {v: k for k, v in CT_LEGACY_TO_PLANNING.items()}
    ```
    At CSV load time: check which format the crosswalk uses (read a few county_fips values). Remap NRI CSV codes to match the crosswalk format.

    **Step 5: compute_national_percentiles()**
    - From all loaded county data, collect all RISK_SCORE values > 0
    - Sort ascending
    - For each county, compute percentile: `rank / (N - 1) * 100` (0-indexed rank gives 0-100 scale)
    - Handle edge cases: single county -> 50.0, zero score -> 0.0
    - Use `bisect.bisect_left` for O(log n) rank lookup
    - Return dict[str, float] keyed by county FIPS

    **Step 6: _aggregate_tribe_nri(tribe_id, geoids)**
    - Collect county weights from crosswalk (sum when same county via multiple GEOIDs)
    - Match to loaded county data
    - Normalize weights to matched counties only
    - Compute area-weighted values for: risk_score, eal_total, eal_buildings, eal_population, eal_agriculture, eal_population_equivalence, community_resilience (RESL_SCORE), social_vulnerability_nri (SOVI_SCORE)
    - Compute risk_percentile: look up weighted risk_score in the national percentile distribution
    - Count non-zero hazard types, identify top 5 by EAL
    - Compute coverage_pct = sum of matched weights / sum of all weights
    - Return dict matching NRIExpanded schema fields

    **Step 7: build_all_profiles()**
    - Iterate all tribe_ids from crosswalk
    - Call _aggregate_tribe_nri for each
    - Validate output against NRIExpanded Pydantic model (log errors, don't crash)
    - Write per-Tribe JSON via atomic write (Step 8)
    - Return count of profiles built

    **Step 8: _atomic_write(tribe_id, data)**
    - Path traversal guard: `Path(tribe_id).name` + reject `..`
    - `tempfile.mkstemp()` + `os.fdopen(encoding="utf-8")` + `json.dump(indent=2, ensure_ascii=False)` + `os.replace()`
    - `contextlib.suppress(OSError)` for temp file cleanup on exception

    **Step 9: generate_coverage_report()**
    - Count profiles built, coverage percentage
    - Per-state breakdown of Tribes with expanded NRI data
    - Log NRI version metadata (SHA256, row count, column count)
    - Output JSON + markdown summary

    **Helper: validate_nri_checksum(csv_path, expected_sha)**
    - Public function for external validation (scripts can call this)
    - Returns dict with version, sha256, row_count, column_count, validated, missing_headers

    **Import from hazards.py:** Import `_safe_float` directly from `src.packets.hazards` (do NOT duplicate). If the function is module-private (underscore prefix), refactor it to a shared utility OR duplicate with a comment noting the source. Prefer import if possible since it's the same module ecosystem.

    CRITICAL: Use `logging.getLogger(__name__)` for all logging.
  </action>
  <verify>
    Run: `python -c "from src.packets.nri_expanded import NRIExpandedBuilder, validate_nri_checksum; print('Import OK')"` -- succeeds.
    If NRI CSV exists: `python -c "from src.packets.nri_expanded import NRIExpandedBuilder; b = NRIExpandedBuilder({}); print(f'Loaded {len(b._county_data)} counties')"` -- prints county count.
  </verify>
  <done>
    NRIExpandedBuilder class implements all 9 builder steps: CSV loading with SHA256 validation, CT FIPS handling, national percentile computation, area-weighted aggregation, atomic writes. All code follows hazards.py patterns exactly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write NRI expanded builder tests</name>
  <files>tests/test_nri_expanded.py</files>
  <action>
    Create `tests/test_nri_expanded.py` with comprehensive test coverage:

    **TestSafeFloat (if duplicated, otherwise test via import):**
    - test_none_returns_default
    - test_empty_string_returns_default
    - test_nan_returns_default
    - test_inf_returns_default
    - test_valid_float_passthrough
    - test_valid_int_converted

    **TestCTFIPSMapping:**
    - test_legacy_to_planning_completeness: All 8 legacy CT counties mapped
    - test_planning_to_legacy_completeness: All planning regions mapped back
    - test_roundtrip_consistency: legacy -> planning -> legacy gives original
    - test_non_ct_fips_untouched: Non-09xxx FIPS codes are unaffected

    **TestNationalPercentile:**
    - test_three_counties_ranked: [10, 20, 30] -> [0.0, 50.0, 100.0]
    - test_single_county: One county gets 50.0 percentile
    - test_zero_score_gets_zero_percentile: Risk score 0 -> 0th percentile
    - test_tied_scores: Same scores get same rank position
    - test_percentile_range: All values in [0, 100]

    **TestNRIExpandedBuilder (unit tests with mocked data):**
    - test_builder_init_with_defaults: Default config resolves paths correctly
    - test_sha256_computation: Known file content produces expected hash
    - test_sha256_mismatch_logs_warning: Mismatched SHA logs warning, does not crash
    - test_header_validation_missing_column: Missing required column logs warning
    - test_area_weighted_aggregation_basic: 2 counties with known weights produce expected weighted average
    - test_weight_normalization_matched_only: Only matched counties contribute to normalization
    - test_zero_weight_fallback: When total weight is 0, equal weight 1/N used
    - test_coverage_pct_calculation: Correct fraction of matched weight
    - test_top_hazards_sorted_by_eal: Top 5 hazards ordered by EAL descending
    - test_hazard_count_non_zero_types: Only non-zero hazard types counted
    - test_path_traversal_rejection: tribe_id with ".." raises ValueError
    - test_atomic_write_creates_file: Valid write produces JSON file (use tmp_path fixture)
    - test_atomic_write_cleanup_on_error: Failed write does not leave temp file

    **TestNRIExpandedIntegration (conditional on data existence):**
    - Use `@pytest.mark.skipif(not NRI_DIR.exists(), reason="NRI data not available")` for tests that need real CSV
    - test_real_csv_loads_without_error: If CSV exists, loading succeeds
    - test_real_csv_has_required_headers: If CSV exists, all required headers present
    - test_real_csv_county_count: Expect ~3,200 rows

    All tests use pytest fixtures for test data. Mock the filesystem for unit tests -- do not depend on real NRI CSV for unit tests.
  </action>
  <verify>
    Run: `python -m pytest tests/test_nri_expanded.py -v` -- all tests pass.
    Run: `python -m pytest tests/ -x -q` -- all tests pass (no regressions).
  </verify>
  <done>
    Comprehensive test suite for NRI expanded builder with 25+ tests covering CT FIPS mapping, national percentile computation, SHA256 validation, area-weighted aggregation, atomic writes, and path traversal guards. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_nri_expanded.py -v` -- all NRI tests pass
2. `python -m pytest tests/ -x -q` -- all 964+ tests pass
3. `python -c "from src.packets.nri_expanded import NRIExpandedBuilder; print('Import OK')"` -- module loads without error
4. If NRI CSV available: `python -c "from src.packets.nri_expanded import validate_nri_checksum; from src.paths import NRI_DIR; print(validate_nri_checksum(NRI_DIR / 'NRI_Table_Counties.csv'))"` -- prints version metadata
</verification>

<success_criteria>
- NRIExpandedBuilder follows all 9 steps of HazardProfileBuilder template
- SHA256 checksum computed for NRI CSV with validation against expected hash
- National percentile computed from RISK_SCORE rank (not read from CSV -- RISK_NPCTL does not exist)
- EAL breakdown (buildings, population, agriculture, population_equivalence) extracted
- Community resilience score (RESL_SCORE) extracted
- Connecticut FIPS handling works for both legacy and planning region formats
- Area-weighted aggregation uses existing crosswalk with normalized weights
- Atomic writes with utf-8 encoding on all output files
- 25+ tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/19-schema-core-data-foundation/19-04-SUMMARY.md`
</output>

---
phase: 03-monitoring-logic
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/monitors/tribal_consultation.py
  - src/monitors/hot_sheets.py
  - data/program_inventory.json
  - src/main.py
  - src/monitors/__init__.py
autonomous: true

must_haves:
  truths:
    - "Tribal consultation signals (DTLLs, EO 13175 references, consultation notices) produce alerts in scan results"
    - "Hot Sheets sync validator compares scanner CI vs Hot Sheets positions and auto-overrides divergences"
    - "First-time divergence alerts prominently; subsequent runs log silently"
    - "Stale Hot Sheets positions (older than configurable threshold) produce warnings"
    - "After a scan, every program carries an advocacy_goal classification in the output"
    - "Monitor alerts and THREATENS edges flow through the pipeline from graph construction to reporting"
    - "Existing pipeline behavior is preserved -- scraping, scoring, graph building, and report generation still work"
  artifacts:
    - path: "src/monitors/tribal_consultation.py"
      provides: "Tribal consultation signal detector (MON-03)"
      contains: "class TribalConsultationMonitor"
    - path: "src/monitors/hot_sheets.py"
      provides: "Hot Sheets sync validator with override logic (MON-04)"
      contains: "class HotSheetsValidator"
    - path: "data/program_inventory.json"
      provides: "Programs with hot_sheets_status field"
      contains: "hot_sheets_status"
    - path: "src/main.py"
      provides: "Pipeline with monitors + decision engine stages"
      contains: "MonitorRunner"
  key_links:
    - from: "src/monitors/tribal_consultation.py"
      to: "scored_items"
      via: "regex pattern matching on title/abstract/action fields"
      pattern: "CONSULTATION_PATTERNS|dear.*tribal.*leader|EO.*13175"
    - from: "src/monitors/hot_sheets.py"
      to: "data/program_inventory.json"
      via: "reads hot_sheets_status field from program dict"
      pattern: "hot_sheets_status"
    - from: "src/main.py"
      to: "src/monitors/__init__.py"
      via: "imports MonitorRunner, calls run_all() after build_graph()"
      pattern: "from src.monitors import MonitorRunner"
    - from: "src/main.py"
      to: "src/analysis/decision_engine.py"
      via: "imports DecisionEngine, calls classify_all() after monitors"
      pattern: "from src.analysis.decision_engine import DecisionEngine"
    - from: "src/monitors/hot_sheets.py"
      to: "program dict ci_status"
      via: "mutates in-memory program dict to apply Hot Sheets override"
      pattern: "original_ci_status.*ci_status"
---

<objective>
Complete the monitoring suite with Tribal consultation tracker and Hot Sheets validator, add hot_sheets_status data to program inventory, and wire all monitors plus the decision engine into the main pipeline. This plan connects all Phase 3 components into a working pipeline stage.

Purpose: Closes the loop -- monitors detect, decision engine classifies, and the pipeline orchestrates it all. After this plan, running `python -m src.main` executes the full enhanced pipeline.
Output: Two remaining monitors, updated program data, and pipeline integration in main.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-monitoring-logic/03-RESEARCH.md
@.planning/phases/03-monitoring-logic/03-01-SUMMARY.md
@.planning/phases/03-monitoring-logic/03-02-SUMMARY.md

@src/main.py
@src/monitors/__init__.py
@src/analysis/decision_engine.py
@src/graph/builder.py
@src/reports/generator.py
@data/program_inventory.json
@config/scanner_config.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Tribal consultation monitor, Hot Sheets validator, and inventory data</name>
  <files>
    src/monitors/tribal_consultation.py
    src/monitors/hot_sheets.py
    data/program_inventory.json
  </files>
  <action>
    1. Create `src/monitors/tribal_consultation.py` -- TribalConsultationMonitor (MON-03):
       - Inherits BaseMonitor from src.monitors
       - Uses tiered regex pattern matching for consultation signals, compiled at module level (not per-call):
         - DTLL tier: r"dear\s+tribal\s+leader" (IGNORECASE), r"\bDTLL\b"
         - EO_13175 tier: r"Executive\s+Order\s+13175" (IGNORECASE), r"EO\s+13175", r"consultation\s+and\s+coordination\s+with\s+indian\s+tribal" (IGNORECASE)
         - CONSULTATION_NOTICE tier: r"tribal\s+consultation\s+(?:notice|meeting|session)" (IGNORECASE), r"government-to-government\s+consultation" (IGNORECASE), r"nation-to-nation\s+consultation" (IGNORECASE)
       - check() iterates scored_items, concatenates title + abstract + action into searchable text
       - For each item, checks all pattern tiers. One alert per signal_type per item (break after first match within a tier)
       - Severity: INFO for all consultation signals (they are informational, not threats)
       - Alert metadata includes: signal_type, source, source_id, url
       - program_ids from item.get("matched_programs", [])
       - Does NOT create THREATENS edges (consultations are signals, not threats)

    2. Create `src/monitors/hot_sheets.py` -- HotSheetsValidator (MON-04):
       - Inherits BaseMonitor from src.monitors
       - Reads config monitors.hot_sheets.staleness_days (default 90)
       - Uses state file `outputs/.monitor_state.json` for tracking known divergences between runs, following the pattern of `outputs/.cfda_tracker.json` (see src/scrapers/base.py lines 113-151)
       - __init__ loads existing state from outputs/.monitor_state.json if it exists (key: "hot_sheets_known_divergences" -> set of program_ids)
       - check() iterates over programs, for each program with a hot_sheets_status field:
         a. Staleness check: parse last_updated, compare to utcnow. If older than staleness_days, create WARNING alert.
         b. Divergence check: compare program["ci_status"] with hot_sheets_status["status"]. If they differ:
            - Check if pid is in known_divergences (loaded from state file)
            - If NEW divergence: severity WARNING, detail says "FIRST DETECTION - alerting prominently"
            - If KNOWN divergence: severity INFO, detail says "Previously known divergence"
            - Apply override: store program["original_ci_status"] = program["ci_status"], then set program["ci_status"] = hot_sheets_status["status"]
            - Add pid to known_divergences
       - After check(), save updated known_divergences to outputs/.monitor_state.json
       - IMPORTANT: Override mutates the in-memory program dict so downstream code (decision engine, reports) sees the Hot Sheets status. Do NOT write back to program_inventory.json file.

    3. Update `data/program_inventory.json` -- add `hot_sheets_status` field to each program:
       - For each of the 16+ programs, add a hot_sheets_status object:
         ```json
         "hot_sheets_status": {
           "status": "<same as ci_status for now -- aligned>",
           "last_updated": "2026-02-09",
           "source": "FY26 Hot Sheets v2"
         }
         ```
       - Set status to match the current ci_status for each program (no initial divergences)
       - This establishes the baseline. When Hot Sheets positions change, the user updates this field manually.
       - Preserve ALL existing program fields. Add hot_sheets_status as a new field per program.

    4. Update `src/monitors/__init__.py` -- add imports for the two new monitors:
       - Import TribalConsultationMonitor and HotSheetsValidator
       - Add them to MonitorRunner's monitor list
       - IMPORTANT: HotSheetsValidator must run BEFORE other monitors in the run order, because it applies CI status overrides that affect decision engine classifications. Order: HotSheetsValidator first, then threat monitors, then consultation monitor.
  </action>
  <verify>
    Run `python -c "from src.monitors.tribal_consultation import TribalConsultationMonitor; print('Tribal OK')"` -- imports.
    Run `python -c "from src.monitors.hot_sheets import HotSheetsValidator; print('HS OK')"` -- imports.
    Run `python -c "import json; inv=json.load(open('data/program_inventory.json')); p=inv['programs'][0]; assert 'hot_sheets_status' in p; print(f'{p[\"id\"]}: hs={p[\"hot_sheets_status\"][\"status\"]}, ci={p[\"ci_status\"]}')"` -- hot_sheets_status present.
    Run `python -c "
import json
from src.monitors import MonitorRunner
config = json.load(open('config/scanner_config.json'))
programs_data = json.load(open('data/program_inventory.json'))
programs = {p['id']: p for p in programs_data['programs']}
runner = MonitorRunner(config, programs)
graph_data = {'nodes': {}, 'edges': []}
alerts = runner.run_all(graph_data, [])
print(f'Total alerts: {len(alerts)}')
types = set(a.monitor for a in alerts)
print(f'Monitor types that fired: {types}')
"` -- runs all 5 monitors without error. Expected: iija_sunset and dhs_funding alerts (no reconciliation/consultation from empty items, no HS divergence since aligned).
  </verify>
  <done>
    TribalConsultationMonitor detects DTLL, EO 13175, and consultation notice patterns in scored items. HotSheetsValidator compares CI vs Hot Sheets, overrides divergences in-memory, and persists known divergences. All programs in inventory have hot_sheets_status. MonitorRunner orchestrates all 5 monitors in correct order.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire monitors and decision engine into pipeline</name>
  <files>
    src/main.py
  </files>
  <action>
    Modify `src/main.py` run_pipeline() to add monitor and decision engine stages between graph construction and reporting. The pipeline becomes:

    ```
    Ingest -> Normalize -> Graph Construction -> [Monitors -> Decision Engine] -> Reporting
    ```

    Specific changes to run_pipeline():

    1. After `graph_data = build_graph(programs, scored)` (line ~174), add:
       ```python
       # Stage 3.5: Monitors (detect threats, consultation signals, Hot Sheets sync)
       from src.monitors import MonitorRunner
       from src.analysis.decision_engine import DecisionEngine

       programs_dict = {p["id"]: p for p in programs}
       monitor_runner = MonitorRunner(config, programs_dict)
       alerts = monitor_runner.run_all(graph_data, scored)
       logger.info("Monitors produced %d alerts", len(alerts))

       # Stage 3.6: Decision Engine (classify programs into advocacy goals)
       decision_engine = DecisionEngine(config, programs_dict)
       classifications = decision_engine.classify_all(graph_data, alerts)
       logger.info("Classified %d programs into advocacy goals", len(classifications))
       ```

    2. Move imports to top of file (not inside function) for consistency with existing import style:
       ```python
       from src.monitors import MonitorRunner
       from src.analysis.decision_engine import DecisionEngine
       ```

    3. Before the reporting stage, attach monitor and decision data to the output:
       - Create a `monitor_data` dict containing:
         ```python
         monitor_data = {
             "alerts": [
                 {
                     "monitor": a.monitor,
                     "severity": a.severity,
                     "program_ids": a.program_ids,
                     "title": a.title,
                     "detail": a.detail,
                     "metadata": a.metadata,
                     "timestamp": a.timestamp,
                 }
                 for a in alerts
             ],
             "classifications": classifications,
             "summary": {
                 "total_alerts": len(alerts),
                 "critical_count": len([a for a in alerts if a.severity == "CRITICAL"]),
                 "warning_count": len([a for a in alerts if a.severity == "WARNING"]),
                 "info_count": len([a for a in alerts if a.severity == "INFO"]),
                 "monitors_run": list(set(a.monitor for a in alerts)),
             },
         }
         ```

    4. Pass monitor_data to ReportGenerator.generate() -- but do NOT modify ReportGenerator yet (that is Phase 4). Instead, pass it as an optional kwarg. Check if generate() accepts **kwargs; if not, just save monitor_data to a separate file for now:
       ```python
       # Save monitor data for Phase 4 reporting
       monitor_output_path = Path("outputs/LATEST-MONITOR-DATA.json")
       with open(monitor_output_path, "w") as f:
           json.dump(monitor_data, f, indent=2, default=str)
       logger.info("Monitor data written to %s", monitor_output_path)
       ```

    5. Update the console output summary to include monitor results:
       ```python
       print(f"  Monitor alerts: {len(alerts)} ({monitor_data['summary']['critical_count']} critical)")
       print(f"  Advocacy goals: {len([c for c in classifications.values() if c.get('advocacy_goal')])} classified")
       print(f"  Monitor data: {monitor_output_path}")
       ```

    6. For report_only and graph_only modes: monitors and decision engine should still run (they analyze cached data). Add the monitor/decision stages after graph construction in all code paths, not just the full-scan path.

    7. IMPORTANT: The programs list loaded by load_programs() returns a list of dicts. Monitors need a dict keyed by program_id. Create programs_dict = {p["id"]: p for p in programs} once and reuse. The Hot Sheets validator mutates program dicts in-place, which is fine since they are the same objects.

    8. Update the dry_run() function to show monitor configuration:
       ```python
       monitor_config = config.get("monitors", {})
       if monitor_config:
           print(f"\nMonitor configuration:")
           for name, mc in monitor_config.items():
               print(f"  - {name}")
       ```
  </action>
  <verify>
    Run `python -m src.main --dry-run` -- shows monitor configuration section in output, no errors.
    Run `python -m src.main --report-only` (if cached data exists) -- runs monitors and decision engine on cached data, produces LATEST-MONITOR-DATA.json, prints monitor/classification summary.
    Run `python -c "
import json
p = json.load(open('outputs/LATEST-MONITOR-DATA.json'))
print(f'Alerts: {p[\"summary\"][\"total_alerts\"]}')
print(f'Classifications: {len(p[\"classifications\"])}')
for pid, c in p['classifications'].items():
    print(f'  {pid}: {c.get(\"goal_label\", \"None\")} ({c.get(\"rule\", \"no rule\")})')
"` -- shows classifications for all programs.
    Verify `python -m src.main --graph-only` still works without error.
  </verify>
  <done>
    Running `python -m src.main` (or --report-only) executes the full enhanced pipeline: scrape -> score -> graph -> monitors -> decision engine -> report. Monitor alerts and advocacy goal classifications are saved to outputs/LATEST-MONITOR-DATA.json. Console output shows alert and classification summaries. Existing pipeline behavior (scraping, scoring, graphing, reporting) is preserved.
  </done>
</task>

</tasks>

<verification>
1. `python -m src.main --dry-run` -- runs without error, shows monitor config
2. `python -m src.main --report-only` -- full pipeline with monitors + decision engine on cached data
3. `outputs/LATEST-MONITOR-DATA.json` exists and contains alerts array + classifications dict
4. Every program in inventory has an advocacy goal classification (or explicit null with reason)
5. Hot Sheets validator loads hot_sheets_status from inventory, no initial divergences (aligned)
6. Tribal consultation monitor ready to detect DTLLs/EO 13175 (will fire when relevant items appear in scan)
7. `python -m pytest tests/test_decision_engine.py -v` -- decision engine tests still pass
8. No import errors: `python -c "from src.monitors import MonitorRunner; from src.analysis.decision_engine import DecisionEngine"`
</verification>

<success_criteria>
- Full pipeline runs: python -m src.main --report-only completes without errors
- LATEST-MONITOR-DATA.json produced with alerts and classifications
- All 5 monitors registered in MonitorRunner and executing
- Hot Sheets override logic works (override in-memory, persist state, first vs subsequent detection)
- Decision engine classifies all programs, results visible in output
- Pipeline stages execute in correct order: Graph -> HotSheets -> Monitors -> Decision -> Report
- Existing CLI modes (--dry-run, --graph-only, --report-only, --programs) still work
- No regressions in scraping or reporting
</success_criteria>

<output>
After completion, create `.planning/phases/03-monitoring-logic/03-03-SUMMARY.md`
</output>

---
phase: 15-congressional-intelligence
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scrapers/congress_gov.py
  - src/scrapers/federal_register.py
  - src/scrapers/grants_gov.py
  - src/scrapers/usaspending.py
autonomous: true

must_haves:
  truths:
    - "CongressGovScraper.scan() paginates through all pages using offset/limit and pagination.next, logging fetched vs total count"
    - "FederalRegisterScraper.scan() paginates through all pages using page parameter and total_pages from response"
    - "GrantsGovScraper.scan() paginates through all results using startRecordNum and hitCount from response"
    - "USASpendingScraper.scan() verifies existing pagination in fetch_tribal_awards methods and logs fetched vs total in _fetch_obligations"
    - "All 4 scrapers log WARNING when results are truncated by safety cap, never silently drop data"
  artifacts:
    - path: "src/scrapers/congress_gov.py"
      provides: "Paginated Congress.gov bill search"
      contains: "pagination"
    - path: "src/scrapers/federal_register.py"
      provides: "Paginated Federal Register search"
      contains: "total_pages"
    - path: "src/scrapers/grants_gov.py"
      provides: "Paginated Grants.gov search"
      contains: "hitCount"
    - path: "src/scrapers/usaspending.py"
      provides: "Verified pagination logging"
      contains: "fetched"
  key_links:
    - from: "src/scrapers/congress_gov.py"
      to: "src/scrapers/base.py"
      via: "_request_with_retry for each page fetch"
    - from: "src/scrapers/federal_register.py"
      to: "src/scrapers/base.py"
      via: "_request_with_retry for each page fetch"
---

<objective>
Harden all 4 scrapers for zero-truncation pagination. Each scraper's scan() or search methods must loop through all available pages, logging fetched vs total counts, and emitting explicit WARNING when hitting safety caps.

Purpose: INTEL-02 requires zero silent truncation. Currently 3 of 4 scrapers (Federal Register, Grants.gov, Congress.gov) fetch only a single page. USASpending already paginates in award methods but not in _fetch_obligations.

Output: All 4 scraper files modified with pagination loops and truncation detection.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/15-congressional-intelligence/15-RESEARCH.md
@src/scrapers/base.py
@src/scrapers/congress_gov.py
@src/scrapers/federal_register.py
@src/scrapers/grants_gov.py
@src/scrapers/usaspending.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pagination to Congress.gov and Federal Register scrapers</name>
  <files>src/scrapers/congress_gov.py, src/scrapers/federal_register.py</files>
  <action>
**A) Congress.gov pagination (congress_gov.py):**

Modify `_search_congress()` method to paginate:
1. Change limit from 50 to 250 (Congress.gov API max per page).
2. Add pagination loop that continues while `data.get("pagination", {}).get("next")` exists.
3. Use offset-based pagination: increment offset by limit each iteration.
4. Add safety cap at 2,500 results (10 pages) with WARNING log.
5. Add `await asyncio.sleep(0.3)` between page fetches.
6. Log total fetched count vs expected total from pagination.count if available.

Similarly modify `_search()` method with the same pagination pattern.

Both methods must log:
```python
logger.info("Congress.gov: fetched %d/%d bills for '%s'", len(all_bills), total_count, term)
```
If `len(all_bills) < total_count` and no safety cap hit, log WARNING:
```python
logger.warning("Congress.gov: truncated %d -> %d for '%s'", total_count, len(all_bills), term)
```

**B) Federal Register pagination (federal_register.py):**

Find the search/fetch method that queries the Federal Register API. Modify it to:
1. Read `total_pages` from response (Federal Register returns this field).
2. Loop from page 1 through total_pages, fetching each page with `page=N` parameter.
3. Add safety cap at 20 pages (1,000 results with per_page=50) with WARNING log.
4. Add `await asyncio.sleep(0.3)` between page fetches.
5. Log fetched vs total:
```python
logger.info("Federal Register: fetched %d/%d items (page %d/%d)", len(all_items), total_count, page, total_pages)
```
If truncated by safety cap, log WARNING.
  </action>
  <verify>
Verify by reading the modified files to confirm:
1. Congress.gov: `_search_congress()` and `_search()` both have `while True` pagination loops with offset increment and `pagination.next` check.
2. Federal Register: search method has `for page in range(1, total_pages+1)` or equivalent loop with `total_pages` check.
3. Both log fetched vs total counts.
4. Existing tests pass: `python -m pytest tests/ -x -q`
  </verify>
  <done>Congress.gov and Federal Register scrapers paginate to completion with logged counts and safety caps.</done>
</task>

<task type="auto">
  <name>Task 2: Add pagination to Grants.gov and verify USASpending</name>
  <files>src/scrapers/grants_gov.py, src/scrapers/usaspending.py</files>
  <action>
**A) Grants.gov pagination (grants_gov.py):**

Find the search method that queries the Grants.gov API (POST to /api/search2). Modify it to:
1. Read `hitCount` from response -- this is the total matching results.
2. Compare `hitCount` to the number of results fetched so far (`oppHits` array length).
3. Loop while `len(results_so_far) < hit_count`, incrementing `startRecordNum` by `rows` each iteration.
4. Add safety cap at 1,000 results (or 20 pages at rows=50) with WARNING log.
5. Add `await asyncio.sleep(0.3)` between page fetches.
6. Log fetched vs total:
```python
logger.info("Grants.gov: fetched %d/%d opportunities", len(all_results), hit_count)
```

**B) USASpending verification (usaspending.py):**

The `fetch_tribal_awards_by_year()` and similar methods already paginate using `page` + `hasNext`. Verify and add:
1. In the `_fetch_obligations()` method (used by scan()), check if there is pagination. If it uses `limit=10` without looping, add a WARNING log when the response indicates more data exists.
2. At the end of scan(), log:
```python
logger.info("USASpending: fetched %d obligation records", len(results))
```
3. Do NOT break existing pagination in the award methods -- only add logging.
  </action>
  <verify>
Verify by reading the modified files to confirm:
1. Grants.gov: search method has pagination loop with `startRecordNum` offset and `hitCount` comparison.
2. USASpending: _fetch_obligations has truncation awareness logging.
3. Existing tests pass: `python -m pytest tests/ -x -q`
  </verify>
  <done>All 4 scrapers have pagination or truncation detection. Zero silent data loss possible.</done>
</task>

</tasks>

<verification>
1. All 4 scraper files contain pagination/truncation logging (grep for "fetched" in all 4 files)
2. No scraper method uses a single `limit=50` without a pagination loop
3. Safety caps exist with WARNING level logging
4. asyncio.sleep delays between page fetches prevent rate limiting
5. Existing tests pass: `python -m pytest tests/ -x -q`
</verification>

<success_criteria>
- INTEL-02: All 4 scrapers paginate to completion or log explicit WARNING with gap count
- Zero silent truncation possible in any scraper
- Rate limit safety: 0.3s delay between page fetches
- Backward compatible: scan() return type unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/15-congressional-intelligence/15-02-SUMMARY.md`
</output>

---
phase: 15-congressional-intelligence
plan: 06
type: execute
wave: 4
depends_on: [15-01, 15-02, 15-04, 15-05]
files_modified:
  - tests/test_congressional_models.py
  - tests/test_pagination.py
  - tests/test_confidence.py
  - tests/test_congressional_rendering.py
autonomous: true

must_haves:
  truths:
    - "Congressional Pydantic model tests validate field types, constraints (bill_type enum, relevance_score range), and cross-field rules"
    - "Pagination tests verify all 4 scrapers handle multi-page responses correctly and log truncation warnings"
    - "Confidence scoring tests verify exponential decay formula, level thresholds, and edge cases (missing date, future date, zero days)"
    - "DOCX rendering tests verify bill intelligence section in Doc A (with talking points) and Doc B (facts only, zero strategy words)"
    - "Total test count exceeds 900 (157+ new tests added to 743 existing)"
  artifacts:
    - path: "tests/test_congressional_models.py"
      provides: "Pydantic model validation tests"
      min_lines: 100
    - path: "tests/test_pagination.py"
      provides: "Scraper pagination tests for all 4 scrapers"
      min_lines: 150
    - path: "tests/test_confidence.py"
      provides: "Confidence scoring unit tests"
      min_lines: 60
    - path: "tests/test_congressional_rendering.py"
      provides: "DOCX rendering tests for congressional sections"
      min_lines: 100
  key_links:
    - from: "tests/test_congressional_models.py"
      to: "src/schemas/models.py"
      via: "Imports and validates BillIntelligence, BillAction, etc."
    - from: "tests/test_pagination.py"
      to: "src/scrapers/"
      via: "Tests pagination in all 4 scraper classes"
    - from: "tests/test_congressional_rendering.py"
      to: "src/packets/docx_sections.py"
      via: "Tests render_bill_intelligence_section output"
---

<objective>
Create the test suite for Phase 15 congressional intelligence: model validation, pagination behavior, confidence scoring, and DOCX rendering. Target: exceed 900 total tests (157+ new tests on top of 743 existing).

Purpose: INTEL-09 requires pagination tests for all 4 scrapers + congressional model validation reaching 900+ total tests.

Output: 4 new test files with 157+ tests covering models, pagination, confidence, and rendering.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/15-congressional-intelligence/15-RESEARCH.md
@.planning/phases/15-congressional-intelligence/15-01-SUMMARY.md
@.planning/phases/15-congressional-intelligence/15-02-SUMMARY.md
@.planning/phases/15-congressional-intelligence/15-04-SUMMARY.md
@.planning/phases/15-congressional-intelligence/15-05-SUMMARY.md
@tests/test_schemas.py
@tests/test_audience_filtering.py
@src/schemas/models.py
@src/packets/confidence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Congressional model tests and confidence scoring tests</name>
  <files>tests/test_congressional_models.py, tests/test_confidence.py</files>
  <action>
**A) tests/test_congressional_models.py (50+ tests):**

Create comprehensive Pydantic model tests:

1. **BillAction tests** (~8 tests):
   - Valid construction with all fields
   - Defaults for optional fields (action_type="", chamber="")
   - Missing required fields (action_date, text) raise ValidationError

2. **BillIntelligence tests** (~25 tests):
   - Valid construction with all required fields
   - bill_type validator: accept HR, S, HJRES, SJRES, HCONRES, SCONRES, HRES, SRES
   - bill_type validator: reject "INVALID", "xyz", ""
   - relevance_score range: accept 0.0, 0.5, 1.0; reject -0.1, 1.1
   - cosponsor_count: accept 0, 100; reject -1
   - Default values: empty lists, empty strings, 0.0 score
   - Nested BillAction list serialization
   - Model serialization (model_dump) produces valid dict
   - From dict construction (model_validate)
   - Real-world bill data sample validation (use realistic Congress.gov field values)

3. **VoteRecord tests** (~5 tests):
   - Valid construction, defaults, missing required field

4. **Legislator tests** (~5 tests):
   - Valid construction, defaults, committee list

5. **CongressionalIntelReport tests** (~8 tests):
   - Valid construction with bills list
   - Empty bills list accepted
   - Nested BillIntelligence validation
   - confidence dict structure

**B) tests/test_confidence.py (30+ tests):**

1. **compute_confidence tests** (~15 tests):
   - Fresh data (0 days): score ~= source_weight
   - Stale data (365 days): score << source_weight
   - Very stale data (730 days): score near 0
   - Invalid date string: returns score based on 365-day assumption
   - None date: returns score based on 365-day assumption
   - Custom decay_rate: faster decay with higher rate
   - Custom reference_date: deterministic scoring
   - Edge case: future date (negative days clamped to 0)
   - Edge case: source_weight 0.0 -> always 0.0
   - Edge case: source_weight 1.0 with fresh date -> ~1.0
   - Timezone-aware vs timezone-naive dates
   - Known value test: 0.8 weight, 0 days = 0.8; 0.8 weight, 69 days ~= 0.4

2. **confidence_level tests** (~8 tests):
   - score >= 0.7 -> "HIGH"
   - score 0.4 -> "MEDIUM"
   - score 0.69 -> "MEDIUM"
   - score 0.39 -> "LOW"
   - score 0.0 -> "LOW"
   - score 1.0 -> "HIGH"
   - Boundary: 0.7 exactly -> "HIGH"
   - Boundary: 0.4 exactly -> "MEDIUM"

3. **section_confidence tests** (~8 tests):
   - Known source returns correct weight
   - Unknown source uses 0.50 default weight
   - Returns dict with score, level, source, last_updated keys
   - Fresh congress_gov data -> HIGH or MEDIUM
   - Stale inferred data -> LOW
  </action>
  <verify>
Run: `python -m pytest tests/test_congressional_models.py tests/test_confidence.py -v --tb=short`
All tests must pass. Count must be 80+.
  </verify>
  <done>50+ model validation tests and 30+ confidence scoring tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Pagination tests and rendering tests</name>
  <files>tests/test_pagination.py, tests/test_congressional_rendering.py</files>
  <action>
**A) tests/test_pagination.py (50+ tests):**

Test pagination behavior for all 4 scrapers using mocked HTTP responses:

1. **Congress.gov pagination tests** (~15 tests):
   - Single page: pagination.next is absent -> returns bills from first page
   - Multi-page: pagination.next present -> fetches additional pages
   - Empty response: no bills -> returns empty list
   - Safety cap: 10+ pages -> logs WARNING, stops at cap
   - Deduplication: same bill_id across pages -> included once
   - Error on page 2: gracefully returns page 1 results

2. **Federal Register pagination tests** (~12 tests):
   - Single page: total_pages=1 -> fetches only page 1
   - Multi-page: total_pages=3 -> fetches pages 1, 2, 3
   - Empty response: no results -> returns empty list
   - Safety cap: total_pages=25 -> logs WARNING, stops at cap 20
   - Missing total_pages field: treats as single page

3. **Grants.gov pagination tests** (~12 tests):
   - Single page: hitCount <= rows -> fetches only first page
   - Multi-page: hitCount > rows -> fetches with incrementing startRecordNum
   - Empty response: hitCount=0 -> returns empty list
   - Safety cap: hitCount=5000 -> logs WARNING, stops at cap 1000

4. **USASpending pagination tests** (~8 tests):
   - Verify existing pagination in fetch_tribal_awards methods still works
   - _fetch_obligations truncation detection: response indicates more data -> logs WARNING
   - scan() logs fetched count

Use `unittest.mock.AsyncMock` for mocking `_request_with_retry`. Create fixture data matching each API's response format.

**B) tests/test_congressional_rendering.py (30+ tests):**

Test DOCX rendering of congressional sections:

1. **render_bill_intelligence_section tests** (~20 tests):
   - Empty bills list: renders "no tracked legislation" note
   - Doc A with bills: renders full briefing for top bills, compact cards for remaining
   - Doc A: talking points present for top bills
   - Doc A: timing/urgency note present
   - Doc B with bills: renders facts only (no talking points, no timing)
   - Doc B: check zero occurrences of strategy words ("strategic", "leverage", "recommend", "talking point")
   - Confidence badge: section heading includes [HIGH], [MEDIUM], or [LOW]
   - Confidence badge: zero numeric scores in rendered text
   - Bills sorted by relevance_score descending
   - Single bill: renders correctly without errors
   - 10+ bills: top 3 get full briefing, rest get compact

2. **DocxEngine.generate() ordering tests** (~5 tests):
   - Bill intelligence section appears in document
   - Bill intelligence appears before delegation section (check paragraph order)
   - Missing congressional_intel in context: no crash, section renders gracefully

3. **Regional congressional section tests** (~5 tests):
   - render_regional_congressional_section with multiple Tribes' bills
   - Shared bills shown once with affected Tribe count
   - Empty bills: renders gracefully

Use python-docx Document() for actual rendering tests. Create TribePacketContext fixtures with sample congressional_intel data. Use DocumentTypeConfig from doc_types.py for Doc A vs Doc B switching.
  </action>
  <verify>
Run: `python -m pytest tests/test_pagination.py tests/test_congressional_rendering.py -v --tb=short`
All tests must pass. Count must be 77+.

Run: `python -m pytest tests/ -v --tb=short | tail -5`
Verify total test count exceeds 900.
  </verify>
  <done>50+ pagination tests and 30+ rendering tests pass; total suite exceeds 900 tests.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_congressional_models.py -v` -- all pass
2. `python -m pytest tests/test_confidence.py -v` -- all pass
3. `python -m pytest tests/test_pagination.py -v` -- all pass, covers all 4 scrapers
4. `python -m pytest tests/test_congressional_rendering.py -v` -- all pass, Doc A and Doc B differentiation verified
5. `python -m pytest tests/ --tb=short -q` -- total count > 900, zero failures
6. Doc B rendering tests confirm zero strategy words in output
</verification>

<success_criteria>
- INTEL-09: 157+ new tests added, total exceeds 900
- Pagination tests for all 4 scrapers (Federal Register, Grants.gov, Congress.gov, USASpending)
- Congressional model validation tests against real API response shapes
- Confidence scoring unit tests with boundary conditions
- DOCX rendering tests verify audience differentiation (Doc A vs Doc B)
</success_criteria>

<output>
After completion, create `.planning/phases/15-congressional-intelligence/15-06-SUMMARY.md`
</output>

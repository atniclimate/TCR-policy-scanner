---
phase: 01-pipeline-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scrapers/grants_gov.py
  - src/scrapers/congress_gov.py
  - outputs/LATEST-BRIEFING.md
  - outputs/LATEST-RESULTS.json
  - outputs/LATEST-GRAPH.json
autonomous: false

user_setup:
  - service: congress_gov
    why: "Congress.gov API requires a free API key for bill queries"
    env_vars:
      - name: CONGRESS_API_KEY
        source: "https://api.congress.gov/sign-up/ -- request free API key"

must_haves:
  truths:
    - "Federal Register scraper returns items from live API without errors"
    - "USASpending scraper returns FY26 obligation data without errors"
    - "Congress.gov scraper returns relevant Tribal legislation bills with API key set"
    - "Grants.gov scraper returns grant opportunities after endpoint fix"
    - "Full pipeline produces LATEST-BRIEFING.md with real scan data from all 4 sources"
    - "Full pipeline produces LATEST-RESULTS.json with scored items"
    - "Second scan detects changes (new/updated/removed) compared to first scan baseline"
    - "A single source failure does not crash the pipeline"
  artifacts:
    - path: "src/scrapers/grants_gov.py"
      provides: "Fixed Grants.gov scraper using /api/search2 endpoint with correct parameter names"
      contains: "api/search2"
    - path: "outputs/LATEST-BRIEFING.md"
      provides: "Markdown briefing with real scan data"
      min_lines: 10
    - path: "outputs/LATEST-RESULTS.json"
      provides: "JSON scan results with scored items from all sources"
      contains: "scan_results"
    - path: "outputs/LATEST-GRAPH.json"
      provides: "Knowledge graph built from scan results"
      contains: "summary"
  key_links:
    - from: "src/scrapers/grants_gov.py"
      to: "https://api.grants.gov/v1/api/search2"
      via: "POST request in _search_cfda and _search"
      pattern: "api/search2"
    - from: "src/scrapers/grants_gov.py"
      to: "oppHits response field"
      via: "response parsing in _search_cfda and _search"
      pattern: "oppHits"
    - from: "src/analysis/change_detector.py"
      to: "outputs/LATEST-RESULTS.json"
      via: "_load_previous reads baseline, detect_changes compares"
      pattern: "_load_previous|detect_changes"
    - from: "src/main.py"
      to: "src/reports/generator.py"
      via: "run_pipeline calls reporter.generate(scored, changes, graph_data)"
      pattern: "reporter\\.generate"
---

<objective>
Fix the Grants.gov API endpoint mismatch, validate all 4 scrapers against live federal APIs, run the full pipeline end-to-end, and verify change detection works across two consecutive scans.

Purpose: This is the foundation validation -- the pipeline must work against live APIs before any new capabilities can be built on top. The Grants.gov scraper has a known critical endpoint mismatch (uses deprecated `/opportunities/search` instead of current `/v1/api/search2` with different parameter names). The Congress.gov scraper's `query` parameter may be silently ignored and needs live testing.

Output: Fixed `grants_gov.py`, validated pipeline producing `LATEST-BRIEFING.md`, `LATEST-RESULTS.json`, and `LATEST-GRAPH.json` with real scan data. Change detection validated via two consecutive scans.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-pipeline-validation/01-RESEARCH.md

@src/main.py
@src/scrapers/grants_gov.py
@src/scrapers/congress_gov.py
@src/scrapers/base.py
@src/scrapers/federal_register.py
@src/scrapers/usaspending.py
@src/analysis/change_detector.py
@src/analysis/relevance.py
@src/reports/generator.py
@config/scanner_config.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix Grants.gov endpoint and validate all 4 scrapers individually</name>
  <files>
    src/scrapers/grants_gov.py
    src/scrapers/congress_gov.py
  </files>
  <action>
**Step 1: Dry run (pre-flight check)**
Run `python -m src.main --dry-run` from project root `F:\tcr-policy-scanner` to validate config loading, CFDA mappings, and search queries. Confirm it completes without errors.

**Step 2: Fix Grants.gov scraper (CRITICAL)**
In `src/scrapers/grants_gov.py`, update the endpoint URL and parameter names to match the current Grants.gov REST API:

1. In `_search_cfda()` (line ~115): Change URL from `f"{self.base_url}/opportunities/search"` to `f"{self.base_url}/api/search2"`. Change payload key `"assistanceListing"` to `"aln"`.
2. In `_search()` (line ~130): Change URL from `f"{self.base_url}/opportunities/search"` to `f"{self.base_url}/api/search2"`.
3. Keep all other payload parameters the same (`keyword`, `oppStatuses`, `sortBy`, `rows`, `postedFrom`) -- these are the same in search2.
4. Keep the response parsing (`data.get("oppHits", [])`) -- search2 uses the same `oppHits` field.
5. The `base_url` in config is `https://api.grants.gov/v1` -- so the full endpoint becomes `https://api.grants.gov/v1/api/search2`.

**Step 3: Validate Federal Register (most likely to work)**
Run `python -m src.main --source federal_register --verbose` from `F:\tcr-policy-scanner`. Confirm:
- No HTTP errors in output
- Items collected count > 0
- Log shows "Federal Register: collected N items" (N > 0)

**Step 4: Validate USASpending (no key needed)**
Run `python -m src.main --source usaspending --verbose`. Confirm:
- Response contains FY26 obligation data
- Items collected count > 0
- Log shows "USASpending: collected N items" (N > 0)

**Step 5: Validate Congress.gov (requires CONGRESS_API_KEY)**
Ensure `CONGRESS_API_KEY` env var is set. Run `python -m src.main --source congress_gov --verbose`. Inspect:
- Whether results are returned (count > 0)
- Whether results are relevant (Tribal/climate/resilience bills) or unfiltered junk
- If the `query` parameter is being silently ignored (result count > 100 with irrelevant bills), note this for remediation but do NOT block the pipeline -- the scraper still functions, just with lower precision
- If `query` IS being ignored: consider whether to add a post-filter in `_search_congress()` that checks if the bill title contains any of the search terms. Only add this if testing confirms the parameter is ignored AND results are clearly unfiltered. Keep the fix minimal.

**Step 6: Validate Grants.gov (after endpoint fix)**
Run `python -m src.main --source grants_gov --verbose`. Confirm:
- No HTTP 404 errors (old endpoint would return 404)
- Items collected count > 0 (CFDA-based and keyword-based)
- Zombie CFDA check runs without errors
- If the `synopsis`, `awardCeiling`, `awardFloor`, or `eligibleApplicants` fields are missing from search2 responses, update the `_normalize()` method to handle missing fields gracefully (use empty string/list defaults). The normalizer already uses `.get()` with defaults, so this should work -- but verify.

**Important:** Test each scraper ONE AT A TIME with pauses between to avoid rate limiting. If any scraper gets 429 or 403, wait 60 seconds before retrying.
  </action>
  <verify>
Run each individually and check exit code + output:
- `python -m src.main --dry-run` exits 0
- `python -m src.main --source federal_register --verbose` shows items collected > 0
- `python -m src.main --source usaspending --verbose` shows items collected > 0
- `python -m src.main --source congress_gov --verbose` shows items collected > 0 (with CONGRESS_API_KEY set)
- `python -m src.main --source grants_gov --verbose` shows items collected > 0 (after endpoint fix)
  </verify>
  <done>
All 4 scrapers return valid data from live APIs. Grants.gov scraper uses `/api/search2` endpoint with `aln` parameter. No HTTP errors. Each scraper's log shows item count > 0.
  </done>
</task>

<task type="auto">
  <name>Task 2: Full pipeline run and change detection validation</name>
  <files>
    outputs/LATEST-BRIEFING.md
    outputs/LATEST-RESULTS.json
    outputs/LATEST-GRAPH.json
  </files>
  <action>
**Step 1: First full pipeline scan (creates baseline)**
Run `python -m src.main --verbose` from `F:\tcr-policy-scanner`. This runs all 4 sources together. Confirm:
- Pipeline completes without crashing (even if one source had issues, graceful degradation should keep it running)
- Log shows total raw items collected > 0
- Log shows item scoring completed
- `outputs/LATEST-BRIEFING.md` is created with real content (not empty or placeholder)
- `outputs/LATEST-RESULTS.json` is created with `scan_results` array containing scored items
- `outputs/LATEST-GRAPH.json` is created with `summary` containing node/edge counts
- `outputs/archive/` contains timestamped copies

Inspect `outputs/LATEST-RESULTS.json` to verify:
- Items from multiple sources (check `source` field for `federal_register`, `grants_gov`, `congress_gov`, `usaspending`)
- Each item has `relevance_score` field
- Items have expected fields (`title`, `url`, `published_date`, etc.)

Inspect `outputs/LATEST-BRIEFING.md` to verify it contains:
- Scan metadata (date, sources, item counts)
- Scored policy items organized by relevance

**Step 2: Modify baseline for controlled change detection test**
To validate PIPE-04 without waiting for real policy changes, create a controlled test:

1. Read `outputs/LATEST-RESULTS.json`
2. Remove the FIRST item from the `scan_results` array (this will appear as "new" on the next scan when it comes back)
3. Add a FAKE item to `scan_results` with `source: "test"` and `source_id: "fake-item-001"` (this will appear as "removed" on the next scan)
4. Save the modified file back to `outputs/LATEST-RESULTS.json`

**Step 3: Second pipeline scan (validates change detection)**
Run `python -m src.main --verbose` again. Inspect the output for:
- Log line "Changes: N new, M updated, K removed" where N > 0 AND K > 0
- The item removed in Step 2 should appear as "new" (it returns from the live scan)
- The fake item from Step 2 should appear as "removed" (it is no longer in live results)

This validates that the ChangeDetector correctly identifies new, updated, and removed items between scan cycles.

**Step 4: Verify graceful failure isolation**
If any of the 4 sources failed during the full pipeline run, verify that:
- The pipeline still completed (did not crash)
- Results from the working sources are present in output
- The failed source is logged but does not block other sources
- If ALL sources succeeded, note this as passing validation (the try/except pattern in `run_scan()` is already verified by code inspection)
  </action>
  <verify>
After both scans:
- `outputs/LATEST-BRIEFING.md` exists, has > 10 lines, contains real policy items
- `outputs/LATEST-RESULTS.json` exists, is valid JSON, contains `scan_results` array with items from multiple sources
- `outputs/LATEST-GRAPH.json` exists, is valid JSON, contains `summary` with `total_nodes` > 0
- Second scan log shows non-zero change detection counts (new > 0 AND removed > 0)
  </verify>
  <done>
Full pipeline produces LATEST-BRIEFING.md, LATEST-RESULTS.json, and LATEST-GRAPH.json with real scan data from all 4 sources. Change detection correctly identifies new/modified/removed items between consecutive scans. Pipeline does not crash when a single source fails.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Fixed the Grants.gov scraper endpoint (changed from deprecated `/opportunities/search` to `/v1/api/search2` with `aln` parameter), validated all 4 scrapers against live federal APIs, ran full pipeline end-to-end producing LATEST-BRIEFING.md and LATEST-RESULTS.json with real data, and verified change detection works across two consecutive scans.
  </what-built>
  <how-to-verify>
1. Open `F:\tcr-policy-scanner\outputs\LATEST-BRIEFING.md` -- confirm it contains real policy intelligence data (Federal Register documents, grant opportunities, congressional bills, spending data), not placeholder text
2. Open `F:\tcr-policy-scanner\outputs\LATEST-RESULTS.json` -- confirm `scan_results` array has items with `source` values from multiple federal APIs
3. Review the Grants.gov fix in `F:\tcr-policy-scanner\src\scrapers\grants_gov.py` -- confirm the endpoint URL uses `/api/search2` and the CFDA parameter uses `aln`
4. If Congress.gov query parameter was found to be ignored, review any post-filter or note added to the scraper
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
1. `python -m src.main --dry-run` completes without errors
2. Each of the 4 scrapers returns > 0 items from live APIs when run individually
3. `python -m src.main --verbose` completes end-to-end, producing all 3 output files
4. `outputs/LATEST-RESULTS.json` contains items from multiple sources with relevance scores
5. Second scan shows change detection working (non-zero new AND removed counts)
6. Pipeline does not crash when a single source encounters an error
</verification>

<success_criteria>
- PIPE-01: All 4 scrapers return valid data from live APIs (Federal Register, Grants.gov, Congress.gov, USASpending)
- PIPE-02: End-to-end pipeline produces LATEST-BRIEFING.md and LATEST-RESULTS.json with real scan data from all sources
- PIPE-04: Change detector correctly identifies new/modified/removed items between two consecutive scans
- Graceful failure: Single source failure does not crash pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-validation/01-01-SUMMARY.md`
</output>

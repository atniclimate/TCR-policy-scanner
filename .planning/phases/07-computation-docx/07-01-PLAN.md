---
phase: 07-computation-docx
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/packets/economic.py
  - src/packets/relevance.py
  - tests/test_economic.py
autonomous: true

must_haves:
  truths:
    - "EconomicImpactCalculator returns dollar ranges (low/high) using BEA 1.8-2.4x output multiplier for any Tribe's award data"
    - "Jobs estimates returned as ranges using BLS 8-15 jobs per $1M methodology"
    - "FEMA BCR 4:1 applied to mitigation programs (fema_bric, fema_tribal_mitigation, usda_wildfire) only; None for others"
    - "Multi-district Tribes get proportional impact allocation by overlap_pct"
    - "Zero-award Tribes get program benchmark averages with First-Time Applicant framing"
    - "ProgramRelevanceFilter returns 8-12 programs per Tribe based on hazard profile and ecoregion priority"
    - "Critical-priority programs always included regardless of hazard relevance"
    - "Methodology citation string included in every ProgramEconomicImpact result"
  artifacts:
    - path: "src/packets/economic.py"
      provides: "EconomicImpactCalculator, ProgramEconomicImpact, TribeEconomicSummary"
      min_lines: 120
    - path: "src/packets/relevance.py"
      provides: "ProgramRelevanceFilter with filter_for_tribe()"
      min_lines: 80
    - path: "tests/test_economic.py"
      provides: "Unit tests for economic calculator and relevance filter"
      min_lines: 150
  key_links:
    - from: "src/packets/economic.py"
      to: "TribePacketContext.awards"
      via: "compute() takes awards list + districts list + programs dict"
      pattern: "def compute"
    - from: "src/packets/relevance.py"
      to: "TribePacketContext.hazard_profile"
      via: "filter_for_tribe() takes hazard_profile + ecoregions + programs"
      pattern: "def filter_for_tribe"
---

<objective>
Build the economic impact calculator and program relevance filter -- the two pure-computation modules that transform raw award/hazard data into structured results for Hot Sheet rendering.

Purpose: ECON-01 requires per-Tribe, per-program economic impact framing with published multiplier ranges and citable methodology. DOC-02 requires filtering 16 programs to 8-12 relevant ones per Tribe. Both are stateless pure-function modules with no DOCX dependency.
Output: Two new modules (economic.py, relevance.py) with comprehensive unit tests.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-computation-docx/07-RESEARCH.md

@src/packets/context.py
@src/packets/orchestrator.py
@data/program_inventory.json
@data/graph_schema.json
@tests/test_packets.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create EconomicImpactCalculator module</name>
  <files>src/packets/economic.py</files>
  <action>
Create `src/packets/economic.py` with these components:

**Dataclasses:**
- `ProgramEconomicImpact`: program_id, total_obligation (float), multiplier_low (1.8), multiplier_high (2.4), estimated_impact_low (float), estimated_impact_high (float), jobs_estimate_low (float), jobs_estimate_high (float), fema_bcr (float|None -- 4.0 for mitigation programs, None otherwise), methodology_citation (str), is_benchmark (bool -- True when using program averages instead of Tribe-specific data)
- `TribeEconomicSummary`: tribe_id, tribe_name, total_obligation, total_impact_low, total_impact_high, total_jobs_low, total_jobs_high, by_program (list[ProgramEconomicImpact]), by_district (dict mapping district string to impact breakdown)

**Constants:**
- `BEA_MULTIPLIER_LOW = 1.8`
- `BEA_MULTIPLIER_HIGH = 2.4`
- `BLS_JOBS_PER_MILLION_LOW = 8`
- `BLS_JOBS_PER_MILLION_HIGH = 15`
- `FEMA_BCR_OVERALL = 4.0`
- `FEMA_BCR_BY_HAZARD`: dict mapping hazard types to BCR values: flood=5.0, wind/hurricane=5.0, earthquake=1.5
- `MITIGATION_PROGRAM_IDS`: set of ["fema_bric", "fema_tribal_mitigation", "usda_wildfire"]
- `METHODOLOGY_CITATION`: the full methodology footnote text from research (BEA RIMS II range, BLS requirements, FEMA/NIBS MitSaves)
- `PROGRAM_BENCHMARK_AVERAGES`: dict mapping program_id to average obligation amount (use reasonable estimates: bia_tcr=150000, fema_bric=500000, etc. -- these are used for zero-award Tribes)

**Class EconomicImpactCalculator:**
- `compute(awards: list[dict], districts: list[dict], programs: dict[str, dict]) -> TribeEconomicSummary`
  1. Group awards by program_id (normalize CFDA to list: `[cfda] if isinstance(cfda, str) else (cfda or [])`)
  2. For each program with awards: calculate impact_low = obligation * 1.8, impact_high = obligation * 2.4, jobs_low = (obligation / 1_000_000) * 8, jobs_high = (obligation / 1_000_000) * 15
  3. For mitigation programs: set fema_bcr = FEMA_BCR_OVERALL (4.0)
  4. For zero-award programs: use PROGRAM_BENCHMARK_AVERAGES with is_benchmark=True
  5. Allocate by district using overlap_pct: for each district, multiply total impact by (overlap_pct / 100). If no districts, skip district allocation.
  6. Return TribeEconomicSummary with aggregated totals

- `format_impact_narrative(impact: ProgramEconomicImpact, tribe_name: str, program_name: str) -> str`
  Returns a natural-language narrative like: "[program] funding to [tribe] generated an estimated $X-$Y in regional economic activity (BEA RIMS II methodology, output multiplier range 1.8-2.4x), supporting approximately N-M jobs (BLS employment requirements methodology)."
  For is_benchmark=True: "Based on program averages, a successful [program] application could generate an estimated $X-$Y in regional economic impact."

- `format_bcr_narrative(impact: ProgramEconomicImpact) -> str | None`
  Returns FEMA BCR narrative for mitigation programs: "Every federal dollar invested in hazard mitigation generates an estimated $4 in future avoided costs (FEMA/NIBS MitSaves, 2018)." Returns None for non-mitigation programs.

**Critical implementation notes:**
- Use `datetime.now(timezone.utc)` if any timestamps needed
- All module-level constants, no file I/O in this module
- Handle edge case: awards with obligation=0 or negative (skip them)
- Handle edge case: districts with overlap_pct=None or 0 (skip them)
- Handle edge case: programs dict may not contain all 16 programs (only iterate programs that exist)
- Zero-award path is the PRIMARY path since all current caches are empty
  </action>
  <verify>
Run: `python -m pytest tests/test_economic.py -v -k "economic" --tb=short`
All economic calculator tests pass. Import works: `python -c "from src.packets.economic import EconomicImpactCalculator"`
  </verify>
  <done>EconomicImpactCalculator.compute() returns TribeEconomicSummary with correct dollar ranges, job estimates, BCR for mitigation programs, and district allocation. Zero-award path returns benchmark averages with is_benchmark=True.</done>
</task>

<task type="auto">
  <name>Task 2: Create ProgramRelevanceFilter module and all tests</name>
  <files>src/packets/relevance.py, tests/test_economic.py</files>
  <action>
Create `src/packets/relevance.py` with:

**Constants:**
- `HAZARD_PROGRAM_MAP`: dict mapping NRI hazard codes to relevant program_ids. Examples:
  - WFIR (wildfire) -> ["usda_wildfire", "fema_bric", "fema_tribal_mitigation", "bia_tcr"]
  - CFLD/IFLD (flooding) -> ["fema_bric", "fema_tribal_mitigation", "epa_stag", "usbr_watersmart"]
  - DRGT (drought) -> ["usbr_watersmart", "usbr_tap", "bia_tcr"]
  - HRCN (hurricane) -> ["fema_bric", "fema_tribal_mitigation", "hud_ihbg"]
  - ERQK (earthquake) -> ["fema_bric", "fema_tribal_mitigation"]
  - HWAV (heat wave) -> ["bia_tcr", "hud_ihbg", "doe_indian_energy"]
  - General (always relevant): ["bia_tcr", "irs_elective_pay", "epa_gap"]
  - Map ALL 18 NRI hazard types. If a hazard type has no specific program, map to ["bia_tcr", "fema_bric"]

- `MIN_PROGRAMS = 8`
- `MAX_PROGRAMS = 12`

**Class ProgramRelevanceFilter:**
- `__init__(self, programs: dict[str, dict], ecoregion_priority: list[str] | None = None)`
  Store programs dict and optional ecoregion priority program list

- `filter_for_tribe(self, hazard_profile: dict, ecoregions: list[str], ecoregion_mapper=None) -> list[dict]`
  1. Score each program based on: hazard relevance (top_hazards match), ecoregion priority match, priority level (critical > high > medium), award history presence
  2. Always include programs where program["priority"] == "critical" (these are always relevant)
  3. Add programs matching the Tribe's top hazards via HAZARD_PROGRAM_MAP
  4. Add programs matching ecoregion priority lists (if ecoregion_mapper provided, call get_priority_programs())
  5. Deduplicate and sort by relevance score descending
  6. Clamp to MIN_PROGRAMS-MAX_PROGRAMS range: if fewer than 8, pad with highest-priority remaining programs; if more than 12, trim lowest-relevance programs (but never remove critical)
  7. Return list of program dicts (not just IDs) with added `_relevance_score` field

- `get_omitted_programs(self, included: list[dict]) -> list[dict]`
  Returns programs NOT in included list, for appendix rendering

**Critical notes:**
- hazard_profile follows the schema: {"fema_nri": {"top_hazards": [{"type": "WFIR", "code": "WFIR", "risk_score": 92.0}], "all_hazards": {...}}, "usfs_wildfire": {...}}
- top_hazards may use "type" field (human name like "Wildfire") or "code" field (NRI code like "WFIR") -- handle both
- Empty hazard_profile (most common case): fall back to ecoregion priority programs + critical programs
- Never return fewer than 3 programs (absolute minimum for a packet)

**Then create tests/test_economic.py** with comprehensive tests for BOTH modules:

Economic tests:
- test_compute_with_awards: Known awards produce correct impact ranges
- test_compute_zero_awards: Empty awards list returns benchmark-based summary
- test_bcr_mitigation_programs: BCR=4.0 for fema_bric, None for epa_gap
- test_district_allocation: Multi-district proportional split by overlap_pct
- test_jobs_estimate_range: Known obligation produces correct job range
- test_format_impact_narrative: Narrative includes tribe name and dollar ranges
- test_format_bcr_narrative: Returns string for mitigation, None for non-mitigation
- test_negative_obligation_skipped: Awards with negative amounts excluded
- test_methodology_citation: Citation string present in every ProgramEconomicImpact

Relevance tests:
- test_filter_critical_always_included: Critical programs always in result
- test_filter_hazard_match: Wildfire hazard includes usda_wildfire
- test_filter_empty_hazard_fallback: Empty hazard profile uses ecoregion priorities
- test_filter_min_programs: Result has at least MIN_PROGRAMS items
- test_filter_max_programs: Result capped at MAX_PROGRAMS
- test_omitted_programs: Programs not in filtered list returned by get_omitted_programs()
- test_relevance_score_ordering: Results sorted by relevance score descending

All tests use mock data (no file I/O). Use pytest tmp_path fixture only if writing output files.
Run full suite at the end: `python -m pytest tests/ -v --tb=short` to verify no regressions (106 existing + new tests).
  </action>
  <verify>
Run: `python -m pytest tests/test_economic.py -v --tb=short`
Run: `python -m pytest tests/ --tb=short` (full suite -- must be 106 + new tests, all passing)
  </verify>
  <done>ProgramRelevanceFilter.filter_for_tribe() returns 8-12 programs per Tribe with correct hazard-based scoring. All economic and relevance tests pass. 106 existing tests still pass.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.packets.economic import EconomicImpactCalculator, ProgramEconomicImpact, TribeEconomicSummary"` -- imports work
2. `python -c "from src.packets.relevance import ProgramRelevanceFilter"` -- imports work
3. `python -m pytest tests/test_economic.py -v` -- all new tests pass
4. `python -m pytest tests/ --tb=short` -- full suite passes with no regressions
</verification>

<success_criteria>
- EconomicImpactCalculator.compute() produces TribeEconomicSummary with correct BEA multiplier ranges (1.8-2.4x)
- Zero-award Tribes get benchmark-based economic estimates with is_benchmark=True
- FEMA BCR (4:1) applied only to mitigation programs
- ProgramRelevanceFilter returns 8-12 programs sorted by relevance
- Critical programs always included in filtered results
- All tests pass with no regressions to 106 existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/07-computation-docx/07-01-SUMMARY.md`
</output>

---
phase: 14-integration
plan: 06
type: execute
wave: 3
depends_on: ["14-03", "14-04"]
files_modified:
  - scripts/validate_coverage.py
  - VERIFICATION.md
  - tests/test_validate_coverage.py
autonomous: true

must_haves:
  truths:
    - "Validation script reports coverage percentages for all 4 cache types (awards, hazards, congressional, registry)"
    - "Validation script reports document generation counts for all 4 doc types"
    - "VERIFICATION.md documents v1.2 testing methodology, data sources, and known gaps"
  artifacts:
    - path: "scripts/validate_coverage.py"
      provides: "Data coverage validation script with JSON + markdown output"
      min_lines: 80
    - path: "VERIFICATION.md"
      provides: "v1.2 testing methodology documentation"
      min_lines: 50
    - path: "tests/test_validate_coverage.py"
      provides: "Tests for validation script logic"
      min_lines: 30
  key_links:
    - from: "scripts/validate_coverage.py"
      to: "data/award_cache/"
      via: "Scans award cache files for non-zero data"
      pattern: "award_cache"
    - from: "scripts/validate_coverage.py"
      to: "data/hazard_profiles/"
      via: "Scans hazard profile files for non-zero scores"
      pattern: "hazard_profiles"
---

<objective>
Create the data validation script and VERIFICATION.md documentation to satisfy INTG-03 and INTG-04.

Purpose: INTG-03 requires a script that reports coverage across all data caches. INTG-04 requires VERIFICATION.md documenting the v1.2 testing methodology. These are the "audit trail" artifacts that prove the system works and document known gaps.

Output: validate_coverage.py produces JSON + markdown coverage reports; VERIFICATION.md documents methodology, data freshness, sample validation approach, and known coverage gaps.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@scripts/validate_data_integrity.py
@src/paths.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data coverage validation script</name>
  <files>scripts/validate_coverage.py, tests/test_validate_coverage.py</files>
  <action>
  Create `scripts/validate_coverage.py` that reports coverage across all data caches and document outputs.

  The script should:

  1. **Award cache analysis** (data/award_cache/*.json):
     - Count total files
     - Count files with at least one award (total_obligation > 0 or award_count > 0)
     - Calculate coverage percentage
     - List Tribe IDs with zero awards

  2. **Hazard profile analysis** (data/hazard_profiles/*.json):
     - Count total files
     - Count files with non-zero risk_score in sources.fema_nri.composite
     - Count files with USFS wildfire data populated
     - Calculate coverage percentage
     - List Tribe IDs with zero hazard data

  3. **Congressional cache analysis** (data/congressional_cache.json):
     - Count total delegations
     - Count delegations with at least one senator or representative
     - Calculate coverage percentage
     - List Tribe IDs without delegation data

  4. **Registry analysis** (data/tribal_registry.json):
     - Count total Tribes
     - Count Tribes with BIA codes
     - Count Tribes with states populated
     - Coverage percentage

  5. **Document generation analysis** (outputs/packets/):
     - Count Doc A files in internal/
     - Count Doc B files in congressional/
     - Count Doc C files in regional/internal/
     - Count Doc D files in regional/congressional/
     - Calculate "complete packet" count (Tribes with both Doc A + Doc B)

  6. **Cross-source completeness** (the key INTG-02 metric):
     - Count Tribes with ALL of: awards + delegation + hazard data = "complete data"
     - Count Tribes with awards + delegation (without hazard) = "partial data"
     - Count Tribes with awards only, delegation only, nothing

  **Output formats:**
  - JSON report at `outputs/coverage_report.json`
  - Markdown summary at `outputs/coverage_report.md`
  - Console summary printed to stdout

  **Console output format:**
  ```
  === TCR Policy Scanner v1.2 Data Coverage Report ===

  Data Sources:
    Awards:        451/592 (76.2%)
    Hazards:       XXX/592 (XX.X%)
    Congressional: 501/592 (84.6%)
    Registry:      592/592 (100.0%)

  Document Generation:
    Doc A (Tribal Internal):       XXX generated
    Doc B (Tribal Congressional):  XXX generated
    Doc C (Regional Internal):     X/8 generated
    Doc D (Regional Congressional): X/8 generated

  Completeness:
    Complete data (all sources):   XXX/592 (XX.X%)
    Complete packets (A+B):        XXX Tribes
    Partial packets (B only):      XXX Tribes

  Coverage gaps saved to outputs/coverage_report.json
  ```

  **Script structure:**
  - Use sys.path insertion pattern (per DEC-0902-01) for standalone execution
  - Import from src.paths for data directory constants
  - Use argparse for optional --json-only flag
  - Main function returns the coverage dict (for testing)

  **Tests (tests/test_validate_coverage.py):**
  1. `test_award_coverage_counting`: Create tmp_path with mix of populated/empty award files, verify counts
  2. `test_hazard_coverage_counting`: Create tmp_path with mix of populated/empty hazard files, verify counts
  3. `test_cross_source_completeness`: Verify the cross-source calculation logic
  4. `test_json_output_format`: Verify JSON output has expected structure
  5. `test_markdown_output_format`: Verify markdown output is well-formatted
  </action>
  <verify>
  1. `python -m pytest tests/test_validate_coverage.py -v` -- tests pass
  2. `python scripts/validate_coverage.py` -- runs without error, prints coverage summary
  3. Check that `outputs/coverage_report.json` was created with expected structure
  4. Check that `outputs/coverage_report.md` was created
  5. `python -m pytest tests/ -x -q` -- full suite passes
  </verify>
  <done>validate_coverage.py reports coverage for awards (451/592), hazards (550+/592), congressional (501/592), registry (592/592), and document generation counts; outputs JSON + markdown + console; tests verify counting logic</done>
</task>

<task type="auto">
  <name>Task 2: Write VERIFICATION.md documentation</name>
  <files>VERIFICATION.md</files>
  <action>
  Create `VERIFICATION.md` at project root documenting the v1.2 testing methodology.

  **Structure:**

  ```markdown
  # TCR Policy Scanner v1.2 -- Verification

  ## Testing Methodology

  ### Automated Test Suite
  - 606+ pytest tests across 22 test files
  - Tests cover: pipeline flow, DOCX generation, economic calculations, hazard aggregation, audience filtering, quality review, regional aggregation
  - Run with: `python -m pytest tests/ -v`

  ### Data Validation
  - Coverage validation script: `python scripts/validate_coverage.py`
  - Validates 4 data sources: awards (USASpending), hazards (FEMA NRI), congressional (Congress.gov), registry (EPA)
  - Reports coverage percentages and identifies gaps
  - Output: `outputs/coverage_report.json`, `outputs/coverage_report.md`

  ### Document Quality Review
  - Automated quality reviewer checks every generated document
  - Checks: audience leakage, air gap compliance, placeholder detection, confidential marking, content completeness
  - Quality report: `outputs/packets/quality_report.md`
  - Target: 95%+ pass rate, zero critical audience leakage

  ## Data Sources

  | Source | Description | Freshness | Coverage |
  |--------|-------------|-----------|----------|
  | USASpending | Federal award data per Tribe per CFDA | FY2022-FY2026 | 451/592 Tribes (76.2%) |
  | FEMA NRI | National Risk Index county-level hazard scores | v1.20 | XXX/592 Tribes |
  | USFS | Wildfire Risk to Communities dataset | 2023 | XXX/592 Tribes |
  | Congress.gov | 119th Congress delegation + committee assignments | 2025-2026 | 501/592 Tribes (84.6%) |
  | EPA | Tribal Names Service (registry) | 2026 | 592/592 Tribes (100%) |

  ## Document Types

  | Type | Audience | Scope | Content | Classification |
  |------|----------|-------|---------|----------------|
  | Doc A | Internal (Tribal leadership) | Per-Tribe | Strategy + leverage + messaging | CONFIDENTIAL |
  | Doc B | Congressional offices | Per-Tribe | Evidence-only, hot sheets | For Congressional Office Use |
  | Doc C | Internal (InterTribal) | Per-Region | Coordinated strategy, coverage gaps | CONFIDENTIAL |
  | Doc D | Congressional offices | Per-Region | Regional evidence synthesis | For Congressional Office Use |

  ## Audience Differentiation Verification

  Documents are classified by audience:
  - Internal docs (A/C): contain strategic leverage, approach recommendations, messaging frameworks
  - Congressional docs (B/D): contain only factual evidence, program data, economic impact with methodology citations

  Automated checks verify no internal-only content appears in congressional documents (see quality_review.py).

  ### Content Classification Line
  - Internal only: leverage, approach strategy, messaging framework, member-specific recommendations, voting record analysis, sequencing strategy
  - Congressional safe: committee assignments, appropriations amounts, program status, award history, hazard scores, economic impact with source citations

  ## Economic Impact Methodology

  - Multipliers: BEA RIMS II regional input-output (range 1.8-2.4x)
  - Employment: BLS employment requirements (8-15 jobs per $1M)
  - Mitigation ROI: FEMA/NIBS MitSaves ($4 return per $1, for mitigation programs)
  - Per-district allocation: proportional to congressional district overlap percentage
  - Real award data used for all Tribes with USASpending records
  - Tribes without awards receive no economic impact section (not benchmark estimates)

  ## Known Coverage Gaps

  ### Awards (141 Tribes at 0%)
  - Primarily small Tribes, state-recognized Tribes, and Tribes with housing authority naming patterns not yet mapped
  - Housing authority alias mapping (Phase 12-04) improved from 418 to 451 Tribes

  ### Hazards
  - [Coverage will be populated after hazard data activation]
  - Tribes not overlapping with FEMA NRI county boundaries may have zero scores
  - Alaska Native villages may have limited NRI county data

  ### Congressional (91 Tribes unmapped)
  - Tribes without Census AIANNH geographic boundaries cannot be mapped to congressional districts
  - State-recognized Tribes may not have district overlap data

  ## Sample Validation

  For manual spot-checking, review 5 Tribes per ecoregion (35 total):
  - Compare Doc A and Doc B for the same Tribe to verify differentiation
  - Verify award amounts match USASpending data
  - Verify hazard scores match NRI source data
  - Verify delegation members match Congress.gov data

  ## Air Gap Verification

  No document surface (header, footer, cover page, body text, metadata) contains:
  - Organizational names (ATNI, NCAI, or any other entity)
  - Tool attribution ("TCR Policy Scanner", "Generated by")
  - Author names or contact information
  - Logos or organizational branding

  Federal data sources are cited explicitly and specifically.

  ## Reproducibility

  To regenerate all documents:
  1. `python scripts/download_nri_data.py` (download FEMA NRI source data)
  2. `python scripts/populate_hazards.py` (populate hazard profiles)
  3. `python scripts/populate_awards.py` (populate award caches from USASpending)
  4. `python -m src.main --prep-packets --all-tribes` (generate all documents)
  5. `python scripts/validate_coverage.py` (validate coverage)

  ---
  *v1.2 Verification document*
  *Generated: [DATE]*
  ```

  Fill in the XXX placeholders with actual numbers from the coverage report if available, or leave as placeholders with a note to update after batch generation (Plan 14-05).
  </action>
  <verify>
  1. `VERIFICATION.md` exists at project root
  2. Contains sections: Testing Methodology, Data Sources, Document Types, Economic Impact Methodology, Known Coverage Gaps, Sample Validation, Air Gap Verification, Reproducibility
  3. No organizational names in VERIFICATION.md (air gap compliance applies to all artifacts)
  4. `python -m pytest tests/ -x -q` -- full suite passes
  </verify>
  <done>VERIFICATION.md documents complete v1.2 methodology; validate_coverage.py reports coverage across all sources and doc types; INTG-03 and INTG-04 satisfied</done>
</task>

</tasks>

<verification>
1. `python scripts/validate_coverage.py` runs and produces coverage report
2. Coverage report shows awards (451/592), congressional (501/592), registry (592/592)
3. VERIFICATION.md exists with complete methodology documentation
4. No air gap violations in VERIFICATION.md
5. All tests pass including new validation tests
</verification>

<success_criteria>
- validate_coverage.py produces JSON + markdown + console reports (INTG-03)
- VERIFICATION.md documents complete v1.2 testing methodology (INTG-04)
- Coverage report correctly counts all data sources and document outputs
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-integration/14-06-SUMMARY.md`
</output>

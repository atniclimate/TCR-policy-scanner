---
phase: 06-data-acquisition
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/packets/hazards.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "NRI county-level CSV data is parsed and mapped to Tribal areas via AIANNH crosswalk"
    - "18 hazard types are extracted per county with risk scores, expected annual loss, and ratings"
    - "USFS Wildfire Risk XLSX is parsed for tribal area wildfire metrics"
    - "Hazard profiles aggregate across overlapping counties for multi-county Tribal areas"
    - "All 592 Tribes get a hazard profile cache file (even if sparse/empty)"
    - "Top 3 hazards are ranked by risk score for each Tribe"
  artifacts:
    - path: "src/packets/hazards.py"
      provides: "HazardProfileBuilder class with NRI + USFS ingestion and per-Tribe cache"
      exports: ["HazardProfileBuilder"]
    - path: "requirements.txt"
      provides: "Updated with openpyxl dependency"
      contains: "openpyxl"
  key_links:
    - from: "src/packets/hazards.py"
      to: "data/aiannh_tribe_crosswalk.json"
      via: "Crosswalk maps AIANNH GEOIDs to tribe_ids for NRI county joins"
      pattern: "aiannh_tribe_crosswalk"
    - from: "src/packets/hazards.py"
      to: "data/nri/"
      via: "CSV parsing of NRI county and tribal relational data"
      pattern: "NRI_Table"
    - from: "src/packets/hazards.py"
      to: "data/usfs/"
      via: "XLSX parsing of USFS wildfire risk data"
      pattern: "wrc_download"
    - from: "src/packets/hazards.py"
      to: "data/hazard_profiles/"
      via: "Per-Tribe JSON cache file writes"
      pattern: "hazard_profiles"
---

<objective>
Build the hazard profiling pipeline: FEMA NRI ingestion (HAZ-01) and USFS Wildfire Risk ingestion (HAZ-02) with per-Tribe hazard profile caching.

Purpose: Enable any Tribe's top climate hazards, risk scores, expected annual losses, and wildfire risk to be retrieved from local cache without API calls at DOCX generation time.
Output: HazardProfileBuilder class, openpyxl dependency, per-Tribe hazard profile JSON cache files.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-data-acquisition/06-RESEARCH.md
@.planning/phases/06-data-acquisition/06-CONTEXT.md
@.planning/phases/05-foundation/05-03-SUMMARY.md

Key source files to understand:
@src/packets/registry.py -- TribalRegistry with get_all(), get_by_id()
@src/packets/context.py -- TribePacketContext (has hazard_profile stub field)
@data/aiannh_tribe_crosswalk.json -- AIANNH GEOID to tribe_id mapping (214 auto + 7 fuzzy matched)
@data/tribal_registry.json -- 592 Tribes with states field

Key data insight from RESEARCH.md:
- NRI does NOT have Tribal areas as a primary geographic unit
- FEMA provides "Tribal County Relational Database" CSV that maps AIANNH boundaries to overlapping NRI counties
- The existing aiannh_tribe_crosswalk.json bridges Census AIANNH GEOIDs to EPA tribe_ids
- Implementation must: (1) load NRI tribal relational CSV, (2) look up corresponding county NRI data, (3) aggregate to per-Tribe profiles
- USFS Wildfire Risk data is XLSX format -- exact column names need schema discovery from the actual file
- 27 AIANNH entities are unmatched in crosswalk -- those Tribes get empty/sparse hazard profiles
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add openpyxl dependency and create HazardProfileBuilder with NRI ingestion</name>
  <files>
    requirements.txt
    src/packets/hazards.py
  </files>
  <action>
**Update `requirements.txt`:**
Add `openpyxl>=3.1.0` to the requirements file (needed for USFS XLSX parsing in Task 2 action within this same file).

**Install:** Run `pip install openpyxl>=3.1.0` to make it available.

**Create `src/packets/hazards.py`:**

Build a `HazardProfileBuilder` class with these components:

**Constants:**
```python
NRI_HAZARD_CODES = {
    "AVLN": "Avalanche", "CFLD": "Coastal Flooding", "CWAV": "Cold Wave",
    "DRGT": "Drought", "ERQK": "Earthquake", "HAIL": "Hail",
    "HWAV": "Heat Wave", "HRCN": "Hurricane", "ISTM": "Ice Storm",
    "IFLD": "Inland Flooding", "LNDS": "Landslide", "LTNG": "Lightning",
    "SWND": "Strong Wind", "TRND": "Tornado", "TSUN": "Tsunami",
    "VLCN": "Volcanic Activity", "WFIR": "Wildfire", "WNTW": "Winter Weather",
}
```

**Constructor `__init__(self, config: dict)`:**
- Loads TribalRegistry (same pattern as other packet modules)
- Loads aiannh_tribe_crosswalk.json (path from config or default "data/aiannh_tribe_crosswalk.json")
- Sets NRI data directory (default "data/nri/")
- Sets USFS data directory (default "data/usfs/")
- Sets hazard cache directory (default "data/hazard_profiles/")
- Builds reverse crosswalk: tribe_id -> list of AIANNH GEOIDs (one Tribe may span multiple AIANNH areas)

**Method `_load_nri_county_data(self) -> dict[str, dict]`:**

Parses the NRI county CSV file (`data/nri/NRI_Table_Counties.csv`):

1. Open with `csv.DictReader`, encoding="utf-8"
2. For each row, key by `STCOFIPS` (state+county FIPS code)
3. Extract composite fields:
   - `RISK_SCORE`, `RISK_RATNG`, `RISK_VALUE`
   - `EAL_VALT` (Expected Annual Loss Total), `EAL_SCORE`, `EAL_RATNG`
   - `SOVI_SCORE`, `SOVI_RATNG` (Social Vulnerability)
   - `RESL_SCORE`, `RESL_RATNG` (Community Resilience)
4. Extract per-hazard metrics for all 18 hazard codes:
   - `{CODE}_RISKS` (risk score), `{CODE}_RISKR` (risk rating)
   - `{CODE}_EALT` (expected annual loss total)
   - `{CODE}_AFREQ` (annualized frequency)
   - `{CODE}_EVNTS` (number of events)
5. Convert numeric fields to float with `_safe_float(value, default=0.0)` helper that handles empty strings, None, non-numeric values
6. Return dict keyed by STCOFIPS

Handle the case where the NRI CSV file does not exist: log a warning and return empty dict. The user must pre-download NRI data -- this code does NOT fetch it from the web.

**Method `_load_nri_tribal_relational(self) -> dict[str, list[str]]`:**

Parses the NRI Tribal County Relational CSV (`data/nri/NRI_Tribal_Counties_Relational.csv` or similar filename -- the executor should look for any CSV in `data/nri/` with "Tribal" and "Relational" in the name):

1. This file maps AIANNH areas to NRI counties
2. Expected columns include an AIANNH identifier (GEOID or similar) and county FIPS codes
3. Since exact column names are uncertain (MEDIUM confidence from research), the executor should:
   - First inspect the CSV headers: `head -1 data/nri/NRI_Tribal_Counties_Relational.csv`
   - Adapt column name references to match actual headers
   - Log the discovered column names at INFO level
4. Return: dict mapping AIANNH GEOID -> list of county FIPS codes that overlap

If the tribal relational CSV is not available, fall back to using the Tribe's state(s) from the registry to identify relevant counties. This is less precise but ensures every Tribe gets some hazard data.

**Method `_build_tribe_nri_profile(self, tribe_id: str, county_data: dict, tribal_counties: dict) -> dict`:**

Aggregates NRI data for a single Tribe:

1. Find AIANNH GEOIDs for this tribe_id (from reverse crosswalk)
2. Find county FIPS codes for those AIANNH areas (from tribal relational data)
3. Collect NRI county records for all overlapping counties
4. Aggregate using MAX strategy (take the highest risk score across overlapping counties for each hazard -- this is conservative and highlights the worst-case exposure)
5. Compute composite scores: max of overlapping counties' RISK_SCORE, EAL_VALT (sum, not max -- total expected loss), SOVI_SCORE (max), RESL_SCORE (min -- worst resilience)
6. Rank all 18 hazards by risk score descending, take top 3 for `top_hazards` list
7. Return structured profile dict matching the schema from RESEARCH.md:
   ```python
   {
       "fema_nri": {
           "version": "1.20",
           "counties_analyzed": len(county_fips_list),
           "composite": {
               "risk_score": ..., "risk_rating": ...,
               "eal_total": ..., "eal_score": ...,
               "sovi_score": ..., "sovi_rating": ...,
               "resl_score": ..., "resl_rating": ...,
           },
           "top_hazards": [
               {"type": "Wildfire", "code": "WFIR", "risk_score": ..., "risk_rating": ..., "eal_total": ...},
               ...  # Top 3
           ],
           "all_hazards": {
               "WFIR": {"risk_score": ..., "eal_total": ..., "risk_rating": ...},
               ...  # All 18
           },
       }
   }
   ```

If no NRI data is available for a Tribe (unmatched in crosswalk, no relational data), return a minimal profile with null/zero values and a note field.

**Method `_load_usfs_wildfire_data(self) -> dict[str, dict]`:**

Parses the USFS Wildfire Risk to Communities XLSX:

1. Look for XLSX file in `data/usfs/` directory (glob for `*.xlsx`)
2. Open with openpyxl: `load_workbook(path, read_only=True, data_only=True)`
3. Since exact column names are LOW confidence from research:
   - Read the header row first
   - Log all column names at INFO level
   - Look for columns containing keywords: "risk", "likelihood", "tribal", "AIANNH", "GEOID", "name"
   - The executor must adapt column references based on actual headers
4. Expected key metrics to extract:
   - Risk to Homes (or similar risk metric)
   - Wildfire Likelihood (burn probability)
   - Geographic identifier (to match against AIANNH GEOIDs or Tribe names)
5. Return dict keyed by geographic identifier -> wildfire metrics dict

If the XLSX file does not exist or cannot be parsed: log a warning and return empty dict. The user must pre-download the file.

**Method `_match_usfs_to_tribe(self, usfs_data: dict, tribe: dict) -> dict | None`:**

Match USFS data rows to a Tribe:
1. Try AIANNH GEOID match first (if USFS data has GEOIDs)
2. Fall back to name matching (if USFS data has area names) -- use simple normalized string comparison, NOT fuzzy matching (USFS data should have clean area names)
3. Return matched wildfire metrics dict or None

**Method `build_all_profiles(self) -> int`:**

Main orchestration:
1. Load NRI county data
2. Load NRI tribal relational data
3. Load USFS wildfire data
4. For each of the 592 Tribes:
   - Build NRI profile via `_build_tribe_nri_profile()`
   - Match USFS wildfire data via `_match_usfs_to_tribe()`
   - Combine into full profile:
     ```python
     {
         "tribe_id": tribe_id,
         "tribe_name": tribe["name"],
         "sources": {
             "fema_nri": {...},  # From NRI profile
             "usfs_wildfire": {...},  # From USFS match (or empty dict)
         },
         "generated_at": datetime.now(timezone.utc).isoformat(),
     }
     ```
   - Write to `data/hazard_profiles/{tribe_id}.json` with encoding="utf-8"
5. Log summary: N profiles written, M with NRI data, K with USFS data
6. Return count of files written

**Critical implementation notes:**
- ALL file operations use `encoding="utf-8"` (Windows default encoding breaks Tribal names)
- All numeric values stored as float (not string) for Phase 7 multiplier math
- `_safe_float()` helper handles empty strings, None, and non-numeric CSV values gracefully
- Log unmatched Tribes at WARNING level (27 expected from crosswalk gaps)
- Create cache directory with `Path.mkdir(parents=True, exist_ok=True)`
  </action>
  <verify>
    - `pip show openpyxl` confirms openpyxl is installed
    - `python -c "from src.packets.hazards import HazardProfileBuilder; print('Import OK')"` confirms module imports
    - `python -c "from src.packets.hazards import HazardProfileBuilder, NRI_HAZARD_CODES; print(f'{len(NRI_HAZARD_CODES)} hazard codes')"` confirms 18 hazard codes defined
    - Existing tests still pass: `pytest tests/ -v`
  </verify>
  <done>
    - `requirements.txt` includes `openpyxl>=3.1.0`
    - `src/packets/hazards.py` exists with HazardProfileBuilder class
    - NRI county CSV parsing extracts all 18 hazard types with risk scores, EAL, and ratings
    - NRI tribal relational data bridges AIANNH areas to counties
    - USFS XLSX parsing extracts wildfire risk metrics (with schema discovery)
    - Per-Tribe hazard profiles aggregate across overlapping counties
    - Top 3 hazards ranked by risk score
    - 592 cache files written (one per Tribe, even if sparse)
    - All existing tests pass (no regressions)
  </done>
</task>

</tasks>

<verification>
1. openpyxl installed and importable
2. HazardProfileBuilder imports cleanly
3. NRI_HAZARD_CODES has exactly 18 entries
4. All 83 existing tests pass (no regressions)
5. Module handles missing data files gracefully (warning, not crash)
</verification>

<success_criteria>
- HazardProfileBuilder class complete with NRI + USFS ingestion and per-Tribe cache
- NRI county data parsed with all 18 hazard types extracted
- USFS wildfire XLSX parsed with schema discovery
- AIANNH crosswalk bridges NRI counties to Tribal areas
- Top 3 hazards ranked per Tribe
- 592 hazard profile cache files written
- Graceful handling of missing/unavailable data files
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/06-data-acquisition/06-02-SUMMARY.md`
</output>

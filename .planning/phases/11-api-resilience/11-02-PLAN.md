---
phase: 11-api-resilience
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - src/main.py
  - src/health.py
  - tests/test_circuit_breaker.py
autonomous: true

must_haves:
  truths:
    - "When a scraper's circuit breaker is OPEN, run_scan loads cached data and continues the pipeline"
    - "Per-source cache files are saved on every successful scan with timestamp metadata"
    - "Degradation warnings include the source name and cache age"
    - "Pipeline completes and produces a report even when 1 or more APIs are unreachable"
    - "--health-check CLI command probes all 4 APIs and prints UP/DOWN/DEGRADED per source"
    - "Health check reports DEGRADED when only cached data is available for a source"
    - "Cache staleness beyond cache_max_age_hours logs a CRITICAL-level warning"
  artifacts:
    - path: "src/health.py"
      provides: "HealthChecker class with probe definitions and check_all() method"
      min_lines: 60
    - path: "src/main.py"
      provides: "Cache save/load functions, CircuitOpenError handling in run_scan, --health-check CLI"
      contains: "_save_source_cache"
  key_links:
    - from: "src/main.py"
      to: "src/scrapers/circuit_breaker.py"
      via: "import CircuitOpenError for catch block in run_scan"
      pattern: "from src\\.scrapers\\.circuit_breaker import CircuitOpenError"
    - from: "src/main.py"
      to: "src/health.py"
      via: "import HealthChecker for --health-check command"
      pattern: "from src\\.health import HealthChecker"
    - from: "src/main.py"
      to: "src/paths.py"
      via: "import OUTPUTS_DIR for cache file paths"
      pattern: "OUTPUTS_DIR"
---

<objective>
Implement graceful degradation to cached data when APIs are unreachable, and add a --health-check CLI command for monitoring API availability.

Purpose: RESL-03 (pipeline completes using cached data) and RESL-04 (health check reports status) -- the user-facing resilience features. Plan 11-01 built the circuit breaker engine; this plan wires it into the pipeline so the scanner actually degrades gracefully and operators can check API health.

Output: Updated `src/main.py` with per-source caching and CircuitOpenError fallback, new `src/health.py` with HealthChecker, `--health-check` CLI command, and tests proving degradation paths.
</objective>

<execution_context>
@D:\Claude-Workspace\.claude/get-shit-done/workflows/execute-plan.md
@D:\Claude-Workspace\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-api-resilience/11-RESEARCH.md
@.planning/phases/11-api-resilience/11-01-SUMMARY.md

@src/main.py
@src/paths.py
@src/scrapers/base.py
@src/scrapers/circuit_breaker.py
@config/scanner_config.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add per-source cache and graceful degradation to run_scan</name>
  <files>
    src/main.py
    tests/test_circuit_breaker.py
  </files>
  <action>
    **Add per-source cache functions** to `src/main.py` (module-level, before `run_scan`):

    1. `_source_cache_path(source_name: str) -> Path` -- returns `OUTPUTS_DIR / f".cache_{source_name}.json"`. Import `Path` from pathlib and `OUTPUTS_DIR` from `src.paths` (OUTPUTS_DIR is already importable; add it to the existing import if not present).

    2. `_save_source_cache(source_name: str, items: list[dict]) -> None`:
       - Creates parent directory if needed.
       - Writes to a `.tmp` file first, then replaces atomically (atomic write pattern per CLAUDE.md rule 8).
       - JSON structure: `{"source": source_name, "cached_at": datetime.now(timezone.utc).isoformat(), "item_count": len(items), "items": items}`.
       - Uses `encoding="utf-8"` and `default=str` for json.dump.
       - Import `datetime` and `timezone` from `datetime` (already imported in base.py but main.py needs its own).

    3. `_load_source_cache(source_name: str, config: dict | None = None) -> list[dict]`:
       - Returns `[]` if cache file doesn't exist.
       - Loads JSON, checks file size < 10MB before parsing (security pattern from MEMORY.md).
       - Logs WARNING with cache age: "DEGRADED: {source_name} using cached data from {cached_at}".
       - Check `cache_max_age_hours` from `config.get("resilience", {}).get("cache_max_age_hours", 168)`. If cache is older than this, log CRITICAL: "STALE CACHE: {source_name} cache is {hours}h old (limit: {cache_max_age_hours}h)".
       - Returns `data.get("items", [])`.
       - Catches `json.JSONDecodeError` and `IOError`, returns `[]` on error.

    **Modify `run_scan`** inner function `_run_one`:

    ```python
    async def _run_one(source_name: str) -> list[dict]:
        if source_name not in SCRAPERS:
            logger.warning("Unknown source: %s", source_name)
            return []
        scraper_cls = SCRAPERS[source_name]
        scraper = scraper_cls(config)
        logger.info("Scanning %s...", source_name)
        try:
            items = await scraper.scan()
            logger.info("  -> %d items from %s", len(items), source_name)
            if items:
                _save_source_cache(source_name, items)
            return items
        except CircuitOpenError:
            logger.warning("DEGRADED: %s circuit OPEN, falling back to cache", source_name)
            return _load_source_cache(source_name, config)
        except Exception:
            logger.exception("Failed to scan %s, falling back to cache", source_name)
            return _load_source_cache(source_name, config)
    ```

    Add `from src.scrapers.circuit_breaker import CircuitOpenError` to imports at top of main.py.

    Add `from datetime import datetime, timezone` to imports (check if already present).

    **Add `cache_max_age_hours`** to the resilience section of `scanner_config.json`:
    ```json
    "cache_max_age_hours": 168
    ```
    (This is 7 days. Add it after `"recovery_timeout"` inside the resilience section created by Plan 11-01.)

    **Add tests** to `tests/test_circuit_breaker.py`:

    - `test_save_source_cache_creates_file` -- saves cache, verifies file exists with correct structure
    - `test_load_source_cache_returns_items` -- round-trip save then load
    - `test_load_source_cache_missing_file` -- returns [] when no cache
    - `test_load_source_cache_corrupt_json` -- returns [] on bad JSON
    - `test_load_source_cache_logs_degradation_warning` -- verify WARNING logged with cache age (use caplog)
    - `test_load_source_cache_logs_critical_for_stale` -- verify CRITICAL logged when cache exceeds max age

    Use `tmp_path` fixture for cache file tests (pytest provides it). Monkeypatch `src.main.OUTPUTS_DIR` or the `_source_cache_path` function to use tmp_path.
  </action>
  <verify>
    ```bash
    python -m pytest tests/test_circuit_breaker.py -v -k "cache"
    python -m pytest tests/ -v --tb=short
    ```
    All cache tests pass. Full test suite passes.
  </verify>
  <done>
    Per-source cache files are saved on successful scans and loaded as fallback when circuit breaker is OPEN or any exception occurs. Cache staleness warnings at WARNING and CRITICAL levels. RESL-03 satisfied.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create HealthChecker and --health-check CLI command</name>
  <files>
    src/health.py
    src/main.py
    tests/test_circuit_breaker.py
  </files>
  <action>
    **Create `src/health.py`** with HealthChecker class:

    ```python
    """API health check for TCR Policy Scanner sources.

    Probes each external API with a minimal request and reports
    UP / DOWN / DEGRADED status per source.
    """

    import asyncio
    import logging
    import time
    from datetime import datetime, timezone

    import aiohttp

    from src.scrapers.base import USER_AGENT

    logger = logging.getLogger(__name__)
    ```

    1. `HealthChecker` class with `__init__(self, config: dict)` that stores config and builds probe definitions:
       ```python
       PROBES = {
           "federal_register": {
               "url": "https://www.federalregister.gov/api/v1/documents.json?per_page=1",
               "method": "GET",
           },
           "grants_gov": {
               "url": "https://api.grants.gov/v1/api/search2",
               "method": "POST",
               "json": {"keyword": "test", "rows": 1},
           },
           "congress_gov": {
               "url": "https://api.congress.gov/v3/bill?limit=1",
               "method": "GET",
               "requires_key": True,
           },
           "usaspending": {
               "url": "https://api.usaspending.gov/api/v2/search/spending_by_award/",
               "method": "POST",
               "json": {"filters": {"award_type_codes": ["02"]}, "limit": 1},
           },
       }
       ```

    2. `async def check_all(self) -> dict[str, dict]`:
       - For each source in PROBES, call `_probe_one()`.
       - Returns dict mapping source name to `{"status": "UP"|"DOWN"|"DEGRADED", "latency_ms": int, "detail": str}`.
       - Check for cached data: if probe is DOWN but cache exists, report DEGRADED with cache age.

    3. `async def _probe_one(self, source_name: str, probe: dict) -> dict`:
       - Creates aiohttp session with USER_AGENT header.
       - For congress_gov, check `CONGRESS_API_KEY` env var. If missing, return `{"status": "DOWN", "detail": "API key not configured"}`.
       - If `requires_key` and key is available, add it as `api_key` query parameter.
       - Measure latency with `time.monotonic()`.
       - Timeout: use `aiohttp.ClientTimeout(total=10)` (quick probe, not full request).
       - On success (HTTP 2xx): return `{"status": "UP", "latency_ms": latency, "detail": "OK"}`.
       - On timeout/error: check if per-source cache exists (import `_source_cache_path` from main or duplicate the path logic using OUTPUTS_DIR). If cache exists, return DEGRADED with cache age. If no cache, return DOWN.

    4. `def format_report(results: dict[str, dict]) -> str`:
       - Format as aligned text table:
         ```
         API Health Check
         ----------------------------------------
         federal_register:  UP       (237ms)
         grants_gov:        UP       (412ms)
         congress_gov:      DOWN     (API key not configured)
         usaspending:       DEGRADED (cached data from 2026-02-10T14:30:00Z)
         ```
       - Return the formatted string.

    **Update `src/main.py`** CLI:

    1. Add `--health-check` argument to argparse:
       ```python
       parser.add_argument("--health-check", action="store_true",
                           help="Check API availability for all sources")
       ```

    2. Add handler after config loading, before `args.prep_packets` block:
       ```python
       if args.health_check:
           from src.health import HealthChecker
           checker = HealthChecker(config)
           results = asyncio.run(checker.check_all())
           from src.health import format_report
           print(format_report(results))
           return
       ```

    **Add tests** to `tests/test_circuit_breaker.py`:

    - `test_health_checker_format_report` -- format_report produces aligned output with all 4 sources
    - `test_health_checker_up_result` -- mock aiohttp response returning 200 -> UP status
    - `test_health_checker_down_result` -- mock aiohttp raising timeout -> DOWN status
    - `test_health_checker_degraded_with_cache` -- DOWN probe + existing cache file -> DEGRADED status
    - `test_health_check_cli_argument` -- argparse accepts --health-check flag

    For health check tests, mock aiohttp.ClientSession to avoid real HTTP calls. Use `unittest.mock.AsyncMock` for async context managers. Use `tmp_path` for cache files.
  </action>
  <verify>
    ```bash
    python -m pytest tests/test_circuit_breaker.py -v -k "health"
    python -m pytest tests/ -v --tb=short
    ruff check src/health.py src/main.py
    python -m src.main --help | grep "health-check"
    ```
    All health tests pass. Full suite passes. ruff clean. CLI shows --health-check option.
  </verify>
  <done>
    HealthChecker probes all 4 APIs and reports UP/DOWN/DEGRADED. --health-check CLI command available. DEGRADED reported when cache exists but API is down. RESL-04 satisfied. Combined with Task 1, Phase 11 delivers all 4 RESL requirements.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_circuit_breaker.py -v` -- all tests pass (19+ from Plan 01 + 11 from Plan 02 = 30+ total)
2. `python -m pytest tests/ -v` -- full test suite passes with zero regressions
3. `ruff check .` -- zero violations
4. `python -m src.main --help` -- shows --health-check option
5. `python -c "from src.health import HealthChecker, format_report; print('OK')"` -- imports work
6. `python -c "from src.main import _save_source_cache, _load_source_cache; print('OK')"` -- cache functions importable
7. Verify `scanner_config.json` has `cache_max_age_hours` in resilience section
</verification>

<success_criteria>
- Per-source cache files (.cache_{source}.json) saved on every successful scan with atomic write
- CircuitOpenError in run_scan triggers cache fallback instead of empty list
- Any exception in run_scan triggers cache fallback (not just CircuitOpenError)
- Cache staleness logged at WARNING level, CRITICAL when beyond cache_max_age_hours
- Pipeline completes and produces a report even when APIs are unreachable (uses cached data)
- HealthChecker probes all 4 APIs with minimal requests (1 result each)
- Health check reports UP (API responding), DOWN (unreachable, no cache), DEGRADED (unreachable, cache available)
- --health-check CLI command prints formatted status table
- cache_max_age_hours configurable in scanner_config.json (default 168 = 7 days)
- 30+ total tests across both plans cover all circuit breaker + degradation + health paths
- Full test suite passes with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/11-api-resilience/11-02-SUMMARY.md`
</output>

"""Automated quality review for generated advocacy documents.

Checks audience differentiation, air gap compliance, placeholder detection,
content completeness, and confidential marking correctness across batch
document sets.

Operates on generated DOCX files, scanning paragraph text for prohibited
patterns. Returns structured results enabling automated pass/fail gating
in the batch generation pipeline.

Air gap enforced: no organizational names, tool names, or generation
attribution appear in any document surface.
"""

from __future__ import annotations

import logging
import re
from dataclasses import dataclass, field
from pathlib import Path

from docx import Document

from src.packets.doc_types import DOC_A, DOC_B, DOC_C, DOC_D, DocumentTypeConfig

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------


@dataclass
class ReviewIssue:
    """A single quality issue found during document review.

    Attributes:
        category: Issue category (audience_leakage, air_gap, placeholder,
            formatting, content).
        severity: Issue severity (critical, warning).
        message: Human-readable description of the issue.
    """

    category: str
    severity: str
    message: str


@dataclass
class ReviewResult:
    """Review outcome for a single document.

    Attributes:
        path: Path to the reviewed DOCX file.
        doc_type: Document type identifier (A, B, C, D).
        issues: List of ReviewIssue objects found.
        passed: True if no critical-severity issues found.
    """

    path: Path
    doc_type: str
    issues: list[ReviewIssue]
    passed: bool


@dataclass
class BatchReviewResult:
    """Aggregate review outcome for a batch of documents.

    Attributes:
        total_reviewed: Number of documents reviewed.
        total_passed: Number of documents that passed all critical checks.
        total_failed: Number of documents that failed one or more critical checks.
        critical_issues: Total count of critical-severity issues.
        warning_issues: Total count of warning-severity issues.
        issues_by_category: Count of issues per category.
        failed_documents: List of ReviewResult objects for failed documents.
    """

    total_reviewed: int
    total_passed: int
    total_failed: int
    critical_issues: int
    warning_issues: int
    issues_by_category: dict[str, int]
    failed_documents: list[ReviewResult]


# ---------------------------------------------------------------------------
# DocumentQualityReviewer
# ---------------------------------------------------------------------------


class DocumentQualityReviewer:
    """Automated quality review for generated advocacy documents.

    Checks audience differentiation, air gap compliance, placeholder detection,
    and content completeness.
    """

    # Words/phrases that must NEVER appear in congressional docs (B/D).
    # These indicate internal strategy content that would be inappropriate
    # for a document handed to a congressional office.
    INTERNAL_ONLY_PATTERNS: list[str] = [
        "strategic leverage",
        "approach strategy",
        "messaging framework",
        "pressure point",
        "leverage point",
        "window of opportunity",
        "sequencing",
        "prioritize meeting",
        r"approach.*with.*frame",
        "voting record",
        "political dynamic",
        "lobby",
        "lobbying",
        "push for",
        "the lever",
        "impoundment",
        "hollowing",
        "withholding",
        "circumventing",
    ]

    # Words/phrases that must NEVER appear in any document (air gap).
    # These reveal the tool, organization, or generation provenance.
    AIR_GAP_VIOLATIONS: list[str] = [
        "TCR Policy Scanner",
        "ATNI",
        "NCAI",
        "IndigenousACCESS",
        "Climate Resilience Committee",
        "Tribal Advocacy Packet",
        "Generated by",
        "Prepared by",
        "our organization",
        "the Task Force",
        "the Committee",
    ]

    # Placeholder patterns that should not survive in documents for Tribes
    # with real data. These indicate incomplete or template content.
    PLACEHOLDER_PATTERNS: list[str] = [
        "data pending",
        "TBD",
        "placeholder",
        "[INSERT",
        "PLACEHOLDER",
        "data not available",
    ]

    def review_document(
        self,
        docx_path: Path,
        doc_type_config: DocumentTypeConfig,
    ) -> ReviewResult:
        """Review a single document against quality criteria.

        Checks audience leakage, air gap violations, placeholder text,
        confidential marking correctness, minimum content, and executive
        summary presence.

        Args:
            docx_path: Path to the DOCX file to review.
            doc_type_config: DocumentTypeConfig for this document's type.

        Returns:
            ReviewResult with pass/fail and list of issues.
        """
        doc = Document(str(docx_path))
        all_text = " ".join(p.text for p in doc.paragraphs)
        all_text_lower = all_text.lower()

        issues: list[ReviewIssue] = []

        # Check 1: Audience leakage (congressional docs only)
        if doc_type_config.is_congressional:
            for pattern in self.INTERNAL_ONLY_PATTERNS:
                if re.search(pattern, all_text_lower):
                    issues.append(
                        ReviewIssue(
                            category="audience_leakage",
                            severity="critical",
                            message=(
                                f"Internal-only pattern found in congressional "
                                f"doc: '{pattern}'"
                            ),
                        )
                    )

        # Check 2: Air gap violations (all docs)
        for pattern in self.AIR_GAP_VIOLATIONS:
            if pattern.lower() in all_text_lower:
                issues.append(
                    ReviewIssue(
                        category="air_gap",
                        severity="critical",
                        message=f"Air gap violation: '{pattern}' found in document",
                    )
                )

        # Check 3: Placeholder text
        for pattern in self.PLACEHOLDER_PATTERNS:
            if pattern.lower() in all_text_lower:
                issues.append(
                    ReviewIssue(
                        category="placeholder",
                        severity="warning",
                        message=f"Placeholder text found: '{pattern}'",
                    )
                )

        # Check 4: Confidential marking correctness
        if doc_type_config.confidential:
            if "confidential" not in all_text_lower:
                issues.append(
                    ReviewIssue(
                        category="formatting",
                        severity="warning",
                        message="Internal doc missing CONFIDENTIAL marking",
                    )
                )
        else:
            if (
                "confidential" in all_text_lower
                and "for congressional" not in all_text_lower
            ):
                issues.append(
                    ReviewIssue(
                        category="formatting",
                        severity="critical",
                        message="Congressional doc contains CONFIDENTIAL marking",
                    )
                )

        # Check 5: Minimum content (not an empty shell)
        if len(doc.paragraphs) < 20:
            issues.append(
                ReviewIssue(
                    category="content",
                    severity="warning",
                    message=(
                        f"Document has only {len(doc.paragraphs)} paragraphs "
                        f"(expected 20+)"
                    ),
                )
            )

        # Check 6: Has executive summary
        if "executive summary" not in all_text_lower:
            issues.append(
                ReviewIssue(
                    category="content",
                    severity="warning",
                    message="Missing Executive Summary section",
                )
            )

        passed = not any(i.severity == "critical" for i in issues)

        return ReviewResult(
            path=docx_path,
            doc_type=doc_type_config.doc_type,
            issues=issues,
            passed=passed,
        )

    def review_batch(
        self,
        output_dir: Path,
        doc_type_config_map: dict[str, DocumentTypeConfig] | None = None,
    ) -> BatchReviewResult:
        """Review all documents in output directory.

        Scans internal/, congressional/, regional/internal/, and
        regional/congressional/ subdirectories. Maps each subdirectory
        to its corresponding DocumentTypeConfig.

        Args:
            output_dir: Root output directory containing subdirectories.
            doc_type_config_map: Optional custom mapping of subdirectory
                pattern to DocumentTypeConfig. If None, uses default mapping.

        Returns:
            BatchReviewResult with aggregate pass/fail counts and issues.
        """
        if doc_type_config_map is None:
            doc_type_config_map = {
                "internal": DOC_A,
                "congressional": DOC_B,
                "regional_internal": DOC_C,
                "regional_congressional": DOC_D,
            }

        # Build list of (docx_path, doc_type_config) tuples
        review_targets: list[tuple[Path, DocumentTypeConfig]] = []

        # Top-level internal/ and congressional/
        internal_dir = output_dir / "internal"
        if internal_dir.is_dir():
            for docx_path in sorted(internal_dir.glob("*.docx")):
                review_targets.append(
                    (docx_path, doc_type_config_map["internal"])
                )

        congressional_dir = output_dir / "congressional"
        if congressional_dir.is_dir():
            for docx_path in sorted(congressional_dir.glob("*.docx")):
                review_targets.append(
                    (docx_path, doc_type_config_map["congressional"])
                )

        # Regional subdirectories
        regional_internal = output_dir / "regional" / "internal"
        if regional_internal.is_dir():
            for docx_path in sorted(regional_internal.glob("*.docx")):
                review_targets.append(
                    (docx_path, doc_type_config_map["regional_internal"])
                )

        regional_congressional = output_dir / "regional" / "congressional"
        if regional_congressional.is_dir():
            for docx_path in sorted(regional_congressional.glob("*.docx")):
                review_targets.append(
                    (docx_path, doc_type_config_map["regional_congressional"])
                )

        # Review all documents
        results: list[ReviewResult] = []
        for docx_path, dtc in review_targets:
            try:
                result = self.review_document(docx_path, dtc)
                results.append(result)
            except Exception as exc:
                logger.warning(
                    "Failed to review %s: %s", docx_path, exc
                )
                results.append(
                    ReviewResult(
                        path=docx_path,
                        doc_type=dtc.doc_type,
                        issues=[
                            ReviewIssue(
                                category="error",
                                severity="critical",
                                message=f"Review failed: {exc}",
                            )
                        ],
                        passed=False,
                    )
                )

        # Aggregate results
        total_reviewed = len(results)
        total_passed = sum(1 for r in results if r.passed)
        total_failed = total_reviewed - total_passed

        critical_issues = sum(
            1
            for r in results
            for i in r.issues
            if i.severity == "critical"
        )
        warning_issues = sum(
            1
            for r in results
            for i in r.issues
            if i.severity == "warning"
        )

        issues_by_category: dict[str, int] = {}
        for r in results:
            for i in r.issues:
                issues_by_category[i.category] = (
                    issues_by_category.get(i.category, 0) + 1
                )

        failed_documents = [r for r in results if not r.passed]

        return BatchReviewResult(
            total_reviewed=total_reviewed,
            total_passed=total_passed,
            total_failed=total_failed,
            critical_issues=critical_issues,
            warning_issues=warning_issues,
            issues_by_category=issues_by_category,
            failed_documents=failed_documents,
        )

    def generate_report(self, batch_result: BatchReviewResult) -> str:
        """Generate markdown report of batch review results.

        Args:
            batch_result: BatchReviewResult from review_batch().

        Returns:
            Formatted markdown string suitable for saving to a file.
        """
        lines: list[str] = []
        lines.append("# Document Quality Review Report")
        lines.append("")

        # Summary
        pass_rate = (
            (batch_result.total_passed / batch_result.total_reviewed * 100)
            if batch_result.total_reviewed > 0
            else 0.0
        )
        lines.append("## Summary")
        lines.append("")
        lines.append(f"- **Total Reviewed:** {batch_result.total_reviewed}")
        lines.append(f"- **Passed:** {batch_result.total_passed}")
        lines.append(f"- **Failed:** {batch_result.total_failed}")
        lines.append(f"- **Pass Rate:** {pass_rate:.1f}%")
        lines.append(
            f"- **Critical Issues:** {batch_result.critical_issues}"
        )
        lines.append(f"- **Warning Issues:** {batch_result.warning_issues}")
        lines.append("")

        # Issues by category
        if batch_result.issues_by_category:
            lines.append("## Issues by Category")
            lines.append("")
            lines.append("| Category | Count |")
            lines.append("|----------|-------|")
            for cat, count in sorted(
                batch_result.issues_by_category.items(),
                key=lambda x: -x[1],
            ):
                lines.append(f"| {cat} | {count} |")
            lines.append("")

        # Failed documents
        if batch_result.failed_documents:
            lines.append("## Failed Documents")
            lines.append("")
            for r in batch_result.failed_documents[:50]:
                lines.append(f"### {r.path.name} (Doc {r.doc_type})")
                lines.append("")
                for issue in r.issues:
                    severity_marker = (
                        "CRITICAL" if issue.severity == "critical" else "warning"
                    )
                    lines.append(
                        f"- [{severity_marker}] [{issue.category}] "
                        f"{issue.message}"
                    )
                lines.append("")
            if len(batch_result.failed_documents) > 50:
                lines.append(
                    f"*... and {len(batch_result.failed_documents) - 50} "
                    f"more failed documents*"
                )
                lines.append("")

        # Pass/fail verdict
        lines.append("## Verdict")
        lines.append("")
        if batch_result.total_failed == 0:
            lines.append(
                "All documents passed critical quality checks."
            )
        else:
            lines.append(
                f"{batch_result.total_failed} documents failed critical "
                f"quality checks. Review the issues above."
            )
        lines.append("")

        return "\n".join(lines)
